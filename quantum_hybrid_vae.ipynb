{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNdXTyi1MgY3DumSyS294ne",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arunsingh/API_PythonSampleScripts/blob/master/quantum_hybrid_vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca11f974"
      },
      "source": [
        "# Hybrid Quantum-Classical VAE\n",
        "This notebook is a hybrid Quantum-Classical VAE, incorporating MLOps best practices based on the principles outlined at \"https://ml-ops.org/content/mlops-principles\".\n",
        "\n",
        "Author: Arun Singh | arunsingh.in@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71ee27b9"
      },
      "source": [
        "## Install necessary libraries\n",
        "\n",
        "Install libraries for quantum computing (e.g., PennyLane, Qiskit) and MLOps (e.g., MLflow, DVC).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0be99b8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c9db19f-36ed-4e79-8c24-531be3b1a296"
      },
      "source": [
        "!pip install pennylane qiskit mlflow dvc --quiet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m115.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m461.0/461.0 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.8/930.8 kB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.3/77.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.8/438.8 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.9/374.9 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m680.6/680.6 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.0/109.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m121.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff9679f5"
      },
      "source": [
        "## Define the quantum-classical vae architecture\n",
        "\n",
        "Design of the hybrid model, including the quantum layer. Define the hybrid Quantum-Classical VAE model, including the classical encoder, quantum layer, and classical decoder, and implement the forward pass.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5dcddc9"
      },
      "source": [
        "import pennylane as qml\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class HybridVAE(nn.Module):\n",
        "    def __init__(self, input_dim=100, latent_dim_classical=32, latent_dim_quantum=4):\n",
        "        super().__init__()\n",
        "        self.latent_dim_classical = latent_dim_classical\n",
        "        self.latent_dim_quantum = latent_dim_quantum\n",
        "\n",
        "        # Classical Encoder\n",
        "        self.encoder_classical = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, latent_dim_classical + latent_dim_quantum) # Output mean and log_var for both classical and quantum latent space\n",
        "        )\n",
        "\n",
        "        # Quantum Layer\n",
        "        self.dev = qml.device(\"default.qubit\", wires=latent_dim_quantum)\n",
        "\n",
        "        @qml.qnode(self.dev, interface=\"torch\")\n",
        "        def quantum_circuit(inputs):\n",
        "            # Simple variational quantum circuit\n",
        "            for i in range(self.latent_dim_quantum):\n",
        "                qml.RY(inputs[i], wires=i)\n",
        "            for i in range(self.latent_dim_quantum - 1):\n",
        "                qml.CZ(wires=[i, i+1])\n",
        "            return [qml.expval(qml.PauliZ(i)) for i in range(self.latent_dim_quantum)]\n",
        "\n",
        "        self.quantum_layer = quantum_circuit\n",
        "\n",
        "        # Classical Decoder\n",
        "        self.decoder_classical = nn.Sequential(\n",
        "            nn.Linear(latent_dim_classical + latent_dim_quantum, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, input_dim)\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Classical Encoder forward pass\n",
        "        encoded = self.encoder_classical(x)\n",
        "        mu_log_var = encoded.view(-1, 2, self.latent_dim_classical + self.latent_dim_quantum)\n",
        "        mu = mu_log_var[:, 0, :]\n",
        "        log_var = mu_log_var[:, 1, :]\n",
        "\n",
        "        # Split latent space for classical and quantum parts\n",
        "        mu_classical = mu[:, :self.latent_dim_classical]\n",
        "        log_var_classical = log_var[:, :self.latent_dim_classical]\n",
        "        mu_quantum = mu[:, self.latent_dim_classical:]\n",
        "        log_var_quantum = log_var[:, self.latent_dim_classical:]\n",
        "\n",
        "        # Reparameterization trick for classical latent space\n",
        "        z_classical = self.reparameterize(mu_classical, log_var_classical)\n",
        "\n",
        "        # Pass quantum latent space through quantum layer\n",
        "        # Apply reparameterization trick for quantum latent space inputs to the quantum circuit\n",
        "        z_quantum_input = self.reparameterize(mu_quantum, log_var_quantum)\n",
        "        z_quantum_output = torch.stack([self.quantum_layer(z_quantum_input[i, :]) for i in range(z_quantum_input.size(0))])\n",
        "\n",
        "        # Concatenate classical and quantum latent representations\n",
        "        z = torch.cat((z_classical, z_quantum_output), dim=1)\n",
        "\n",
        "        # Classical Decoder forward pass\n",
        "        reconstruction = self.decoder_classical(z)\n",
        "\n",
        "        return reconstruction, mu, log_var"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdb693e0"
      },
      "source": [
        "## Prepare data\n",
        "\n",
        "Adapt the data simulation or loading process for the VAE input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c932f42"
      },
      "source": [
        "## Implement training loop with mlops tracking\n",
        "\n",
        "Set up the training process and integrate MLOps tools for experiment tracking (e.g., logging parameters, metrics, and models).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "f552f26d",
        "outputId": "5dbc06b7-57e7-4848-8999-0361eb211fb2"
      },
      "source": [
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import torch.nn.functional as F\n",
        "import pennylane as qml\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "'''\n",
        "Implement the training loop for the Hybrid VAE, including MLflow tracking\n",
        "for parameters and metrics, and logging the final model.\n",
        "'''\n",
        "\n",
        "# Start an MLflow run\n",
        "with mlflow.start_run():\n",
        "    # Log hyperparameters\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 10\n",
        "    classical_latent_dim = 32\n",
        "    quantum_latent_dim = 4\n",
        "\n",
        "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
        "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
        "    mlflow.log_param(\"classical_latent_dim\", classical_latent_dim)\n",
        "    mlflow.log_param(\"quantum_latent_dim\", quantum_latent_dim)\n",
        "\n",
        "    # Define the Hybrid VAE model within the training script for clarity and to ensure the latest definition is used\n",
        "    class HybridVAE(nn.Module):\n",
        "        def __init__(self, input_dim=100, latent_dim_classical=32, latent_dim_quantum=4):\n",
        "            super().__init__()\n",
        "            self.latent_dim_classical = latent_dim_classical\n",
        "            self.latent_dim_quantum = latent_dim_quantum\n",
        "\n",
        "            # Classical Encoder\n",
        "            self.encoder_classical = nn.Sequential(\n",
        "                nn.Linear(input_dim, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, 2 * (latent_dim_classical + latent_dim_quantum)) # Output 2x for mu and log_var\n",
        "            )\n",
        "\n",
        "            # Quantum Layer\n",
        "            self.dev = qml.device(\"default.qubit\", wires=latent_dim_quantum)\n",
        "\n",
        "            @qml.qnode(self.dev, interface=\"torch\")\n",
        "            def quantum_circuit(inputs):\n",
        "                # Simple variational quantum circuit\n",
        "                # inputs should have shape (batch_size, latent_dim_quantum)\n",
        "                for i in range(self.latent_dim_quantum):\n",
        "                    # Apply RY gate to each qubit with the corresponding input from the latent space\n",
        "                    qml.RY(inputs[:, i], wires=i)\n",
        "                for i in range(self.latent_dim_quantum - 1):\n",
        "                    qml.CZ(wires=[i, i+1])\n",
        "                # Return a list of expectation values. PennyLane with torch interface\n",
        "                # should handle batching and convert this list of measurement processes to a tensor\n",
        "                # of shape (batch_size, latent_dim_quantum).\n",
        "                return [qml.expval(qml.PauliZ(i)) for i in range(self.latent_dim_quantum)]\n",
        "\n",
        "            self.quantum_layer = quantum_circuit\n",
        "\n",
        "            # Classical Decoder\n",
        "            self.decoder_classical = nn.Sequential(\n",
        "                nn.Linear(latent_dim_classical + latent_dim_quantum, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, input_dim)\n",
        "            )\n",
        "\n",
        "        def reparameterize(self, mu, log_var):\n",
        "            std = torch.exp(0.5 * log_var)\n",
        "            eps = torch.randn_like(std)\n",
        "            return mu + eps * std\n",
        "\n",
        "        def forward(self, x):\n",
        "            # Classical Encoder forward pass\n",
        "            encoded = self.encoder_classical(x)\n",
        "            # Reshape to separate mu and log_var for the combined latent space\n",
        "            mu_log_var = encoded.view(-1, 2, self.latent_dim_classical + self.latent_dim_quantum)\n",
        "            mu = mu_log_var[:, 0, :]\n",
        "            log_var = mu_log_var[:, 1, :]\n",
        "\n",
        "\n",
        "            # Split latent space for classical and quantum parts\n",
        "            mu_classical = mu[:, :self.latent_dim_classical]\n",
        "            log_var_classical = log_var[:, :self.latent_dim_classical]\n",
        "            mu_quantum = mu[:, self.latent_dim_classical:]\n",
        "            log_var_quantum = log_var[:, self.latent_dim_classical:]\n",
        "\n",
        "            # Reparameterization trick for classical latent space\n",
        "            z_classical = self.reparameterize(mu_classical, log_var_classical)\n",
        "\n",
        "            # Pass quantum latent space through quantum layer\n",
        "            # Apply reparameterization trick for quantum latent space inputs to the quantum circuit\n",
        "            z_quantum_input = self.reparameterize(mu_quantum, log_var_quantum)\n",
        "\n",
        "            # Pass the entire batch of quantum latent inputs to the quantum layer\n",
        "            # PennyLane with interface=\"torch\" should handle the batching and return a tensor\n",
        "            z_quantum_output = self.quantum_layer(z_quantum_input)\n",
        "\n",
        "            # If the output is a list of tensors, stack it. If it's already a single tensor, keep it.\n",
        "            if isinstance(z_quantum_output, list):\n",
        "                 z_quantum_output = torch.stack(z_quantum_output, dim=1)\n",
        "\n",
        "            # Ensure data types match before concatenation\n",
        "            z_quantum_output = z_quantum_output.float()\n",
        "\n",
        "\n",
        "            # Concatenate classical and quantum latent representations\n",
        "            z = torch.cat((z_classical, z_quantum_output), dim=1)\n",
        "\n",
        "            # Classical Decoder forward pass\n",
        "            reconstruction = self.decoder_classical(z)\n",
        "\n",
        "            return reconstruction, mu, log_var\n",
        "\n",
        "\n",
        "    model = HybridVAE(input_dim=X_train_tensor.shape[1],\n",
        "                      latent_dim_classical=classical_latent_dim,\n",
        "                      latent_dim_quantum=quantum_latent_dim)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Define the loss function (VAE loss: Reconstruction + KL Divergence)\n",
        "    def vae_loss(recon_x, x, mu, log_var):\n",
        "        # Ensure recon_x and x have the same batch size\n",
        "        BCE = F.mse_loss(recon_x, x, reduction='sum') # Using MSE for reconstruction loss\n",
        "        KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        return BCE + KLD\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        # Pass the entire training tensor to the model\n",
        "        recon, mu, log_var = model(X_train_tensor)\n",
        "        loss = vae_loss(recon, X_train_tensor, mu, log_var)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / X_train_tensor.size(0)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Log training loss for the epoch\n",
        "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "\n",
        "    # Log the trained model\n",
        "    mlflow.pytorch.log_model(model, \"hybrid_vae_model\")\n",
        "\n",
        "# The MLflow run automatically ends when exiting the 'with' block"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_train_tensor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-57693562.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     model = HybridVAE(input_dim=X_train_tensor.shape[1],\n\u001b[0m\u001b[1;32m    116\u001b[0m                       \u001b[0mlatent_dim_classical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassical_latent_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                       latent_dim_quantum=quantum_latent_dim)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train_tensor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "j-p7VDuvNma6",
        "outputId": "16a4c0fe-2523-4cb4-eb1c-be10535988f0"
      },
      "source": [
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "\n",
        "\n",
        "'''\n",
        " The error \"TypeError: expected Tensor as element 0 in argument 0, but got list\" in\n",
        " the torch.stack call within the forward method of the HybridVAE indicates that the\n",
        " self.quantum_layer (the QNode) is returning a list instead of a tensor when called\n",
        "inside a list comprehension. The QNode should return a tensor directly when the interface\n",
        "is set to \"torch\". The code block needs to be regenerated with the fix to ensure the QNode\n",
        "returns a tensor compatible with torch.stack.\n",
        "'''\n",
        "# Start an MLflow run\n",
        "with mlflow.start_run():\n",
        "    # Log hyperparameters\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 10\n",
        "    classical_latent_dim = 32\n",
        "    quantum_latent_dim = 4\n",
        "\n",
        "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
        "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
        "    mlflow.log_param(\"classical_latent_dim\", classical_latent_dim)\n",
        "    mlflow.log_param(\"quantum_latent_dim\", quantum_latent_dim)\n",
        "\n",
        "    # Initialize the model, optimizer, and loss function\n",
        "    model = HybridVAE(input_dim=X_train_tensor.shape[1],\n",
        "                      latent_dim_classical=classical_latent_dim,\n",
        "                      latent_dim_quantum=quantum_latent_dim)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Define the loss function (VAE loss: Reconstruction + KL Divergence)\n",
        "    def vae_loss(recon_x, x, mu, log_var):\n",
        "        BCE = nn.functional.mse_loss(recon_x, x, reduction='sum') # Using MSE for reconstruction loss\n",
        "        KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        return BCE + KLD\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        # Ensure the quantum_layer returns a tensor directly\n",
        "        def quantum_circuit_tensor_output(inputs):\n",
        "            return torch.tensor([qml.expval(qml.PauliZ(i)) for i in range(model.latent_dim_quantum)], requires_grad=True).float()\n",
        "\n",
        "        model.quantum_layer = qml.QNode(model.dev, quantum_circuit_tensor_output, interface=\"torch\")\n",
        "\n",
        "        recon, mu, log_var = model(X_train_tensor)\n",
        "        loss = vae_loss(recon, X_train_tensor, mu, log_var)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / X_train_tensor.size(0)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Log training loss for the epoch\n",
        "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "\n",
        "    # Log the trained model\n",
        "    mlflow.pytorch.log_model(model, \"hybrid_vae_model\")\n",
        "\n",
        "# The MLflow run automatically ends when exiting the 'with' block"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "QuantumFunctionError",
          "evalue": "Invalid device. Device must be a valid PennyLane device.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mQuantumFunctionError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3077238918.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPauliZ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_dim_quantum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantum_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantum_circuit_tensor_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterface\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"torch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pennylane/workflow/qnode.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, device, interface, diff_method, grad_on_execution, cache, cachesize, max_diff, device_vjp, postselect_mode, mcm_method, gradient_kwargs, static_argnums, autograph, executor_backend)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mqml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLegacyDevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mQuantumFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid device. Device must be a valid PennyLane device.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mQuantumFunctionError\u001b[0m: Invalid device. Device must be a valid PennyLane device."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "OwMEFBrCNpee",
        "outputId": "cc96b39d-8422-41ea-8e3c-c740e1a9d802"
      },
      "source": [
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "\n",
        "'''\n",
        "\n",
        "The error `QuantumFunctionError: Invalid device. Device must be a valid PennyLane device.`\n",
        "indicates that `model.dev` is not a valid PennyLane device when trying to redefine `model.quantum_layer`\n",
        "inside the training loop. The QNode should be defined once with the correct device during the model's\n",
        "initialization, and the forward pass should use the already defined QNode. The previous attempt to\n",
        "redefine the QNode in the loop was incorrect. The code block needs to be regenerated to remove the\n",
        " redefinition of the QNode inside the loop and use the QNode as defined in the `HybridVAE` class.\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "# Start an MLflow run\n",
        "with mlflow.start_run():\n",
        "    # Log hyperparameters\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 10\n",
        "    classical_latent_dim = 32\n",
        "    quantum_latent_dim = 4\n",
        "\n",
        "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
        "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
        "    mlflow.log_param(\"classical_latent_dim\", classical_latent_dim)\n",
        "    mlflow.log_param(\"quantum_latent_dim\", quantum_latent_dim)\n",
        "\n",
        "    # Initialize the model, optimizer, and loss function\n",
        "    model = HybridVAE(input_dim=X_train_tensor.shape[1],\n",
        "                      latent_dim_classical=classical_latent_dim,\n",
        "                      latent_dim_quantum=quantum_latent_dim)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Define the loss function (VAE loss: Reconstruction + KL Divergence)\n",
        "    def vae_loss(recon_x, x, mu, log_var):\n",
        "        BCE = nn.functional.mse_loss(recon_x, x, reduction='sum') # Using MSE for reconstruction loss\n",
        "        KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        return BCE + KLD\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        recon, mu, log_var = model(X_train_tensor)\n",
        "        loss = vae_loss(recon, X_train_tensor, mu, log_var)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / X_train_tensor.size(0)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Log training loss for the epoch\n",
        "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "\n",
        "    # Log the trained model\n",
        "    mlflow.pytorch.log_model(model, \"hybrid_vae_model\")\n",
        "\n",
        "# The MLflow run automatically ends when exiting the 'with' block"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected Tensor as element 0 in argument 0, but got list",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3562364503.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1760583071.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Apply reparameterization trick for quantum latent space inputs to the quantum circuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mz_quantum_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_quantum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var_quantum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mz_quantum_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantum_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_quantum_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_quantum_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Concatenate classical and quantum latent representations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ZFHeoywSNvmL",
        "outputId": "f50c51dc-3817-4bc3-b729-1ba41b129679"
      },
      "source": [
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "\n",
        "'''\n",
        "Fixed error: The traceback indicates the same `TypeError: expected Tensor as element 0 in argument 0, but got list` when calling `torch.stack`\n",
        "with the output of `self.quantum_layer`. This confirms that the QNode within the `HybridVAE` class definition is still returning a\n",
        "list of tensors (from `qml.expval`) for each element in the batch, and `torch.stack` is expecting a\n",
        "list of single tensors to stack into a new tensor. The QNode should be modified to directly return a single tensor containing\n",
        "the expectation values.'''\n",
        "\n",
        "# Start an MLflow run\n",
        "with mlflow.start_run():\n",
        "    # Log hyperparameters\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 10\n",
        "    classical_latent_dim = 32\n",
        "    quantum_latent_dim = 4\n",
        "\n",
        "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
        "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
        "    mlflow.log_param(\"classical_latent_dim\", classical_latent_dim)\n",
        "    mlflow.log_param(\"quantum_latent_dim\", quantum_latent_dim)\n",
        "\n",
        "    # Initialize the model, optimizer, and loss function\n",
        "    # Redefine HybridVAE to ensure the QNode returns a tensor\n",
        "    class HybridVAE(nn.Module):\n",
        "        def __init__(self, input_dim=100, latent_dim_classical=32, latent_dim_quantum=4):\n",
        "            super().__init__()\n",
        "            self.latent_dim_classical = latent_dim_classical\n",
        "            self.latent_dim_quantum = latent_dim_quantum\n",
        "\n",
        "            # Classical Encoder\n",
        "            self.encoder_classical = nn.Sequential(\n",
        "                nn.Linear(input_dim, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, latent_dim_classical + latent_dim_quantum)\n",
        "            )\n",
        "\n",
        "            # Quantum Layer\n",
        "            self.dev = qml.device(\"default.qubit\", wires=latent_dim_quantum)\n",
        "\n",
        "            @qml.qnode(self.dev, interface=\"torch\")\n",
        "            def quantum_circuit(inputs):\n",
        "                # Simple variational quantum circuit\n",
        "                for i in range(self.latent_dim_quantum):\n",
        "                    qml.RY(inputs[i], wires=i)\n",
        "                for i in range(self.latent_dim_quantum - 1):\n",
        "                    qml.CZ(wires=[i, i+1])\n",
        "                # Return a single tensor containing expectation values\n",
        "                return torch.tensor([qml.expval(qml.PauliZ(i)) for i in range(self.latent_dim_quantum)], requires_grad=True).float()\n",
        "\n",
        "            self.quantum_layer = quantum_circuit\n",
        "\n",
        "            # Classical Decoder\n",
        "            self.decoder_classical = nn.Sequential(\n",
        "                nn.Linear(latent_dim_classical + latent_dim_quantum, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, input_dim)\n",
        "            )\n",
        "\n",
        "        def reparameterize(self, mu, log_var):\n",
        "            std = torch.exp(0.5 * log_var)\n",
        "            eps = torch.randn_like(std)\n",
        "            return mu + eps * std\n",
        "\n",
        "        def forward(self, x):\n",
        "            # Classical Encoder forward pass\n",
        "            encoded = self.encoder_classical(x)\n",
        "            mu_log_var = encoded.view(-1, 2, self.latent_dim_classical + self.latent_dim_quantum)\n",
        "            mu = mu_log_var[:, 0, :]\n",
        "            log_var = mu_log_var[:, 1, :]\n",
        "\n",
        "            # Split latent space for classical and quantum parts\n",
        "            mu_classical = mu[:, :self.latent_dim_classical]\n",
        "            log_var_classical = log_var[:, :self.latent_dim_classical]\n",
        "            mu_quantum = mu[:, self.latent_dim_classical:]\n",
        "            log_var_quantum = log_var[:, self.latent_dim_classical:]\n",
        "\n",
        "            # Reparameterization trick for classical latent space\n",
        "            z_classical = self.reparameterize(mu_classical, log_var_classical)\n",
        "\n",
        "            # Pass quantum latent space through quantum layer\n",
        "            # Apply reparameterization trick for quantum latent space inputs to the quantum circuit\n",
        "            z_quantum_input = self.reparameterize(mu_quantum, log_var_quantum)\n",
        "            # Pass each sample through the quantum layer and stack the results\n",
        "            z_quantum_output = torch.stack([self.quantum_layer(z_quantum_input[i, :]) for i in range(z_quantum_input.size(0))])\n",
        "\n",
        "\n",
        "            # Concatenate classical and quantum latent representations\n",
        "            z = torch.cat((z_classical, z_quantum_output), dim=1)\n",
        "\n",
        "            # Classical Decoder forward pass\n",
        "            reconstruction = self.decoder_classical(z)\n",
        "\n",
        "            return reconstruction, mu, log_var\n",
        "\n",
        "\n",
        "    model = HybridVAE(input_dim=X_train_tensor.shape[1],\n",
        "                      latent_dim_classical=classical_latent_dim,\n",
        "                      latent_dim_quantum=quantum_latent_dim)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Define the loss function (VAE loss: Reconstruction + KL Divergence)\n",
        "    def vae_loss(recon_x, x, mu, log_var):\n",
        "        BCE = nn.functional.mse_loss(recon_x, x, reduction='sum') # Using MSE for reconstruction loss\n",
        "        KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        return BCE + KLD\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        recon, mu, log_var = model(X_train_tensor)\n",
        "        loss = vae_loss(recon, X_train_tensor, mu, log_var)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / X_train_tensor.size(0)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Log training loss for the epoch\n",
        "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "\n",
        "    # Log the trained model\n",
        "    mlflow.pytorch.log_model(model, \"hybrid_vae_model\")\n",
        "\n",
        "# The MLflow run automatically ends when exiting the 'with' block"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Could not infer dtype of ExpectationMP",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2752056560.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2752056560.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mz_quantum_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_quantum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var_quantum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m# Pass each sample through the quantum layer and stack the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mz_quantum_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantum_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_quantum_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_quantum_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2752056560.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mz_quantum_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_quantum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var_quantum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m# Pass each sample through the quantum layer and stack the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mz_quantum_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantum_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_quantum_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_quantum_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pennylane/workflow/qnode.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcapture_qnode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impl_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pennylane/workflow/qnode.py\u001b[0m in \u001b[0;36m_impl_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0;31m# construct the tape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         \u001b[0mtape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;31m# Calculate the classical jacobians if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pennylane/logging/decorators.py\u001b[0m in \u001b[0;36mwrapper_entry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0m_debug_log_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             )\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pennylane/workflow/qnode.py\u001b[0m in \u001b[0;36mconstruct\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpldb_device_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mAnnotatedQueue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qfunc_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0mtape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQuantumScript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_queue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2752056560.py\u001b[0m in \u001b[0;36mquantum_circuit\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0mqml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCZ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwires\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;31m# Return a single tensor containing expectation values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPauliZ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_dim_quantum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantum_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquantum_circuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of ExpectationMP"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "y9E_nxYNNzMk",
        "outputId": "0714e512-f30c-42fd-c48a-e21cda58dffd"
      },
      "source": [
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "\n",
        "\n",
        "'''\n",
        "Fixed error: The `RuntimeError: Could not infer dtype of ExpectationMP` suggests an issue with how PennyLane is handling\n",
        "the expectation values within the QNode when trying to convert them to a PyTorch tensor. PennyLane's `qml.expval` returns\n",
        " a measurement process, not a numerical value directly within the QNode construction. The conversion to a tensor should\n",
        " happen *after* the QNode execution.\n",
        "\n",
        "The QNode should return the list of measurement processes, and the stacking and tensor conversion should be handled in the\n",
        "`forward` pass of the `HybridVAE`.\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "# Start an MLflow run\n",
        "with mlflow.start_run():\n",
        "    # Log hyperparameters\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 10\n",
        "    classical_latent_dim = 32\n",
        "    quantum_latent_dim = 4\n",
        "\n",
        "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
        "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
        "    mlflow.log_param(\"classical_latent_dim\", classical_latent_dim)\n",
        "    mlflow.log_param(\"quantum_latent_dim\", quantum_latent_dim)\n",
        "\n",
        "    # Initialize the model, optimizer, and loss function\n",
        "    # Redefine HybridVAE to ensure the QNode returns measurement processes\n",
        "    class HybridVAE(nn.Module):\n",
        "        def __init__(self, input_dim=100, latent_dim_classical=32, latent_dim_quantum=4):\n",
        "            super().__init__()\n",
        "            self.latent_dim_classical = latent_dim_classical\n",
        "            self.latent_dim_quantum = latent_dim_quantum\n",
        "\n",
        "            # Classical Encoder\n",
        "            self.encoder_classical = nn.Sequential(\n",
        "                nn.Linear(input_dim, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, latent_dim_classical + latent_dim_quantum)\n",
        "            )\n",
        "\n",
        "            # Quantum Layer\n",
        "            self.dev = qml.device(\"default.qubit\", wires=latent_dim_quantum)\n",
        "\n",
        "            @qml.qnode(self.dev, interface=\"torch\")\n",
        "            def quantum_circuit(inputs):\n",
        "                # Simple variational quantum circuit\n",
        "                for i in range(self.latent_dim_quantum):\n",
        "                    qml.RY(inputs[i], wires=i)\n",
        "                for i in range(self.latent_dim_quantum - 1):\n",
        "                    qml.CZ(wires=[i, i+1])\n",
        "                # Return a list of expectation values (measurement processes)\n",
        "                return [qml.expval(qml.PauliZ(i)) for i in range(self.latent_dim_quantum)]\n",
        "\n",
        "            self.quantum_layer = quantum_circuit\n",
        "\n",
        "            # Classical Decoder\n",
        "            self.decoder_classical = nn.Sequential(\n",
        "                nn.Linear(latent_dim_classical + latent_dim_quantum, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, input_dim)\n",
        "            )\n",
        "\n",
        "        def reparameterize(self, mu, log_var):\n",
        "            std = torch.exp(0.5 * log_var)\n",
        "            eps = torch.randn_like(std)\n",
        "            return mu + eps * std\n",
        "\n",
        "        def forward(self, x):\n",
        "            # Classical Encoder forward pass\n",
        "            encoded = self.encoder_classical(x)\n",
        "            mu_log_var = encoded.view(-1, 2, self.latent_dim_classical + self.latent_dim_quantum)\n",
        "            mu = mu_log_var[:, 0, :]\n",
        "            log_var = mu_log_var[:, 1, :]\n",
        "\n",
        "            # Split latent space for classical and quantum parts\n",
        "            mu_classical = mu[:, :self.latent_dim_classical]\n",
        "            log_var_classical = log_var[:, :self.latent_dim_classical]\n",
        "            mu_quantum = mu[:, self.latent_dim_classical:]\n",
        "            log_var_quantum = log_var[:, self.latent_dim_classical:]\n",
        "\n",
        "            # Reparameterization trick for classical latent space\n",
        "            z_classical = self.reparameterize(mu_classical, log_var_classical)\n",
        "\n",
        "            # Pass quantum latent space through quantum layer\n",
        "            # Apply reparameterization trick for quantum latent space inputs to the quantum circuit\n",
        "            z_quantum_input = self.reparameterize(mu_quantum, log_var_quantum)\n",
        "            # Pass each sample through the quantum layer and stack the results\n",
        "            # The QNode returns a list of tensors when interface=\"torch\"\n",
        "            z_quantum_output_list = [self.quantum_layer(z_quantum_input[i, :]) for i in range(z_quantum_input.size(0))]\n",
        "            # Stack the list of tensors\n",
        "            z_quantum_output = torch.stack(z_quantum_output_list)\n",
        "\n",
        "\n",
        "            # Concatenate classical and quantum latent representations\n",
        "            z = torch.cat((z_classical, z_quantum_output), dim=1)\n",
        "\n",
        "            # Classical Decoder forward pass\n",
        "            reconstruction = self.decoder_classical(z)\n",
        "\n",
        "            return reconstruction, mu, log_var\n",
        "\n",
        "\n",
        "    model = HybridVAE(input_dim=X_train_tensor.shape[1],\n",
        "                      latent_dim_classical=classical_latent_dim,\n",
        "                      latent_dim_quantum=quantum_latent_dim)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Define the loss function (VAE loss: Reconstruction + KL Divergence)\n",
        "    def vae_loss(recon_x, x, mu, log_var):\n",
        "        BCE = nn.functional.mse_loss(recon_x, x, reduction='sum') # Using MSE for reconstruction loss\n",
        "        KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        return BCE + KLD\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        recon, mu, log_var = model(X_train_tensor)\n",
        "        loss = vae_loss(recon, X_train_tensor, mu, log_var)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / X_train_tensor.size(0)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Log training loss for the epoch\n",
        "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "\n",
        "    # Log the trained model\n",
        "    mlflow.pytorch.log_model(model, \"hybrid_vae_model\")\n",
        "\n",
        "# The MLflow run automatically ends when exiting the 'with' block"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected Tensor as element 0 in argument 0, but got list",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2123590380.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2123590380.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mz_quantum_output_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantum_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_quantum_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_quantum_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;31m# Stack the list of tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mz_quantum_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_quantum_output_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "-jR9Tiq7N6HQ",
        "outputId": "2acac002-eebe-47d9-d93e-e8b46c1a6cfc"
      },
      "source": [
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "'''\n",
        "Fixed error loop:\n",
        "\n",
        "The `TypeError` persists, indicating that `torch.stack` is still receiving a list of lists instead of a list of tensors.\n",
        "This means the `quantum_layer` (the QNode) is returning a list of measurement process objects, and the list comprehension\n",
        "in the `forward` pass is creating a list where each element is the list of measurement processes from one call to the QNode.\n",
        " The QNode needs to be modified to return a single tensor when the interface is \"torch\", and the batch processing in the\n",
        " forward pass should be handled by passing the entire batch to the QNode if possible, or ensuring the QNode's output is a\n",
        " tensor for each sample before stacking. Given the QNode is designed to process a single input vector\n",
        " (representing one sample's quantum latent space), the list comprehension is necessary,\n",
        " but the QNode's return type needs to be a single tensor for each call.\n",
        "'''\n",
        "\n",
        "# Start an MLflow run\n",
        "with mlflow.start_run():\n",
        "    # Log hyperparameters\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 10\n",
        "    classical_latent_dim = 32\n",
        "    quantum_latent_dim = 4\n",
        "\n",
        "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
        "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
        "    mlflow.log_param(\"classical_latent_dim\", classical_latent_dim)\n",
        "    mlflow.log_param(\"quantum_latent_dim\", quantum_latent_dim)\n",
        "\n",
        "    # Initialize the model, optimizer, and loss function\n",
        "    # Redefine HybridVAE to ensure the QNode returns a tensor\n",
        "    class HybridVAE(nn.Module):\n",
        "        def __init__(self, input_dim=100, latent_dim_classical=32, latent_dim_quantum=4):\n",
        "            super().__init__()\n",
        "            self.latent_dim_classical = latent_dim_classical\n",
        "            self.latent_dim_quantum = latent_dim_quantum\n",
        "\n",
        "            # Classical Encoder\n",
        "            self.encoder_classical = nn.Sequential(\n",
        "                nn.Linear(input_dim, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, latent_dim_classical + latent_dim_quantum)\n",
        "            )\n",
        "\n",
        "            # Quantum Layer\n",
        "            self.dev = qml.device(\"default.qubit\", wires=latent_dim_quantum)\n",
        "\n",
        "            @qml.qnode(self.dev, interface=\"torch\")\n",
        "            def quantum_circuit(inputs):\n",
        "                # Simple variational quantum circuit\n",
        "                for i in range(self.latent_dim_quantum):\n",
        "                    qml.RY(inputs[i], wires=i)\n",
        "                for i in range(self.latent_dim_quantum - 1):\n",
        "                    qml.CZ(wires=[i, i+1])\n",
        "                # Return a list of expectation values. PennyLane with torch interface should handle conversion.\n",
        "                return [qml.expval(qml.PauliZ(i)) for i in range(self.latent_dim_quantum)]\n",
        "\n",
        "            self.quantum_layer = quantum_circuit\n",
        "\n",
        "            # Classical Decoder\n",
        "            self.decoder_classical = nn.Sequential(\n",
        "                nn.Linear(latent_dim_classical + latent_dim_quantum, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, input_dim)\n",
        "            )\n",
        "\n",
        "        def reparameterize(self, mu, log_var):\n",
        "            std = torch.exp(0.5 * log_var)\n",
        "            eps = torch.randn_like(std)\n",
        "            return mu + eps * std\n",
        "\n",
        "        def forward(self, x):\n",
        "            # Classical Encoder forward pass\n",
        "            encoded = self.encoder_classical(x)\n",
        "            # Ensure the output dimension matches the expected mu and log_var\n",
        "            mu_log_var = encoded.view(-1, 2 * (self.latent_dim_classical + self.latent_dim_quantum))\n",
        "            mu = mu_log_var[:, :(self.latent_dim_classical + self.latent_dim_quantum)]\n",
        "            log_var = mu_log_var[:, (self.latent_dim_classical + self.latent_dim_quantum):]\n",
        "\n",
        "\n",
        "            # Split latent space for classical and quantum parts\n",
        "            mu_classical = mu[:, :self.latent_dim_classical]\n",
        "            log_var_classical = log_var[:, :self.latent_dim_classical]\n",
        "            mu_quantum = mu[:, self.latent_dim_classical:]\n",
        "            log_var_quantum = log_var[:, self.latent_dim_classical:]\n",
        "\n",
        "            # Reparameterization trick for classical latent space\n",
        "            z_classical = self.reparameterize(mu_classical, log_var_classical)\n",
        "\n",
        "            # Pass quantum latent space through quantum layer\n",
        "            # Apply reparameterization trick for quantum latent space inputs to the quantum circuit\n",
        "            z_quantum_input = self.reparameterize(mu_quantum, log_var_quantum)\n",
        "            # Pass each sample through the quantum layer and stack the results\n",
        "            # The QNode should return a tensor for each call when interface=\"torch\"\n",
        "            z_quantum_output = torch.stack([self.quantum_layer(z_quantum_input[i, :]) for i in range(z_quantum_input.size(0))])\n",
        "\n",
        "\n",
        "            # Concatenate classical and quantum latent representations\n",
        "            z = torch.cat((z_classical, z_quantum_output), dim=1)\n",
        "\n",
        "            # Classical Decoder forward pass\n",
        "            reconstruction = self.decoder_classical(z)\n",
        "\n",
        "            return reconstruction, mu, log_var\n",
        "\n",
        "\n",
        "    model = HybridVAE(input_dim=X_train_tensor.shape[1],\n",
        "                      latent_dim_classical=classical_latent_dim,\n",
        "                      latent_dim_quantum=quantum_latent_dim)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Define the loss function (VAE loss: Reconstruction + KL Divergence)\n",
        "    def vae_loss(recon_x, x, mu, log_var):\n",
        "        BCE = F.mse_loss(recon_x, x, reduction='sum') # Using MSE for reconstruction loss\n",
        "        KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        return BCE + KLD\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        recon, mu, log_var = model(X_train_tensor)\n",
        "        loss = vae_loss(recon, X_train_tensor, mu, log_var)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / X_train_tensor.size(0)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Log training loss for the epoch\n",
        "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "\n",
        "    # Log the trained model\n",
        "    mlflow.pytorch.log_model(model, \"hybrid_vae_model\")\n",
        "\n",
        "# The MLflow run automatically ends when exiting the 'with' block"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected Tensor as element 0 in argument 0, but got list",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-559174588.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-559174588.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;31m# Pass each sample through the quantum layer and stack the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;31m# The QNode should return a tensor for each call when interface=\"torch\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mz_quantum_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantum_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_quantum_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_quantum_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "u3n-uaR0OCjF",
        "outputId": "a669653a-fffd-456d-befd-17376d081cef"
      },
      "source": [
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "'''\n",
        "Fixed error loop:\n",
        "\n",
        "The `TypeError` persists, indicating that `torch.stack` is still receiving a list of lists instead of a list of tensors.\n",
        "The persistent TypeError indicates that the quantum_layer (QNode) is still returning a list for each sample in the batch,\n",
        " even with interface=\"torch\". This is likely because the list comprehension [qml.expval(qml.PauliZ(i))\n",
        " for i in range(self.latent_dim_quantum)] within the QNode's quantum_circuit function is causing PennyLane to return a\n",
        " list of individual expectation value tensors, rather than a single combined tensor. To fix this, the QNode should directly\n",
        "  return a single tensor by stacking the expectation values inside the QNode function before returning.\n",
        "\n",
        "'''\n",
        "\n",
        "# Start an MLflow run\n",
        "with mlflow.start_run():\n",
        "    # Log hyperparameters\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 10\n",
        "    classical_latent_dim = 32\n",
        "    quantum_latent_dim = 4\n",
        "\n",
        "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
        "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
        "    mlflow.log_param(\"classical_latent_dim\", classical_latent_dim)\n",
        "    mlflow.log_param(\"quantum_latent_dim\", quantum_latent_dim)\n",
        "\n",
        "    # Initialize the model, optimizer, and loss function\n",
        "    # Redefine HybridVAE to ensure the QNode returns a single tensor\n",
        "    class HybridVAE(nn.Module):\n",
        "        def __init__(self, input_dim=100, latent_dim_classical=32, latent_dim_quantum=4):\n",
        "            super().__init__()\n",
        "            self.latent_dim_classical = latent_dim_classical\n",
        "            self.latent_dim_quantum = latent_dim_quantum\n",
        "\n",
        "            # Classical Encoder\n",
        "            self.encoder_classical = nn.Sequential(\n",
        "                nn.Linear(input_dim, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, latent_dim_classical + latent_dim_quantum)\n",
        "            )\n",
        "\n",
        "            # Quantum Layer\n",
        "            self.dev = qml.device(\"default.qubit\", wires=latent_dim_quantum)\n",
        "\n",
        "            @qml.qnode(self.dev, interface=\"torch\")\n",
        "            def quantum_circuit(inputs):\n",
        "                # Simple variational quantum circuit\n",
        "                for i in range(self.latent_dim_quantum):\n",
        "                    qml.RY(inputs[i], wires=i)\n",
        "                for i in range(self.latent_dim_quantum - 1):\n",
        "                    qml.CZ(wires=[i, i+1])\n",
        "                # Return a single tensor by stacking the expectation values\n",
        "                return torch.stack([qml.expval(qml.PauliZ(i)) for i in range(self.latent_dim_quantum)])\n",
        "\n",
        "            self.quantum_layer = quantum_circuit\n",
        "\n",
        "            # Classical Decoder\n",
        "            self.decoder_classical = nn.Sequential(\n",
        "                nn.Linear(latent_dim_classical + latent_dim_quantum, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, input_dim)\n",
        "            )\n",
        "\n",
        "        def reparameterize(self, mu, log_var):\n",
        "            std = torch.exp(0.5 * log_var)\n",
        "            eps = torch.randn_like(std)\n",
        "            return mu + eps * std\n",
        "\n",
        "        def forward(self, x):\n",
        "            # Classical Encoder forward pass\n",
        "            encoded = self.encoder_classical(x)\n",
        "            # Ensure the output dimension matches the expected mu and log_var\n",
        "            mu_log_var = encoded.view(-1, 2 * (self.latent_dim_classical + self.latent_dim_quantum))\n",
        "            mu = mu_log_var[:, :(self.latent_dim_classical + self.latent_dim_quantum)]\n",
        "            log_var = mu_log_var[:, (self.latent_dim_classical + self.latent_dim_quantum):]\n",
        "\n",
        "\n",
        "            # Split latent space for classical and quantum parts\n",
        "            mu_classical = mu[:, :self.latent_dim_classical]\n",
        "            log_var_classical = log_var[:, :self.latent_dim_classical]\n",
        "            mu_quantum = mu[:, self.latent_dim_classical:]\n",
        "            log_var_quantum = log_var[:, self.latent_dim_classical:]\n",
        "\n",
        "            # Reparameterization trick for classical latent space\n",
        "            z_classical = self.reparameterize(mu_classical, log_var_classical)\n",
        "\n",
        "            # Pass quantum latent space through quantum layer\n",
        "            # Apply reparameterization trick for quantum latent space inputs to the quantum circuit\n",
        "            z_quantum_input = self.reparameterize(mu_quantum, log_var_quantum)\n",
        "            # Pass each sample through the quantum layer and stack the results\n",
        "            # The QNode now returns a tensor for each call\n",
        "            z_quantum_output = torch.stack([self.quantum_layer(z_quantum_input[i, :]) for i in range(z_quantum_input.size(0))])\n",
        "\n",
        "\n",
        "            # Concatenate classical and quantum latent representations\n",
        "            z = torch.cat((z_classical, z_quantum_output), dim=1)\n",
        "\n",
        "            # Classical Decoder forward pass\n",
        "            reconstruction = self.decoder_classical(z)\n",
        "\n",
        "            return reconstruction, mu, log_var\n",
        "\n",
        "\n",
        "    model = HybridVAE(input_dim=X_train_tensor.shape[1],\n",
        "                      latent_dim_classical=classical_latent_dim,\n",
        "                      latent_dim_quantum=quantum_latent_dim)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Define the loss function (VAE loss: Reconstruction + KL Divergence)\n",
        "    def vae_loss(recon_x, x, mu, log_var):\n",
        "        BCE = F.mse_loss(recon_x, x, reduction='sum') # Using MSE for reconstruction loss\n",
        "        KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        return BCE + KLD\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        recon, mu, log_var = model(X_train_tensor)\n",
        "        loss = vae_loss(recon, X_train_tensor, mu, log_var)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / X_train_tensor.size(0)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Log training loss for the epoch\n",
        "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "\n",
        "    # Log the trained model\n",
        "    mlflow.pytorch.log_model(model, \"hybrid_vae_model\")\n",
        "\n",
        "# The MLflow run automatically ends when exiting the 'with' block"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected Tensor as element 0 in argument 0, but got ExpectationMP",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3669121859.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3669121859.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;31m# Pass each sample through the quantum layer and stack the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;31m# The QNode now returns a tensor for each call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mz_quantum_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantum_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_quantum_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_quantum_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3669121859.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;31m# Pass each sample through the quantum layer and stack the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;31m# The QNode now returns a tensor for each call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mz_quantum_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantum_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_quantum_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_quantum_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pennylane/workflow/qnode.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcapture_qnode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impl_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pennylane/workflow/qnode.py\u001b[0m in \u001b[0;36m_impl_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0;31m# construct the tape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         \u001b[0mtape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;31m# Calculate the classical jacobians if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pennylane/logging/decorators.py\u001b[0m in \u001b[0;36mwrapper_entry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0m_debug_log_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             )\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pennylane/workflow/qnode.py\u001b[0m in \u001b[0;36mconstruct\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpldb_device_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mAnnotatedQueue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 876\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qfunc_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0mtape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQuantumScript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_queue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3669121859.py\u001b[0m in \u001b[0;36mquantum_circuit\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0mqml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCZ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwires\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;31m# Return a single tensor by stacking the expectation values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPauliZ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_dim_quantum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantum_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquantum_circuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got ExpectationMP"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Yv2ohhncOG2M",
        "outputId": "c3903d31-7985-4156-c455-272e3733b1cd"
      },
      "source": [
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "'''\n",
        "Fixed error loop:\n",
        "\n",
        "The error message TypeError: expected Tensor as element 0 in argument 0, but got ExpectationMP within the QNode's\n",
        "torch.stack call indicates that qml.expval inside the QNode is returning ExpectationMP objects, which are not directly\n",
        " stackable into a tensor by torch.stack within the QNode definition itself. When using the interface=\"torch\",\n",
        " PennyLane handles the conversion of measurement results to tensors after the quantum circuit execution. The QNode\n",
        " should return the list of ExpectationMP objects, and the stacking into a single tensor for the batch should happen\n",
        " in the forward pass using torch.stack. The issue might be how the QNode's output is being handled in the forward pass.\n",
        " The previous attempts to modify the QNode's return within its definition were incorrect. The original QNode definition\n",
        " returning a list of expectation values was closer to the correct approach, but the stacking in the forward pass needs to\n",
        "  correctly handle the output structure from the QNode when processing a batch. A potential fix is to ensure that the\n",
        "  stacking in the forward pass correctly processes the list of tensors that the QNode should be returning for each\n",
        "  sample when using the \"torch\" interface.\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "# Start an MLflow run\n",
        "with mlflow.start_run():\n",
        "    # Log hyperparameters\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 10\n",
        "    classical_latent_dim = 32\n",
        "    quantum_latent_dim = 4\n",
        "\n",
        "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
        "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
        "    mlflow.log_param(\"classical_latent_dim\", classical_latent_dim)\n",
        "    mlflow.log_param(\"quantum_latent_dim\", quantum_latent_dim)\n",
        "\n",
        "    # Initialize the model, optimizer, and loss function\n",
        "    # Ensure HybridVAE definition is correct for QNode output\n",
        "    class HybridVAE(nn.Module):\n",
        "        def __init__(self, input_dim=100, latent_dim_classical=32, latent_dim_quantum=4):\n",
        "            super().__init__()\n",
        "            self.latent_dim_classical = latent_dim_classical\n",
        "            self.latent_dim_quantum = latent_dim_quantum\n",
        "\n",
        "            # Classical Encoder\n",
        "            self.encoder_classical = nn.Sequential(\n",
        "                nn.Linear(input_dim, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, latent_dim_classical + latent_dim_quantum)\n",
        "            )\n",
        "\n",
        "            # Quantum Layer\n",
        "            self.dev = qml.device(\"default.qubit\", wires=latent_dim_quantum)\n",
        "\n",
        "            @qml.qnode(self.dev, interface=\"torch\")\n",
        "            def quantum_circuit(inputs):\n",
        "                # Simple variational quantum circuit\n",
        "                for i in range(self.latent_dim_quantum):\n",
        "                    qml.RY(inputs[i], wires=i)\n",
        "                for i in range(self.latent_dim_quantum - 1):\n",
        "                    qml.CZ(wires=[i, i+1])\n",
        "                # Return a list of expectation values. PennyLane with torch interface\n",
        "                # should convert this list of measurement processes to a tensor.\n",
        "                return [qml.expval(qml.PauliZ(i)) for i in range(self.latent_dim_quantum)]\n",
        "\n",
        "            self.quantum_layer = quantum_circuit\n",
        "\n",
        "            # Classical Decoder\n",
        "            self.decoder_classical = nn.Sequential(\n",
        "                nn.Linear(latent_dim_classical + latent_dim_quantum, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, input_dim)\n",
        "            )\n",
        "\n",
        "        def reparameterize(self, mu, log_var):\n",
        "            std = torch.exp(0.5 * log_var)\n",
        "            eps = torch.randn_like(std)\n",
        "            return mu + eps * std\n",
        "\n",
        "        def forward(self, x):\n",
        "            # Classical Encoder forward pass\n",
        "            encoded = self.encoder_classical(x)\n",
        "            # Ensure the output dimension matches the expected mu and log_var\n",
        "            mu_log_var = encoded.view(-1, 2 * (self.latent_dim_classical + self.latent_dim_quantum))\n",
        "            mu = mu_log_var[:, :(self.latent_dim_classical + self.latent_dim_quantum)]\n",
        "            log_var = mu_log_var[:, (self.latent_dim_classical + self.latent_dim_quantum):]\n",
        "\n",
        "\n",
        "            # Split latent space for classical and quantum parts\n",
        "            mu_classical = mu[:, :self.latent_dim_classical]\n",
        "            log_var_classical = log_var[:, :self.latent_dim_classical]\n",
        "            mu_quantum = mu[:, self.latent_dim_classical:]\n",
        "            log_var_quantum = log_var[:, self.latent_dim_classical:]\n",
        "\n",
        "            # Reparameterization trick for classical latent space\n",
        "            z_classical = self.reparameterize(mu_classical, log_var_classical)\n",
        "\n",
        "            # Pass quantum latent space through quantum layer\n",
        "            # Apply reparameterization trick for quantum latent space inputs to the quantum circuit\n",
        "            z_quantum_input = self.reparameterize(mu_quantum, log_var_quantum)\n",
        "\n",
        "            # Process each sample through the quantum layer.\n",
        "            # The QNode with interface=\"torch\" should return a tensor for each sample.\n",
        "            # We then stack these tensors to form the batch output for the quantum part.\n",
        "            z_quantum_output_list = []\n",
        "            for i in range(z_quantum_input.size(0)):\n",
        "                # Each call to self.quantum_layer should return a tensor\n",
        "                quantum_output_sample = self.quantum_layer(z_quantum_input[i, :])\n",
        "                z_quantum_output_list.append(quantum_output_sample)\n",
        "\n",
        "            # Stack the list of tensors from the quantum layer outputs\n",
        "            z_quantum_output = torch.stack(z_quantum_output_list)\n",
        "\n",
        "\n",
        "            # Concatenate classical and quantum latent representations\n",
        "            z = torch.cat((z_classical, z_quantum_output), dim=1)\n",
        "\n",
        "            # Classical Decoder forward pass\n",
        "            reconstruction = self.decoder_classical(z)\n",
        "\n",
        "            return reconstruction, mu, log_var\n",
        "\n",
        "\n",
        "    model = HybridVAE(input_dim=X_train_tensor.shape[1],\n",
        "                      latent_dim_classical=classical_latent_dim,\n",
        "                      latent_dim_quantum=quantum_latent_dim)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Define the loss function (VAE loss: Reconstruction + KL Divergence)\n",
        "    def vae_loss(recon_x, x, mu, log_var):\n",
        "        BCE = F.mse_loss(recon_x, x, reduction='sum') # Using MSE for reconstruction loss\n",
        "        KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        return BCE + KLD\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        recon, mu, log_var = model(X_train_tensor)\n",
        "        loss = vae_loss(recon, X_train_tensor, mu, log_var)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / X_train_tensor.size(0)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Log training loss for the epoch\n",
        "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "\n",
        "    # Log the trained model\n",
        "    mlflow.pytorch.log_model(model, \"hybrid_vae_model\")\n",
        "\n",
        "# The MLflow run automatically ends when exiting the 'with' block"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected Tensor as element 0 in argument 0, but got list",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4029378332.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4029378332.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;31m# Stack the list of tensors from the quantum layer outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mz_quantum_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_quantum_output_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got list"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40263a59"
      },
      "source": [
        "## Implement training loop with mlops tracking feature\n",
        "\n",
        "### Fixed loop errors:\n",
        "Implement the training loop for the Hybrid VAE, integrating MLflow for experiment tracking. This is a retry of the previous failed subtask.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d4c7e6b"
      },
      "source": [
        "## Implemented inference, Define the inference process for the trained VAE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bee9785"
      },
      "source": [
        "# Implement inference\n",
        "\n",
        "'''\n",
        "Author: Arun Singh, arunsingh.in@gmail.com\n",
        "\n",
        "Review the previous error and the QNode definition. The error occurs because `torch.stack` in the `forward` method\n",
        "receives a list of lists. This suggests that `self.quantum_layer(z_quantum_input[i, :])` is returning a list for each\n",
        "sample `i`. The PennyLane QNode `quantum_circuit` is defined to return a list of `qml.expval` objects. While the\n",
        "`interface=\"torch\"` is supposed to convert this list of measurement processes to a tensor, it seems this conversion\n",
        "is not happening as expected within the loop structure, or `torch.stack` is being called on a list of these lists.\n",
        "To fix this, I will modify the `forward` method to pass the entire batch `z_quantum_input` to the `quantum_layer`.\n",
        "PennyLane's PyTorch interface should handle batching if the QNode is structured to accept batched inputs. I will also\n",
        " ensure the QNode is correctly defined to handle a batch by removing the explicit loop in the forward pass and relying\n",
        "  on the interface's batching capability.\n",
        "\n",
        "'''\n",
        "\n",
        "model.eval() # Set the model to evaluation mode\n",
        "with torch.no_grad(): # Disable gradient calculation for inference\n",
        "    # Encode the test data to get latent representations\n",
        "    encoded_test_data = model.encoder_classical(X_test_tensor)\n",
        "    mu_log_var_test = encoded_test_data.view(-1, 2, model.latent_dim_classical + model.latent_dim_quantum)\n",
        "    mu_test = mu_log_var_test[:, 0, :]\n",
        "    log_var_test = mu_log_var_test[:, 1, :]\n",
        "\n",
        "    # Split latent space for classical and quantum parts\n",
        "    mu_classical_test = mu_test[:, :model.latent_dim_classical]\n",
        "    log_var_classical_test = log_var_test[:, :model.latent_dim_classical]\n",
        "    mu_quantum_test = mu_test[:, model.latent_dim_classical:]\n",
        "    log_var_quantum_test = log_var_test[:, model.latent_dim_classical:]\n",
        "\n",
        "    # Reparameterization trick for classical latent space\n",
        "    z_classical_test = model.reparameterize(mu_classical_test, log_var_classical_test)\n",
        "\n",
        "    # Pass quantum latent space through quantum layer\n",
        "    # Apply reparameterization trick for quantum latent space inputs to the quantum circuit\n",
        "    z_quantum_input_test = model.reparameterize(mu_quantum_test, log_var_quantum_test)\n",
        "\n",
        "    # Pass the entire batch of quantum latent inputs to the quantum layer\n",
        "    z_quantum_output_test = model.quantum_layer(z_quantum_input_test)\n",
        "\n",
        "    # If the output is a list of tensors, stack it. If it's already a single tensor, keep it.\n",
        "    if isinstance(z_quantum_output_test, list):\n",
        "         z_quantum_output_test = torch.stack(z_quantum_output_test, dim=1)\n",
        "\n",
        "    # Ensure data types match before concatenation\n",
        "    z_quantum_output_test = z_quantum_output_test.float()\n",
        "\n",
        "    # Concatenate classical and quantum latent representations\n",
        "    z_test = torch.cat((z_classical_test, z_quantum_output_test), dim=1)\n",
        "\n",
        "    # Decode the latent representations to reconstruct the data\n",
        "    reconstructed_test_data = model.decoder_classical(z_test)\n",
        "\n",
        "# You can now use 'reconstructed_test_data' for further analysis or visualization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d644ef82"
      },
      "source": [
        "## Adding data versioning and model registry (MLOps)Incorporate data versioning using DVC and utilize MLflow for model registry."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10bea55d",
        "outputId": "56f628f0-8a9a-4f9c-e7f9-3d9c53050293"
      },
      "source": [
        "# Data Versioning with DVC (Illustrative - requires DVC setup outside this notebook)\n",
        "# This would typically involve:\n",
        "# 1. dvc init\n",
        "# 2. dvc add data/simulated_data.npy (assuming you save your simulated data)\n",
        "# 3. git add data/.gitignore data/simulated_data.npy.dvc\n",
        "# 4. git commit -m \"Add simulated data\"\n",
        "# 5. dvc push\n",
        "\n",
        "print(\"Data versioning with DVC would be configured outside this notebook.\")\n",
        "print(\"Assuming data is versioned at a specific DVC remote.\")\n",
        "\n",
        "# Model Registry with MLflow\n",
        "# The model was logged in the training loop.\n",
        "# To register it, you would typically use the MLflow UI or the API:\n",
        "# mlflow.register_model(\"runs:/<run_id>/hybrid_vae_model\", \"HybridVAEModel\")\n",
        "\n",
        "print(\"\\nModel logged to MLflow. Use MLflow UI or API to register the model.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data versioning with DVC would be configured outside this notebook.\n",
            "Assuming data is versioned at a specific DVC remote.\n",
            "\n",
            "Model logged to MLflow. Use MLflow UI or API to register the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98d16488"
      },
      "source": [
        "## Benchmark performance: Comparing the performance of the Quantum-Classical VAE with the classical AutoEncoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "b9c33787",
        "outputId": "229f67eb-7c33-42b8-f3de-1d3b5f400e6b"
      },
      "source": [
        "# To benchmark the Hybrid VAE, we need to define a similar benchmarking function\n",
        "# as used for the classical AutoEncoder, but applied to the HybridVAE model.\n",
        "\n",
        "def benchmark_hybrid_vae(model, X_train_tensor, X_test_tensor, optimizer, loss_fn, num_epochs=10, device=\"cpu\"):\n",
        "    model.to(device)\n",
        "    # Move data tensors to the specified device\n",
        "    X_train_dev = X_train_tensor.to(device)\n",
        "    X_test_dev = X_test_tensor.to(device)\n",
        "\n",
        "\n",
        "    start_train = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        recon, mu, log_var = model(X_train_dev)\n",
        "        # Ensure tensors passed to loss function are on the correct device\n",
        "        loss = vae_loss_benchmark(recon.to(device), X_train_dev.to(device), mu.to(device), log_var.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    end_train = time.time()\n",
        "\n",
        "    start_infer = time.time()\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # For inference benchmarking, we only need the forward pass without loss calculation\n",
        "        encoded_test_data = model.encoder_classical(X_test_dev)\n",
        "        mu_log_var_test = encoded_test_data.view(-1, 2, model.latent_dim_classical + model.latent_dim_quantum)\n",
        "        mu_test = mu_log_var_test[:, 0, :]\n",
        "        log_var_test = mu_log_var_test[:, 1, :]\n",
        "        z_classical_test = model.reparameterize(mu_classical_test, log_var_classical_test)\n",
        "        z_quantum_input_test = model.reparameterize(mu_quantum_test, log_var_quantum_test)\n",
        "\n",
        "        # Pass the entire batch of quantum latent inputs to the quantum layer\n",
        "        z_quantum_output_test = model.quantum_layer(z_quantum_input_test)\n",
        "\n",
        "        # Ensure data types match before concatenation and stack if necessary\n",
        "        if isinstance(z_quantum_output_test, list):\n",
        "             z_quantum_output_test = torch.stack(z_quantum_output_test, dim=1).float()\n",
        "        else:\n",
        "            z_quantum_output_test = z_quantum_output_test.float()\n",
        "\n",
        "        # Ensure both tensors are on the correct device before concatenation\n",
        "        z_classical_test = z_classical_test.to(device)\n",
        "        z_quantum_output_test = z_quantum_output_test.to(device)\n",
        "\n",
        "        z_test = torch.cat((z_classical_test, z_quantum_output_test), dim=1)\n",
        "        reconstructed_test_data = model.decoder_classical(z_test)\n",
        "\n",
        "    end_infer = time.time()\n",
        "\n",
        "    return end_train - start_train, end_infer - start_infer\n",
        "\n",
        "# Define loss function for benchmarking\n",
        "def vae_loss_benchmark(recon_x, x, mu, log_var):\n",
        "    BCE = F.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "# Initialize a new model and optimizer for benchmarking to ensure a clean run\n",
        "benchmark_model = HybridVAE(input_dim=X_train_tensor.shape[1],\n",
        "                            latent_dim_classical=classical_latent_dim,\n",
        "                            latent_dim_quantum=quantum_latent_dim)\n",
        "\n",
        "benchmark_optimizer = optim.Adam(benchmark_model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "# Run benchmark on CPU\n",
        "print(\"Benchmarking Hybrid VAE on CPU...\")\n",
        "hybrid_cpu_train, hybrid_cpu_infer = benchmark_hybrid_vae(benchmark_model,\n",
        "                                                           X_train_tensor,\n",
        "                                                           X_test_tensor,\n",
        "                                                           benchmark_optimizer,\n",
        "                                                           vae_loss_benchmark,\n",
        "                                                           num_epochs=num_epochs,\n",
        "                                                           device=\"cpu\")\n",
        "print(f\"CPU Training Time: {hybrid_cpu_train:.4f}s, CPU Inference Time: {hybrid_cpu_infer:.4f}s\")\n",
        "\n",
        "# Run benchmark on GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    print(\"\\nBenchmarking Hybrid VAE on GPU...\")\n",
        "    # Re-initialize model and optimizer for GPU benchmark\n",
        "    benchmark_model_gpu = HybridVAE(input_dim=X_train_tensor.shape[1],\n",
        "                                    latent_dim_classical=classical_latent_dim,\n",
        "                                    latent_dim_quantum=quantum_latent_dim)\n",
        "    benchmark_optimizer_gpu = optim.Adam(benchmark_model_gpu.parameters(), lr=learning_rate)\n",
        "\n",
        "    hybrid_gpu_train, hybrid_gpu_infer = benchmark_hybrid_vae(benchmark_model_gpu,\n",
        "                                                               X_train_tensor,\n",
        "                                                               X_test_tensor,\n",
        "                                                               benchmark_optimizer_gpu,\n",
        "                                                               vae_loss_benchmark,\n",
        "                                                               num_epochs=num_epochs,\n",
        "                                                               device=\"cuda\")\n",
        "    print(f\"GPU Training Time: {hybrid_gpu_train:.4f}s, GPU Inference Time: {hybrid_gpu_infer:.4f}s\")\n",
        "else:\n",
        "    hybrid_gpu_train, hybrid_gpu_infer = None, None\n",
        "    print(\"\\nGPU not available. Skipping GPU benchmark.\")\n",
        "\n",
        "# Store results\n",
        "hybrid_vae_benchmark_results = {\n",
        "    \"Device\": [\"CPU\"],\n",
        "    \"Training Time (s)\": [hybrid_cpu_train],\n",
        "    \"Inference Time (s)\": [hybrid_cpu_infer]\n",
        "}\n",
        "if torch.cuda.is_available():\n",
        "    hybrid_vae_benchmark_results[\"Device\"].append(\"GPU\")\n",
        "    hybrid_vae_benchmark_results[\"Training Time (s)\"].append(hybrid_gpu_train)\n",
        "    hybrid_vae_benchmark_results[\"Inference Time (s)\"].append(hybrid_gpu_infer)\n",
        "\n",
        "hybrid_vae_benchmark_df = pd.DataFrame(hybrid_vae_benchmark_results)\n",
        "\n",
        "display(hybrid_vae_benchmark_df)\n",
        "\n",
        "# Compare with classical AutoEncoder results (assuming they are available from previous cells)\n",
        "# classical_benchmark_df = pd.DataFrame({\n",
        "#     \"Device\": [\"CPU\", \"GPU\" if torch.cuda.is_available() else \"CPU\"],\n",
        "#     \"Training Time (s)\": [cpu_train, gpu_train],\n",
        "#     \"Inference Time (s)\": [cpu_infer, gpu_infer]\n",
        "# })\n",
        "# print(\"\\nClassical AutoEncoder Benchmark Results:\")\n",
        "# display(classical_benchmark_df)\n",
        "\n",
        "# print(\"\\nComparison:\")\n",
        "# comparison_df = pd.concat([classical_benchmark_df.assign(Model=\"Classical AutoEncoder\"),\n",
        "#                            hybrid_vae_benchmark_df.assign(Model=\"Hybrid VAE\")])\n",
        "# display(comparison_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Benchmarking Hybrid VAE on CPU...\n",
            "CPU Training Time: 0.5229s, CPU Inference Time: 0.0103s\n",
            "\n",
            "Benchmarking Hybrid VAE on GPU...\n",
            "GPU Training Time: 0.3002s, GPU Inference Time: 0.0096s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  Device  Training Time (s)  Inference Time (s)\n",
              "0    CPU           0.522933            0.010331\n",
              "1    GPU           0.300217            0.009568"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b1c3e984-4546-4f9f-bef6-515a717489f1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Device</th>\n",
              "      <th>Training Time (s)</th>\n",
              "      <th>Inference Time (s)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CPU</td>\n",
              "      <td>0.522933</td>\n",
              "      <td>0.010331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GPU</td>\n",
              "      <td>0.300217</td>\n",
              "      <td>0.009568</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b1c3e984-4546-4f9f-bef6-515a717489f1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b1c3e984-4546-4f9f-bef6-515a717489f1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b1c3e984-4546-4f9f-bef6-515a717489f1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7cdd9744-401f-4fba-8e75-4cb914cf17fc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7cdd9744-401f-4fba-8e75-4cb914cf17fc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7cdd9744-401f-4fba-8e75-4cb914cf17fc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_3db43d1c-2e71-4b17-be0c-a4f530e186a6\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('hybrid_vae_benchmark_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3db43d1c-2e71-4b17-be0c-a4f530e186a6 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('hybrid_vae_benchmark_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "hybrid_vae_benchmark_df",
              "summary": "{\n  \"name\": \"hybrid_vae_benchmark_df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"Device\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"GPU\",\n          \"CPU\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Training Time (s)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.15748372250968407,\n        \"min\": 0.3002169132232666,\n        \"max\": 0.5229325294494629,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.3002169132232666,\n          0.5229325294494629\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Inference Time (s)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0005396482483334872,\n        \"min\": 0.009567737579345703,\n        \"max\": 0.010330915451049805,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.009567737579345703,\n          0.010330915451049805\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36bbf74c"
      },
      "source": [
        "## Visualize the results of the benchmarking and the VAE's performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "id": "829fc2a3",
        "outputId": "9eef9062-d7e7-4c6e-8d64-473e1c48d25c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Combine the benchmark results for comparison\n",
        "# Assuming 'classical_benchmark_df' is available from previous cells in the original notebook\n",
        "# If not, you might need to re-run the classical benchmark or manually create the DataFrame\n",
        "\n",
        "# For demonstration purposes, assuming classical_benchmark_df exists and has the structure:\n",
        "# Device | Training Time (s) | Inference Time (s)\n",
        "# CPU    | ...               | ...\n",
        "# GPU    | ...               | ...\n",
        "\n",
        "classical_benchmark_results = {\n",
        "    \"Device\": [\"CPU\", \"GPU\" if torch.cuda.is_available() else \"CPU\"],\n",
        "    \"Training Time (s)\": [cpu_train, gpu_train if 'gpu_train' in locals() else None],\n",
        "    \"Inference Time (s)\": [cpu_infer, gpu_infer if 'gpu_infer' in locals() else None]\n",
        "}\n",
        "classical_benchmark_df = pd.DataFrame(classical_benchmark_results)\n",
        "\n",
        "\n",
        "comparison_df = pd.concat([classical_benchmark_df.assign(Model=\"Classical AutoEncoder\"),\n",
        "                           hybrid_vae_benchmark_df.assign(Model=\"Hybrid VAE\")])\n",
        "\n",
        "print(\"Benchmark Comparison:\")\n",
        "display(comparison_df)\n",
        "\n",
        "# Visualize the comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Training Time Comparison\n",
        "sns.barplot(x=\"Device\", y=\"Training Time (s)\", hue=\"Model\", data=comparison_df, ax=axes[0])\n",
        "axes[0].set_title(\"Training Time Comparison (CPU vs GPU)\")\n",
        "axes[0].set_ylabel(\"Time (s)\")\n",
        "\n",
        "# Inference Time Comparison\n",
        "sns.barplot(x=\"Device\", y=\"Inference Time (s)\", hue=\"Model\", data=comparison_df, ax=axes[1])\n",
        "axes[1].set_title(\"Inference Time Comparison (CPU vs GPU)\")\n",
        "axes[1].set_ylabel(\"Time (s)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Benchmark Comparison:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  Device  Training Time (s)  Inference Time (s)                  Model\n",
              "0    CPU           0.157445            0.000817  Classical AutoEncoder\n",
              "1    GPU           0.517496            0.000548  Classical AutoEncoder\n",
              "0    CPU           0.522933            0.010331             Hybrid VAE\n",
              "1    GPU           0.300217            0.009568             Hybrid VAE"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2dab4ad5-268b-4e63-95d9-1efb3413e7b9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Device</th>\n",
              "      <th>Training Time (s)</th>\n",
              "      <th>Inference Time (s)</th>\n",
              "      <th>Model</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CPU</td>\n",
              "      <td>0.157445</td>\n",
              "      <td>0.000817</td>\n",
              "      <td>Classical AutoEncoder</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GPU</td>\n",
              "      <td>0.517496</td>\n",
              "      <td>0.000548</td>\n",
              "      <td>Classical AutoEncoder</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CPU</td>\n",
              "      <td>0.522933</td>\n",
              "      <td>0.010331</td>\n",
              "      <td>Hybrid VAE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GPU</td>\n",
              "      <td>0.300217</td>\n",
              "      <td>0.009568</td>\n",
              "      <td>Hybrid VAE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2dab4ad5-268b-4e63-95d9-1efb3413e7b9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2dab4ad5-268b-4e63-95d9-1efb3413e7b9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2dab4ad5-268b-4e63-95d9-1efb3413e7b9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7564ea9a-b6c1-4edd-9dec-7eaf203cbe4a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7564ea9a-b6c1-4edd-9dec-7eaf203cbe4a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7564ea9a-b6c1-4edd-9dec-7eaf203cbe4a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_134066d9-9a34-45ee-b5fd-b7dcc25aa787\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('comparison_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_134066d9-9a34-45ee-b5fd-b7dcc25aa787 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('comparison_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "comparison_df",
              "summary": "{\n  \"name\": \"comparison_df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Device\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"GPU\",\n          \"CPU\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Training Time (s)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.17805498316772356,\n        \"min\": 0.15744519233703613,\n        \"max\": 0.5229325294494629,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.5174956321716309,\n          0.3002169132232666\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Inference Time (s)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.005360344305587593,\n        \"min\": 0.0005478858947753906,\n        \"max\": 0.010330915451049805,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0005478858947753906,\n          0.009567737579345703\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Hybrid VAE\",\n          \"Classical AutoEncoder\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ed1714f"
      },
      "source": [
        "### Analysis of Benchmark Results\n",
        "\n",
        "Author: Arun Singh | arunsingh.in@gmail.com\n",
        "\n",
        "The visualizations above compare the training and inference times of the classical AutoEncoder and the Hybrid Quantum-Classical VAE on both CPU and GPU.\n",
        "\n",
        "**Key Observations:**\n",
        "\n",
        "*   **Training Time:** Observe how the training times compare between the classical and hybrid models on each device. Quantum circuits can be computationally intensive, which might impact training time, especially on classical simulators.\n",
        "*   **Inference Time:** Compare the inference times. For certain tasks or hardware, the quantum part might introduce overhead or, conversely, offer speedups.\n",
        "*   **CPU vs GPU:** Analyze the performance differences between CPU and GPU for each model. GPUs are generally expected to accelerate classical neural network computations. The impact on the hybrid model will depend on how efficiently the quantum operations are simulated or executed on the available hardware.\n",
        "\n",
        "These results provide insights into the computational cost of incorporating a quantum layer into the VAE architecture and the benefits of using a GPU for both classical and hybrid models.\n",
        "\n",
        "* * *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6910ada5"
      },
      "source": [
        "## Adding documentation and explanations\n",
        "\n",
        "Include markdown cells to explain the code, the hybrid VAE concept, and the MLOps practices implemented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0242ce91"
      },
      "source": [
        "This notebook extends the initial exploration of AutoEncoders for single-cell data by introducing a **Hybrid Quantum-Classical Variational AutoEncoder (VAE)**. This architecture combines classical neural network layers with a quantum circuit to explore the potential benefits of quantum computation for this task.\n",
        "\n",
        "We have also incorporated **MLOps best practices** throughout the development process, following principles outlined at [https://ml-ops.org/content/mlops-principles](https://ml-ops.org/content/mlops-principles). This includes:\n",
        "\n",
        "*   **Experiment Tracking:** Using MLflow to log hyperparameters, metrics (like training loss), and the trained model, enabling reproducibility and comparison of different runs.\n",
        "*   **Data Versioning (Illustrative):** Mentioning the use of DVC for versioning the simulated data, which is crucial for maintaining a history of data used for training and ensuring reproducibility.\n",
        "*   **Model Registry:** Logging the trained model with MLflow for potential registration and management.\n",
        "\n",
        "The Hybrid VAE architecture consists of:\n",
        "\n",
        "1.  **Classical Encoder:** A standard neural network that processes the input data and outputs parameters (mean and log-variance) for both the classical and quantum parts of the latent space.\n",
        "2.  **Quantum Layer:** A parameterized quantum circuit that takes the quantum latent space parameters as input and performs quantum operations. The expectation values of measurements on the quantum circuit form the quantum part of the latent representation.\n",
        "3.  **Classical Decoder:** Another standard neural network that takes the combined classical and quantum latent representation and reconstructs the original input data.\n",
        "\n",
        "The training process aims to minimize a VAE loss function, which typically includes a reconstruction loss (measuring how well the VAE reconstructs the input) and a Kullback-Leibler (KL) divergence term (regularizing the latent space distribution).\n",
        "\n",
        "The benchmarking results provide an initial comparison of the Hybrid VAE's performance against the classical AutoEncoder in terms of training and inference time on different hardware. These results highlight the current computational cost of simulating quantum circuits and the potential benefits of specialized hardware (like GPUs) for accelerating both classical and hybrid models. Further research and advancements in quantum hardware and simulation techniques are expected to improve the performance of quantum-enhanced models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b74c256"
      },
      "source": [
        "## Summary\n",
        "Summarize the findings and concluding the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "473ce593"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "This notebook successfully demonstrates the implementation of a Hybrid Quantum-Classical Variational AutoEncoder for single-cell-like data and integrates key MLOps practices using MLflow and DVC (illustrative).\n",
        "\n",
        "We have:\n",
        "\n",
        "*   Defined and implemented the Hybrid VAE architecture.\n",
        "*   Set up a training loop with MLflow tracking for hyperparameters and metrics.\n",
        "*   Implemented the inference process.\n",
        "*   Incorporated data versioning and model registry principles.\n",
        "*   Benchmarked the performance of the Hybrid VAE on CPU and GPU and compared it with a classical AutoEncoder.\n",
        "*   Visualized and analyzed the benchmarking results.\n",
        "\n",
        "The benchmarking results show that the Hybrid VAE, as implemented with a classical simulator, has higher training and inference times compared to the classical AutoEncoder. However, this is a preliminary exploration, and the performance of quantum-enhanced models is expected to improve with advancements in quantum hardware and more optimized hybrid algorithms.\n",
        "\n",
        "This notebook serves as a foundation for further exploration of quantum machine learning models for single-cell analysis and the application of MLOps principles in this emerging field. Future work could involve exploring different quantum circuit architectures, leveraging actual quantum hardware (when accessible), and implementing more advanced MLOps workflows for deployment and monitoring."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeba9903"
      },
      "source": [
        "# Inculcating best practices, Monitoring, MELT, SRE golden Signals\n",
        "Executing MLOps best practices on hybrid Quantum-Classical VAE, following MLOps best practices from \"https://ml-ops.org/content/mlops-principles\". Implementing a training loop with MLOps tracking using Weights & Biases or MLflow. Fix any errors encountered. Add detailed documentation and explanations. Include coding instrumentation best practices and set up monitoring (Metrics, Events, Logs, Traces) using SRE golden signals for end-user research outcomes. Finally, provide a summary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "357ee2a9"
      },
      "source": [
        "## Introduce coding instrumentation\n",
        "\n",
        "Explain the importance of adding instrumentation to the code for better observability.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "7c824282",
        "outputId": "69f9aefd-e477-4bdb-df8d-95260d8f9f43"
      },
      "source": [
        "## 📊 Code Instrumentation for Observability\n",
        "\n",
        "\n",
        "'''\n",
        "The importance of code instrumentation for observability in the context of MLOps, focusing on\n",
        "debugging, performance monitoring, and understanding the quantum component's impact.\n",
        "'''\n",
        "In the realm of MLOps, particularly with complex and novel architectures like the Hybrid Quantum-Classical VAE, **code instrumentation** is paramount for achieving **observability**. Observability refers to the ability to understand the internal state of a system based on external data it generates.\n",
        "\n",
        "**Why is Instrumentation Crucial?**\n",
        "\n",
        "1.  **Debugging Complex Models:** Hybrid models introduce unique challenges. Instrumentation allows us to track the flow of data through both classical and quantum layers, pinpointing where issues might arise. This is essential for debugging errors that could originate from the interaction between the two domains.\n",
        "2.  **Performance Monitoring:** Beyond simple execution time (as benchmarked earlier), instrumentation enables fine-grained monitoring of performance within different parts of the model. We can track metrics like:\n",
        "    *   Latency of the quantum circuit execution.\n",
        "    *   Computational cost of specific classical layers.\n",
        "    *   Memory usage during training and inference.\n",
        "    This helps identify bottlenecks and optimize resource allocation.\n",
        "3.  **Understanding Quantum Impact:** For a Hybrid VAE, understanding the contribution and behavior of the quantum layer is vital. Instrumentation can provide insights into:\n",
        "    *   The range and distribution of expectation values from the quantum circuit.\n",
        "    *   How changes in quantum circuit parameters affect the overall latent space and reconstruction.\n",
        "    *   The impact of quantum noise or simulation inaccuracies on model performance.\n",
        "4.  **Reproducibility and Reliability:** By logging key metrics and events through instrumentation, we create a detailed record of each experiment run. This is fundamental for MLOps, allowing us to:\n",
        "    *   Reproduce specific results.\n",
        "    *   Compare different model versions or training configurations effectively.\n",
        "    *   Ensure the reliability of the deployed model by monitoring its performance in production.\n",
        "\n",
        "In essence, instrumentation transforms our code from a black box into a transparent system, providing the necessary data to debug effectively, monitor performance, gain scientific insights, and build robust, reliable ML systems."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1044405644.py, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1044405644.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    In the realm of MLOps, particularly with complex and novel architectures like the Hybrid Quantum-Classical VAE, **code instrumentation** is paramount for achieving **observability**. Observability refers to the ability to understand the internal state of a system based on external data it generates.\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "wz2wkKZAR1Ds",
        "outputId": "080d0500-6dab-4234-cb6b-b2a9336e629d"
      },
      "source": [
        "```markdown\n",
        "## 📊 Code Instrumentation for Observability\n",
        "\n",
        "'''\n",
        "he previous command failed because a markdown cell was incorrectly placed within a\n",
        "`code_block`. To fix this, I need to generate a markdown cell directly.\n",
        "'''\n",
        "\n",
        "In the realm of MLOps, particularly with complex and novel architectures like the Hybrid Quantum-Classical VAE, **code instrumentation** is paramount for achieving **observability**. Observability refers to the ability to understand the internal state of a system based on external data it generates.\n",
        "\n",
        "**Why is Instrumentation Crucial?**\n",
        "\n",
        "1.  **Debugging Complex Models:** Hybrid models introduce unique challenges. Instrumentation allows us to track the flow of data through both classical and quantum layers, pinpointing where issues might arise. This is essential for debugging errors that could originate from the interaction between the two domains.\n",
        "2.  **Performance Monitoring:** Beyond simple execution time (as benchmarked earlier), instrumentation enables fine-grained monitoring of performance within different parts of the model. We can track metrics like:\n",
        "    *   Latency of the quantum circuit execution.\n",
        "    *   Computational cost of specific classical layers.\n",
        "    *   Memory usage during training and inference.\n",
        "    This helps identify bottlenecks and optimize resource allocation.\n",
        "3.  **Understanding Quantum Impact:** For a Hybrid VAE, understanding the contribution and behavior of the quantum layer is vital. Instrumentation can provide insights into:\n",
        "    *   The range and distribution of expectation values from the quantum circuit.\n",
        "    *   How changes in quantum circuit parameters affect the overall latent space and reconstruction.\n",
        "    *   The impact of quantum noise or simulation inaccuracies on model performance.\n",
        "4.  **Reproducibility and Reliability:** By logging key metrics and events through instrumentation, we create a detailed record of each experiment run. This is fundamental for MLOps, allowing us to:\n",
        "    *   Reproduce specific results.\n",
        "    *   Compare different model versions or training configurations effectively.\n",
        "    *   Ensure the reliability of the deployed model by monitoring its performance in production.\n",
        "\n",
        "In essence, instrumentation transforms our code from a black box into a transparent system, providing the necessary data to debug effectively, monitor performance, gain scientific insights, and build robust, reliable ML systems."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-4269402933.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-4269402933.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ```markdown\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "BXG_q95jR3oZ",
        "outputId": "4ff5a3a3-98f3-42ce-8539-54b8d514e3b4"
      },
      "source": [
        "## 📊 Code Instrumentation for Observability\n",
        "\n",
        "'''\n",
        "The previous attempts to create a markdown cell using code_block with markdown syntax inside failed.\n",
        " I need to output the markdown content directly without the code block delimiters or the markdown tag.\n",
        "\n",
        "'''\n",
        "\n",
        "In the realm of MLOps, particularly with complex and novel architectures like the Hybrid Quantum-Classical VAE, **code instrumentation** is paramount for achieving **observability**. Observability refers to the ability to understand the internal state of a system based on external data it generates.\n",
        "\n",
        "**Why is Instrumentation Crucial?**\n",
        "\n",
        "1.  **Debugging Complex Models:** Hybrid models introduce unique challenges. Instrumentation allows us to track the flow of data through both classical and quantum layers, pinpointing where issues might arise. This is essential for debugging errors that could originate from the interaction between the two domains.\n",
        "2.  **Performance Monitoring:** Beyond simple execution time (as benchmarked earlier), instrumentation enables fine-grained monitoring of performance within different parts of the model. We can track metrics like:\n",
        "    *   Latency of the quantum circuit execution.\n",
        "    *   Computational cost of specific classical layers.\n",
        "    *   Memory usage during training and inference.\n",
        "    This helps identify bottlenecks and optimize resource allocation.\n",
        "3.  **Understanding Quantum Impact:** For a Hybrid VAE, understanding the contribution and behavior of the quantum layer is vital. Instrumentation can provide insights into:\n",
        "    *   The range and distribution of expectation values from the quantum circuit.\n",
        "    *   How changes in quantum circuit parameters affect the overall latent space and reconstruction.\n",
        "    *   The impact of quantum noise or simulation inaccuracies on model performance.\n",
        "4.  **Reproducibility and Reliability:** By logging key metrics and events through instrumentation, we create a detailed record of each experiment run. This is fundamental for MLOps, allowing us to:\n",
        "    *   Reproduce specific results.\n",
        "    *   Compare different model versions or training configurations effectively.\n",
        "    *   Ensure the reliability of the deployed model by monitoring its performance in production.\n",
        "\n",
        "In essence, instrumentation transforms our code from a black box into a transparent system, providing the necessary data to debug effectively, monitor performance, gain scientific insights, and build robust, reliable ML systems."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1044405644.py, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1044405644.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    In the realm of MLOps, particularly with complex and novel architectures like the Hybrid Quantum-Classical VAE, **code instrumentation** is paramount for achieving **observability**. Observability refers to the ability to understand the internal state of a system based on external data it generates.\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "d8JBKeEjR6fx",
        "outputId": "f4324358-abbc-4006-c829-2f8220f1082f"
      },
      "source": [
        "```markdown\n",
        "## 📊 Code Instrumentation for Observability\n",
        "\n",
        "\n",
        "'''\n",
        "Error fixed: The previous attempts to create a markdown cell failed due to incorrect formatting within the code_block command.\n",
        " To generate a markdown cell, the content should be directly provided without any enclosing code block syntax.\n",
        "'''\n",
        "\n",
        "In the realm of MLOps, particularly with complex and novel architectures like the Hybrid Quantum-Classical VAE, **code instrumentation** is paramount for achieving **observability**. Observability refers to the ability to understand the internal state of a system based on external data it generates.\n",
        "\n",
        "**Why is Instrumentation Crucial?**\n",
        "\n",
        "1.  **Debugging Complex Models:** Hybrid models introduce unique challenges. Instrumentation allows us to track the flow of data through both classical and quantum layers, pinpointing where issues might arise. This is essential for debugging errors that could originate from the interaction between the two domains.\n",
        "2.  **Performance Monitoring:** Beyond simple execution time (as benchmarked earlier), instrumentation enables fine-grained monitoring of performance within different parts of the model. We can track metrics like:\n",
        "    *   Latency of the quantum circuit execution.\n",
        "    *   Computational cost of specific classical layers.\n",
        "    *   Memory usage during training and inference.\n",
        "    This helps identify bottlenecks and optimize resource allocation.\n",
        "3.  **Understanding Quantum Impact:** For a Hybrid VAE, understanding the contribution and behavior of the quantum layer is vital. Instrumentation can provide insights into:\n",
        "    *   The range and distribution of expectation values from the quantum circuit.\n",
        "    *   How changes in quantum circuit parameters affect the overall latent space and reconstruction.\n",
        "    *   The impact of quantum noise or simulation inaccuracies on model performance.\n",
        "4.  **Reproducibility and Reliability:** By logging key metrics and events through instrumentation, we create a detailed record of each experiment run. This is fundamental for MLOps, allowing us to:\n",
        "    *   Reproduce specific results.\n",
        "    *   Compare different model versions or training configurations effectively.\n",
        "    *   Ensure the reliability of the deployed model by monitoring its performance in production.\n",
        "\n",
        "In essence, instrumentation transforms our code from a black box into a transparent system, providing the necessary data to debug effectively, monitor performance, gain scientific insights, and build robust, reliable ML systems."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-4269402933.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-4269402933.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ```markdown\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "4FeSqyn0R8tW",
        "outputId": "c80306dd-dad1-4ee9-9622-d40fc86c9c7d"
      },
      "source": [
        "## 📊 Code Instrumentation for Observability\n",
        "\n",
        "In the realm of MLOps, particularly with complex and novel architectures like the Hybrid Quantum-Classical VAE, **code instrumentation** is paramount for achieving **observability**. Observability refers to the ability to understand the internal state of a system based on external data it generates.\n",
        "\n",
        "**Why is Instrumentation Crucial?**\n",
        "\n",
        "1.  **Debugging Complex Models:** Hybrid models introduce unique challenges. Instrumentation allows us to track the flow of data through both classical and quantum layers, pinpointing where issues might arise. This is essential for debugging errors that could originate from the interaction between the two domains.\n",
        "2.  **Performance Monitoring:** Beyond simple execution time (as benchmarked earlier), instrumentation enables fine-grained monitoring of performance within different parts of the model. We can track metrics like:\n",
        "    *   Latency of the quantum circuit execution.\n",
        "    *   Computational cost of specific classical layers.\n",
        "    *   Memory usage during training and inference.\n",
        "    This helps identify bottlenecks and optimize resource allocation.\n",
        "3.  **Understanding Quantum Impact:** For a Hybrid VAE, understanding the contribution and behavior of the quantum layer is vital. Instrumentation can provide insights into:\n",
        "    *   The range and distribution of expectation values from the quantum circuit.\n",
        "    *   How changes in quantum circuit parameters affect the overall latent space and reconstruction.\n",
        "    *   The impact of quantum noise or simulation inaccuracies on model performance.\n",
        "4.  **Reproducibility and Reliability:** By logging key metrics and events through instrumentation, we create a detailed record of each experiment run. This is fundamental for MLOps, allowing us to:\n",
        "    *   Reproduce specific results.\n",
        "    *   Compare different model versions or training configurations effectively.\n",
        "    *   Ensure the reliability of the deployed model by monitoring its performance in production.\n",
        "\n",
        "In essence, instrumentation transforms our code from a black box into a transparent system, providing the necessary data to debug effectively, monitor performance, gain scientific insights, and build robust, reliable ML systems."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1044405644.py, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1044405644.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    In the realm of MLOps, particularly with complex and novel architectures like the Hybrid Quantum-Classical VAE, **code instrumentation** is paramount for achieving **observability**. Observability refers to the ability to understand the internal state of a system based on external data it generates.\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2068d729"
      },
      "source": [
        "## Instrument the code\n",
        "\n",
        "\n",
        "Adding illustrative code snippets to demonstrate how to add logging, metrics, and potentially tracing to the Hybrid VAE training and inference process.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23421b1c",
        "outputId": "8f4f065b-edf7-4af1-9fd8-34c2eea8b4ab"
      },
      "source": [
        "import logging\n",
        "\n",
        "'''\n",
        "Author: Arun Singh, arunsingh.in@gmail.com\n",
        "\n",
        "Coding Instrumentation:\n",
        "Adding illustrative code snippets to demonstrate how to add logging, metrics, and potentially\n",
        "tracing to the Hybrid VAE training and inference process.\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Simple dictionary to simulate metrics collection\n",
        "metrics = {}\n",
        "\n",
        "# --- Training Loop with Instrumentation ---\n",
        "mlflow.start_run() # Ensure a run is active if not already\n",
        "\n",
        "# Log hyperparameters (already done, but showing again for context)\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "classical_latent_dim = 32\n",
        "quantum_latent_dim = 4\n",
        "\n",
        "mlflow.log_param(\"learning_rate\", learning_rate)\n",
        "mlflow.log_param(\"num_epochs\", num_epochs)\n",
        "mlflow.log_param(\"classical_latent_dim\", classical_latent_dim)\n",
        "mlflow.log_param(\"quantum_latent_dim\", quantum_latent_dim)\n",
        "\n",
        "model = HybridVAE(input_dim=X_train_tensor.shape[1],\n",
        "                  latent_dim_classical=classical_latent_dim,\n",
        "                  latent_dim_quantum=quantum_latent_dim)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def vae_loss(recon_x, x, mu, log_var):\n",
        "    BCE = F.mse_loss(recon_x, x, reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "    return BCE + KLD\n",
        "\n",
        "logging.info(\"Starting Hybrid VAE training...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Instrumentation: Log start of epoch and data shape\n",
        "    logging.info(f\"Epoch {epoch+1}/{num_epochs} started. Training data shape: {X_train_tensor.shape}\")\n",
        "\n",
        "    # In a real scenario with batches, you would iterate through a DataLoader\n",
        "    # For this example, we process the whole tensor as a single batch\n",
        "    recon, mu, log_var = model(X_train_tensor)\n",
        "    loss = vae_loss(recon, X_train_tensor, mu, log_var)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    # Instrumentation: Log gradient norms (illustrative - requires iterating through parameters)\n",
        "    # logging.debug(\"Logging gradient norms...\")\n",
        "    # for name, param in model.named_parameters():\n",
        "    #     if param.grad is not None:\n",
        "    #         logging.debug(f\"  {name} gradient norm: {param.grad.norm().item():.4f}\")\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = train_loss / X_train_tensor.size(0)\n",
        "\n",
        "    # Instrumentation: Log epoch loss\n",
        "    logging.info(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Instrumentation: Collect and log metrics\n",
        "    if epoch not in metrics:\n",
        "        metrics[epoch] = {}\n",
        "    metrics[epoch]['train_loss'] = avg_train_loss\n",
        "    mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "\n",
        "logging.info(\"Hybrid VAE training finished.\")\n",
        "\n",
        "# Log the trained model\n",
        "mlflow.pytorch.log_model(model, \"hybrid_vae_model\")\n",
        "\n",
        "mlflow.end_run() # End the MLflow run\n",
        "\n",
        "# --- Inference with Instrumentation ---\n",
        "\n",
        "logging.info(\"Starting Hybrid VAE inference...\")\n",
        "\n",
        "model.eval() # Set the model to evaluation mode\n",
        "with torch.no_grad(): # Disable gradient calculation for inference\n",
        "    start_inference_time = time.time()\n",
        "\n",
        "    # Encode the test data\n",
        "    encoded_test_data = model.encoder_classical(X_test_tensor)\n",
        "    mu_log_var_test = encoded_test_data.view(-1, 2, model.latent_dim_classical + model.latent_dim_quantum)\n",
        "    mu_test = mu_log_var_test[:, 0, :]\n",
        "    log_var_test = mu_log_var_test[:, 1, :]\n",
        "\n",
        "    z_classical_test = model.reparameterize(mu_classical_test, log_var_classical_test)\n",
        "    z_quantum_input_test = model.reparameterize(mu_quantum_test, log_var_quantum_test)\n",
        "\n",
        "    # Pass the entire batch of quantum latent inputs to the quantum layer\n",
        "    # Instrumentation: Log the shape of the input to the quantum layer\n",
        "    logging.info(f\"Quantum layer input shape: {z_quantum_input_test.shape}\")\n",
        "    z_quantum_output_test = model.quantum_layer(z_quantum_input_test)\n",
        "\n",
        "    # If the output is a list of tensors, stack it.\n",
        "    if isinstance(z_quantum_output_test, list):\n",
        "         z_quantum_output_test = torch.stack(z_quantum_output_test, dim=1)\n",
        "\n",
        "    z_quantum_output_test = z_quantum_output_test.float()\n",
        "\n",
        "    # Concatenate classical and quantum latent representations\n",
        "    z_test = torch.cat((z_classical_test, z_quantum_output_test), dim=1)\n",
        "\n",
        "    # Decode the latent representations\n",
        "    reconstructed_test_data = model.decoder_classical(z_test)\n",
        "\n",
        "    end_inference_time = time.time()\n",
        "    inference_duration = end_inference_time - start_inference_time\n",
        "\n",
        "    # Instrumentation: Log inference duration and data shape\n",
        "    logging.info(f\"Inference finished in {inference_duration:.4f} seconds. Reconstructed data shape: {reconstructed_test_data.shape}\")\n",
        "\n",
        "    # Instrumentation: Calculate and log reconstruction error (illustrative)\n",
        "    reconstruction_error = F.mse_loss(reconstructed_test_data, X_test_tensor, reduction='mean').item()\n",
        "    logging.info(f\"Inference Reconstruction Error (MSE): {reconstruction_error:.4f}\")\n",
        "\n",
        "    # Instrumentation: Collect and log inference metrics\n",
        "    metrics['inference_duration'] = inference_duration\n",
        "    metrics['inference_reconstruction_error'] = reconstruction_error\n",
        "    # If using MLflow for inference metrics, log here:\n",
        "    # with mlflow.start_run(run_name=\"Inference Benchmark\"):\n",
        "    #     mlflow.log_metric(\"inference_duration_s\", inference_duration)\n",
        "    #     mlflow.log_metric(\"inference_reconstruction_error\", reconstruction_error)\n",
        "\n",
        "\n",
        "# Instrumentation: Tracing (Illustrative)\n",
        "# For more detailed performance analysis, tracing tools (like PyTorch's profiler,\n",
        "# or integration with OpenTelemetry and a tracing backend like Jaeger) could be used.\n",
        "# This would allow visualizing the execution flow and timing of operations within\n",
        "# the forward and backward passes, including the quantum circuit execution.\n",
        "# Example (conceptual):\n",
        "# with torch.profiler.profile(...) as prof:\n",
        "#     recon, mu, log_var = model(X_train_tensor)\n",
        "#     loss = vae_loss(recon, X_train_tensor, mu, log_var)\n",
        "#     loss.backward()\n",
        "# prof.export_chrome_trace(\"trace.json\")\n",
        "\n",
        "\n",
        "print(\"\\nInstrumentation added to training and inference loops.\")\n",
        "print(\"Example metrics collected (simulated):\")\n",
        "print(metrics)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/04 18:11:38 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/08/04 18:11:38 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu124) contains a local version label (+cu124). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "2025/08/04 18:11:44 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu124) contains a local version label (+cu124). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "\u001b[31m2025/08/04 18:11:44 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Instrumentation added to training and inference loops.\n",
            "Example metrics collected (simulated):\n",
            "{0: {'train_loss': 1560.495875}, 1: {'train_loss': 1553.114125}, 2: {'train_loss': 1544.734625}, 3: {'train_loss': 1538.12}, 4: {'train_loss': 1531.7535}, 5: {'train_loss': 1526.787625}, 6: {'train_loss': 1521.26675}, 7: {'train_loss': 1516.598375}, 8: {'train_loss': 1511.485125}, 9: {'train_loss': 1506.995375}, 'inference_duration': 0.01080179214477539, 'inference_reconstruction_error': 15.446125984191895}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40503b6f"
      },
      "source": [
        "## Added monitoring to quantum VAE\n",
        "\n",
        "Introduce the key concepts of monitoring, including Metrics, Events, Logs, and Traces, and their relevance to MLOps and understanding model behavior.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "15694365",
        "outputId": "42b4bc9d-86f1-4a10-f595-3619393b4987"
      },
      "source": [
        "### 📊 Monitoring Concepts\n",
        "\n",
        "Monitoring is a critical component of MLOps, providing the necessary visibility into the performance and health of machine learning models and the systems they run on. It allows us to understand how our Hybrid VAE is behaving in various environments and identify potential issues early. Key monitoring concepts include:\n",
        "\n",
        "1.  **Metrics:** Quantitative measurements that track the performance, health, and resource utilization of a system or model over time.\n",
        "    *   **What they are:** Numerical values collected at regular intervals (e.g., CPU usage, memory consumption, inference latency, model accuracy, training loss, KL divergence loss).\n",
        "    *   **Relevance in MLOps:** Metrics are essential for tracking the success of experiments (e.g., comparing loss curves across different runs), monitoring the health of deployed models (e.g., detecting performance degradation or drift), and understanding resource usage for cost optimization. For the Hybrid VAE, specific metrics could include the distribution of values from the quantum layer outputs or the time taken for the quantum circuit to execute.\n",
        "    *   **Comprehensive View:** Metrics provide a high-level overview of system and model behavior, highlighting trends and anomalies.\n",
        "\n",
        "2.  **Events:** Discrete occurrences that happen within the system at a specific point in time.\n",
        "    *   **What they are:** Records of significant actions or changes in state (e.g., model deployment started, training epoch completed, error occurred, specific data threshold crossed, quantum circuit execution failed).\n",
        "    *   **Relevance in MLOps:** Events help correlate changes in metrics with specific actions. They provide context for understanding why a metric might have changed (e.g., a spike in inference latency might correlate with a new model version deployment event). For the Hybrid VAE, events could log when the quantum device is initialized or when a specific gate operation is attempted.\n",
        "    *   **Comprehensive View:** Events provide timestamps and context for understanding the sequence of operations and changes in the system.\n",
        "\n",
        "3.  **Logs:** Text-based records generated by applications and systems, providing detailed information about their operation.\n",
        "    *   **What they are:** Chronological records of activities, status updates, warnings, and errors generated by the code (e.g., `logging.info(\"Training started\")`, `logging.warning(\"High loss detected\")`, error tracebacks).\n",
        "    *   **Relevance in MLOps:** Logs are invaluable for debugging. When an issue is detected via metrics or events, logs provide the detailed step-by-step information needed to diagnose the root cause. For the Hybrid VAE, logs can track the values of intermediate tensors, the inputs and outputs of the quantum circuit, or details about the quantum simulation process.\n",
        "    *   **Comprehensive View:** Logs offer fine-grained details about the internal workings of the model and infrastructure, crucial for troubleshooting.\n",
        "\n",
        "4.  **Traces:** Representations of the request flow through a distributed system, showing the sequence of operations and their timing.\n",
        "    *   **What they are:** Records that track a single request or transaction as it propagates through various components of a system, capturing the latency and metadata of each step.\n",
        "    *   **Relevance in MLOps:** While less common for a single-model notebook like this, traces are vital in production environments where inference requests might pass through multiple services (e.g., API gateway, pre-processing service, the VAE model service, post-processing). They help identify performance bottlenecks and failures across the entire request path. For a Hybrid VAE served as part of a larger pipeline, tracing could show the time spent in classical pre-processing, the quantum computation service, and classical post-processing.\n",
        "    *   **Comprehensive View:** Traces provide an end-to-end view of how a request is handled, revealing dependencies and performance issues across different services.\n",
        "\n",
        "Together, Metrics, Events, Logs, and Traces provide a layered approach to observability, offering different levels of detail to understand the behavior of the Hybrid VAE and ensure its reliable operation."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1652478532.py, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1652478532.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    Monitoring is a critical component of MLOps, providing the necessary visibility into the performance and health of machine learning models and the systems they run on. It allows us to understand how our Hybrid VAE is behaving in various environments and identify potential issues early. Key monitoring concepts include:\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2df56313"
      },
      "source": [
        "## Implement training loop with mlops tracking\n",
        "\n",
        "Implemented the training loop for the Hybrid VAE, integrating MLflow for experiment tracking. This is a retry of the previous failed subtask.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27187c0f",
        "outputId": "07ecf3d5-d1cd-4308-bb38-35140d88836b"
      },
      "source": [
        "import mlflow\n",
        "import mlflow.pytorch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "'''\n",
        "Implement the training loop for the Hybrid VAE, integrating MLflow for experiment tracking, ensuring the HybridVAE\n",
        "class is defined and accessible, and handling the quantum layer output correctly. This is a retry of the previous failed subtask.\n",
        "'''\n",
        "\n",
        "\n",
        "# Start an MLflow run\n",
        "with mlflow.start_run():\n",
        "    # Log hyperparameters\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 10\n",
        "    classical_latent_dim = 32\n",
        "    quantum_latent_dim = 4\n",
        "\n",
        "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
        "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
        "    mlflow.log_param(\"classical_latent_dim\", classical_latent_dim)\n",
        "    mlflow.log_param(\"quantum_latent_dim\", quantum_latent_dim)\n",
        "\n",
        "    # Initialize the model, optimizer, and loss function\n",
        "    # Ensure the HybridVAE class defined in a previous cell is used\n",
        "    model = HybridVAE(input_dim=X_train_tensor.shape[1],\n",
        "                      latent_dim_classical=classical_latent_dim,\n",
        "                      latent_dim_quantum=quantum_latent_dim)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Define the loss function (VAE loss: Reconstruction + KL Divergence)\n",
        "    def vae_loss(recon_x, x, mu, log_var):\n",
        "        BCE = F.mse_loss(recon_x, x, reduction='sum') # Using MSE for reconstruction loss\n",
        "        KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "        return BCE + KLD\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Pass the entire batch to the model's forward method\n",
        "        # The forward method is responsible for handling the batching\n",
        "        # and calling the quantum layer correctly.\n",
        "        recon, mu, log_var = model(X_train_tensor)\n",
        "        loss = vae_loss(recon, X_train_tensor, mu, log_var)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = train_loss / X_train_tensor.size(0)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Log training loss for the epoch\n",
        "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "\n",
        "    # Log the trained model\n",
        "    mlflow.pytorch.log_model(model, \"hybrid_vae_model\")\n",
        "\n",
        "# The MLflow run automatically ends when exiting the 'with' block"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1563.0351\n",
            "Epoch [2/10], Loss: 1553.5277\n",
            "Epoch [3/10], Loss: 1545.8454\n",
            "Epoch [4/10], Loss: 1539.1155\n",
            "Epoch [5/10], Loss: 1532.4931\n",
            "Epoch [6/10], Loss: 1527.0064\n",
            "Epoch [7/10], Loss: 1522.0916\n",
            "Epoch [8/10], Loss: 1517.5983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/04 18:12:25 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/08/04 18:12:25 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu124) contains a local version label (+cu124). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Loss: 1512.3696\n",
            "Epoch [10/10], Loss: 1507.9644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/04 18:12:31 WARNING mlflow.utils.requirements_utils: Found torch version (2.6.0+cu124) contains a local version label (+cu124). MLflow logged a pip requirement for this package as 'torch==2.6.0' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
            "\u001b[31m2025/08/04 18:12:31 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c556f3c"
      },
      "source": [
        "## Fixing SRE golden signals\n",
        "\n",
        "The SRE Golden Signals (Latency, Traffic, Errors, Saturation) and how they can be applied to monitor the VAE in a production-like scenario to understand end-user experience.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "0f91fe09",
        "outputId": "d5e7c0b8-b340-498b-ddce-2011bf18bedd"
      },
      "source": [
        "### 🚦 Monitoring with SRE Golden Signals\n",
        "\n",
        "To effectively monitor the Hybrid VAE in a production-like environment and understand its impact on end-user experience, we can leverage the **SRE Golden Signals**. These four signals provide a comprehensive view of a system's health and performance from the user's perspective.\n",
        "\n",
        "1.  **Latency:** The time it takes to serve a request.\n",
        "    *   **Definition:** For our VAE, this is the time from when an inference request is received to when the reconstructed output is returned to the user. It can be broken down into different components, such as the time spent in the classical encoder, the quantum layer, and the classical decoder.\n",
        "    *   **Relevance:** High latency directly impacts user experience, leading to slow response times and frustration. Monitoring latency helps identify bottlenecks in the VAE pipeline, whether they are in the classical or quantum parts, or the infrastructure.\n",
        "    *   **End-User Experience:** Users expect timely results. Excessive latency makes the application feel unresponsive and unusable, especially in interactive scenarios.\n",
        "    *   **SLOs:** We can set SLOs for latency, e.g., \"99% of inference requests should be served within 500ms.\"\n",
        "\n",
        "2.  **Traffic:** A measure of how much demand is being placed on the system.\n",
        "    *   **Definition:** For the VAE, this could be the number of inference requests per second, the volume of data being processed, or the number of active users.\n",
        "    *   **Relevance:** Monitoring traffic helps understand the load on the system and anticipate scaling needs. Spikes or drops in traffic can indicate changes in user behavior or potential issues upstream.\n",
        "    *   **End-User Experience:** Insufficient capacity to handle traffic leads to increased latency and errors. Monitoring traffic ensures the system can handle the current demand, providing a consistent experience.\n",
        "    *   **SLOs:** SLOs can be set for traffic capacity, e.g., \"The system should be able to handle 100 inference requests per second with acceptable latency.\"\n",
        "\n",
        "3.  **Errors:** The rate of requests that fail.\n",
        "    *   **Definition:** For the VAE, this includes any requests that result in an error response, exceptions during processing (e.g., issues with quantum circuit execution, data format errors, infrastructure failures), or potentially outputs that are nonsensical or indicate a model failure (though detecting these might require additional monitoring of output characteristics).\n",
        "    *   **Relevance:** Errors directly indicate system instability and negatively impact user trust. Monitoring error rates helps quickly identify and address issues that prevent users from getting results.\n",
        "    *   **End-User Experience:** Users expect reliable service. Frequent errors make the application unreliable and lead to users abandoning the service.\n",
        "    *   **SLOs:** SLOs are crucial for error rates, e.g., \"The error rate for inference requests should be less than 0.1%.\"\n",
        "\n",
        "4.  **Saturation:** How \"full\" the system is; a measure of resource utilization.\n",
        "    *   **Definition:** For the VAE, this includes metrics like CPU utilization, GPU utilization, memory usage, network bandwidth, and potentially the queue length of requests waiting to be processed by the quantum device simulator or hardware.\n",
        "    *   **Relevance:** High saturation indicates that the system is nearing its capacity limits, which can lead to increased latency and errors. Monitoring saturation helps proactively scale resources before performance degrades.\n",
        "    *   **End-User Experience:** Saturated resources manifest as increased latency and errors for the end-user. Monitoring saturation ensures the underlying infrastructure can support the model's operations.\n",
        "    *   **SLOs:** While not always directly user-facing, saturation metrics support other SLOs. For example, an SLO could be \"CPU utilization should remain below 80% under normal load.\"\n",
        "\n",
        "By systematically monitoring these four Golden Signals, we gain valuable insights into the operational health and performance of the Hybrid VAE from the perspective that matters most – the end user. Setting and monitoring SLOs based on these signals ensures that we maintain a high level of service quality and a positive user experience."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 26) (ipython-input-1716697004.py, line 26)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1716697004.py\"\u001b[0;36m, line \u001b[0;32m26\u001b[0m\n\u001b[0;31m    *   **End-User Experience:** Saturated resources manifest as increased latency and errors for the end-user. Monitoring saturation ensures the underlying infrastructure can support the model's operations.\u001b[0m\n\u001b[0m                                                                                                                                                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNeepOXUSXtV",
        "outputId": "9ba7ffa5-c261-41df-b002-59a7745a9202"
      },
      "source": [
        "print(\"### 🚦 Monitoring with SRE Golden Signals\")\n",
        "print(\"\")\n",
        "print(\"To effectively monitor the Hybrid VAE in a production-like environment and understand its impact on end-user experience, we can leverage the **SRE Golden Signals**. These four signals provide a comprehensive view of a system's health and performance from the user's perspective.\")\n",
        "print(\"\")\n",
        "print(\"1.  **Latency:** The time it takes to serve a request.\")\n",
        "print(\"    *   **Definition:** For our VAE, this is the time from when an inference request is received to when the reconstructed output is returned to the user. It can be broken down into different components, such as the time spent in the classical encoder, the quantum layer, and the classical decoder.\")\n",
        "print(\"    *   **Relevance:** High latency directly impacts user experience, leading to slow response times and frustration. Monitoring latency helps identify bottlenecks in the VAE pipeline, whether they are in the classical or quantum parts, or the infrastructure.\")\n",
        "print(\"    *   **End-User Experience:** Users expect timely results. Excessive latency makes the application feel unresponsive and unusable, especially in interactive scenarios.\")\n",
        "print(\"    *   **SLOs:** We can set SLOs for latency, e.g., \\\"99% of inference requests should be served within 500ms.\\\"\")\n",
        "print(\"\")\n",
        "print(\"2.  **Traffic:** A measure of how much demand is being placed on the system.\")\n",
        "print(\"    *   **Definition:** For the VAE, this could be the number of inference requests per second, the volume of data being processed, or the number of active users.\")\n",
        "print(\"    *   **Relevance:** Monitoring traffic helps understand the load on the system and anticipate scaling needs. Spikes or drops in traffic can indicate changes in user behavior or potential issues upstream.\")\n",
        "print(\"    *   **End-User Experience:** Insufficient capacity to handle traffic leads to increased latency and errors. Monitoring traffic ensures the system can handle the current demand, providing a consistent experience.\")\n",
        "print(\"    *   **SLOs:** SLOs can be set for traffic capacity, e.g., \\\"The system should be able to handle 100 inference requests per second with acceptable latency.\\\"\")\n",
        "print(\"\")\n",
        "print(\"3.  **Errors:** The rate of requests that fail.\")\n",
        "print(\"    *   **Definition:** For the VAE, this includes any requests that result in an error response, exceptions during processing (e.g., issues with quantum circuit execution, data format errors, infrastructure failures), or potentially outputs that are nonsensical or indicate a model failure (though detecting these might require additional monitoring of output characteristics).\")\n",
        "print(\"    *   **Relevance:** Errors directly indicate system instability and negatively impact user trust. Monitoring error rates helps quickly identify and address issues that prevent users from getting results.\")\n",
        "print(\"    *   **End-User Experience:** Users expect reliable service. Frequent errors make the application unreliable and lead to users abandoning the service.\")\n",
        "print(\"    *   **SLOs:** SLOs are crucial for error rates, e.g., \\\"The error rate for inference requests should be less than 0.1%.\\\"\")\n",
        "print(\"\")\n",
        "print(\"4.  **Saturation:** How \\\"full\\\" the system is; a measure of resource utilization.\")\n",
        "print(\"    *   **Definition:** For the VAE, this includes metrics like CPU utilization, GPU utilization, memory usage, network bandwidth, and potentially the queue length of requests waiting to be processed by the quantum device simulator or hardware.\")\n",
        "print(\"    *   **Relevance:** High saturation indicates that the system is nearing its capacity limits, which can lead to increased latency and errors. Monitoring saturation helps proactively scale resources before performance degrades.\")\n",
        "print(\"    *   **End-User Experience:** Saturated resources manifest as increased latency and errors for the end-user. Monitoring saturation ensures the underlying infrastructure can support the model's operations.\")\n",
        "print(\"    *   **SLOs:** While not always directly user-facing, saturation metrics support other SLOs. For example, an SLO could be \\\"CPU utilization should remain below 80% under normal load.\\\"\")\n",
        "print(\"\")\n",
        "print(\"By systematically monitoring these four Golden Signals, we gain valuable insights into the operational health and performance of the Hybrid VAE from the perspective that matters most – the end user. Setting and monitoring SLOs based on these signals ensures that we maintain a high level of service quality and a positive user experience.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### 🚦 Monitoring with SRE Golden Signals\n",
            "\n",
            "To effectively monitor the Hybrid VAE in a production-like environment and understand its impact on end-user experience, we can leverage the **SRE Golden Signals**. These four signals provide a comprehensive view of a system's health and performance from the user's perspective.\n",
            "\n",
            "1.  **Latency:** The time it takes to serve a request.\n",
            "    *   **Definition:** For our VAE, this is the time from when an inference request is received to when the reconstructed output is returned to the user. It can be broken down into different components, such as the time spent in the classical encoder, the quantum layer, and the classical decoder.\n",
            "    *   **Relevance:** High latency directly impacts user experience, leading to slow response times and frustration. Monitoring latency helps identify bottlenecks in the VAE pipeline, whether they are in the classical or quantum parts, or the infrastructure.\n",
            "    *   **End-User Experience:** Users expect timely results. Excessive latency makes the application feel unresponsive and unusable, especially in interactive scenarios.\n",
            "    *   **SLOs:** We can set SLOs for latency, e.g., \"99% of inference requests should be served within 500ms.\"\n",
            "\n",
            "2.  **Traffic:** A measure of how much demand is being placed on the system.\n",
            "    *   **Definition:** For the VAE, this could be the number of inference requests per second, the volume of data being processed, or the number of active users.\n",
            "    *   **Relevance:** Monitoring traffic helps understand the load on the system and anticipate scaling needs. Spikes or drops in traffic can indicate changes in user behavior or potential issues upstream.\n",
            "    *   **End-User Experience:** Insufficient capacity to handle traffic leads to increased latency and errors. Monitoring traffic ensures the system can handle the current demand, providing a consistent experience.\n",
            "    *   **SLOs:** SLOs can be set for traffic capacity, e.g., \"The system should be able to handle 100 inference requests per second with acceptable latency.\"\n",
            "\n",
            "3.  **Errors:** The rate of requests that fail.\n",
            "    *   **Definition:** For the VAE, this includes any requests that result in an error response, exceptions during processing (e.g., issues with quantum circuit execution, data format errors, infrastructure failures), or potentially outputs that are nonsensical or indicate a model failure (though detecting these might require additional monitoring of output characteristics).\n",
            "    *   **Relevance:** Errors directly indicate system instability and negatively impact user trust. Monitoring error rates helps quickly identify and address issues that prevent users from getting results.\n",
            "    *   **End-User Experience:** Users expect reliable service. Frequent errors make the application unreliable and lead to users abandoning the service.\n",
            "    *   **SLOs:** SLOs are crucial for error rates, e.g., \"The error rate for inference requests should be less than 0.1%.\"\n",
            "\n",
            "4.  **Saturation:** How \"full\" the system is; a measure of resource utilization.\n",
            "    *   **Definition:** For the VAE, this includes metrics like CPU utilization, GPU utilization, memory usage, network bandwidth, and potentially the queue length of requests waiting to be processed by the quantum device simulator or hardware.\n",
            "    *   **Relevance:** High saturation indicates that the system is nearing its capacity limits, which can lead to increased latency and errors. Monitoring saturation helps proactively scale resources before performance degrades.\n",
            "    *   **End-User Experience:** Saturated resources manifest as increased latency and errors for the end-user. Monitoring saturation ensures the underlying infrastructure can support the model's operations.\n",
            "    *   **SLOs:** While not always directly user-facing, saturation metrics support other SLOs. For example, an SLO could be \"CPU utilization should remain below 80% under normal load.\"\n",
            "\n",
            "By systematically monitoring these four Golden Signals, we gain valuable insights into the operational health and performance of the Hybrid VAE from the perspective that matters most – the end user. Setting and monitoring SLOs based on these signals ensures that we maintain a high level of service quality and a positive user experience.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "938fc198"
      },
      "source": [
        "## Relate monitoring to end-user outcomes\n",
        "\n",
        " how the collected metrics, logs, and golden signals can be used to infer end-user research outcomes related to the VAE's performance and reliability.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac25df2a",
        "outputId": "f4f243dc-a936-4d3e-a4cb-1d81e874beff"
      },
      "source": [
        "print(\"### 🔬 Inferring End-User Research Outcomes from Monitoring Data\")\n",
        "print(\"\")\n",
        "print(\"The monitoring data collected through metrics, logs, events, and the SRE Golden Signals provides crucial insights not just into the operational health of the Hybrid VAE, but also allows us to infer **end-user research outcomes** and the quality of the scientific findings derived from the model's output.\")\n",
        "print(\"\")\n",
        "print(\"Here's how we can connect the monitoring data to research outcomes:\")\n",
        "print(\"\")\n",
        "print(\"1.  **Performance Metrics (Latency, Reconstruction Error) and Research Validity:**\")\n",
        "print(\"    *   **Latency:** While primarily an operational metric, consistently high inference latency can impact research workflows. If researchers need to process large datasets iteratively or perform real-time analysis, slow VAE performance can hinder their progress and the speed of discovery.\")\n",
        "print(\"    *   **Reconstruction Error:** This is a direct indicator of how well the VAE is capturing the underlying patterns in the single-cell data. Lower reconstruction error generally implies that the latent space is a better representation of the data, leading to potentially more accurate and meaningful downstream analyses (e.g., clustering, dimensionality reduction, anomaly detection). Monitoring this metric over time and across different model versions helps ensure the VAE's output remains a reliable basis for research conclusions.\")\n",
        "print(\"    *   **Inference Time vs. Reconstruction Quality:** By comparing inference time metrics with reconstruction error metrics, we can evaluate the trade-off between computational cost and the quality of the VAE's output, which is critical for researchers deciding on appropriate model configurations for their work.\")\n",
        "print(\"\")\n",
        "print(\"2.  **Error Rates and Trustworthiness of Results:**\")\n",
        "print(\"    *   **Error Rate:** High error rates during inference indicate instability or failure in the VAE process. If the VAE fails to produce valid outputs for a significant portion of the data, any downstream analysis performed on the partial or potentially corrupted output will be unreliable and could lead to flawed research conclusions.\")\n",
        "print(\"    *   **Impact on Reproducibility:** Undocumented or unhandled errors make research irreproducible. Monitoring error types and frequencies helps address underlying issues, ensuring that the VAE consistently produces outputs that can be trusted and reproduced by other researchers.\")\n",
        "print(\"\")\n",
        "print(\"3.  **Saturation and Resource Constraints on Research Scale:**\")\n",
        "print(\"    *   **Saturation:** High resource utilization (CPU, GPU, memory) can lead to performance degradation (increased latency) and potentially errors. In a research setting, this translates to limitations on the scale of data that can be processed or the complexity of analyses that can be performed using the VAE outputs.\")\n",
        "print(\"    *   **Planning for Scale:** Monitoring saturation helps researchers and infrastructure teams plan for necessary resources to handle larger datasets or more computationally intensive research tasks, ensuring the VAE doesn't become a bottleneck to scientific progress.\")\n",
        "print(\"\")\n",
        "print(\"4.  **Logs and Traces for Debugging Impacting Outcomes:**\")\n",
        "print(\"    *   **Debugging Critical Issues:** When performance degrades or errors occur (as indicated by metrics and error rates), logs provide the detailed context needed to diagnose the root cause. For example, logs can show specific data points that caused issues, the state of the quantum circuit at the time of failure, or errors in classical tensor operations.\")\n",
        "print(\"    *   **Understanding Data Flow:** Traces, in a more complex pipeline, help visualize the path of a data sample through the VAE and any surrounding processing steps. This is crucial for identifying where unexpected behavior or delays are introduced that might affect the final research outcome.\")\n",
        "print(\"    *   **Ensuring Data Integrity:** Logs and traces can help confirm that data is being processed correctly at each stage, from input to the final VAE output, which is fundamental for the integrity of research findings.\")\n",
        "print(\"\")\n",
        "print(\"In summary, by actively monitoring the Hybrid VAE using a combination of detailed metrics, logs, events, and SRE Golden Signals, we can gain deep insights into its performance and reliability. This operational understanding directly translates into the ability to infer the quality and trustworthiness of the research outcomes derived from the VAE's application in single-cell analysis, ensuring that the model serves as a robust and reliable tool for scientific discovery.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### 🔬 Inferring End-User Research Outcomes from Monitoring Data\n",
            "\n",
            "The monitoring data collected through metrics, logs, events, and the SRE Golden Signals provides crucial insights not just into the operational health of the Hybrid VAE, but also allows us to infer **end-user research outcomes** and the quality of the scientific findings derived from the model's output.\n",
            "\n",
            "Here's how we can connect the monitoring data to research outcomes:\n",
            "\n",
            "1.  **Performance Metrics (Latency, Reconstruction Error) and Research Validity:**\n",
            "    *   **Latency:** While primarily an operational metric, consistently high inference latency can impact research workflows. If researchers need to process large datasets iteratively or perform real-time analysis, slow VAE performance can hinder their progress and the speed of discovery.\n",
            "    *   **Reconstruction Error:** This is a direct indicator of how well the VAE is capturing the underlying patterns in the single-cell data. Lower reconstruction error generally implies that the latent space is a better representation of the data, leading to potentially more accurate and meaningful downstream analyses (e.g., clustering, dimensionality reduction, anomaly detection). Monitoring this metric over time and across different model versions helps ensure the VAE's output remains a reliable basis for research conclusions.\n",
            "    *   **Inference Time vs. Reconstruction Quality:** By comparing inference time metrics with reconstruction error metrics, we can evaluate the trade-off between computational cost and the quality of the VAE's output, which is critical for researchers deciding on appropriate model configurations for their work.\n",
            "\n",
            "2.  **Error Rates and Trustworthiness of Results:**\n",
            "    *   **Error Rate:** High error rates during inference indicate instability or failure in the VAE process. If the VAE fails to produce valid outputs for a significant portion of the data, any downstream analysis performed on the partial or potentially corrupted output will be unreliable and could lead to flawed research conclusions.\n",
            "    *   **Impact on Reproducibility:** Undocumented or unhandled errors make research irreproducible. Monitoring error types and frequencies helps address underlying issues, ensuring that the VAE consistently produces outputs that can be trusted and reproduced by other researchers.\n",
            "\n",
            "3.  **Saturation and Resource Constraints on Research Scale:**\n",
            "    *   **Saturation:** High resource utilization (CPU, GPU, memory) can lead to performance degradation (increased latency) and potentially errors. In a research setting, this translates to limitations on the scale of data that can be processed or the complexity of analyses that can be performed using the VAE outputs.\n",
            "    *   **Planning for Scale:** Monitoring saturation helps researchers and infrastructure teams plan for necessary resources to handle larger datasets or more computationally intensive research tasks, ensuring the VAE doesn't become a bottleneck to scientific progress.\n",
            "\n",
            "4.  **Logs and Traces for Debugging Impacting Outcomes:**\n",
            "    *   **Debugging Critical Issues:** When performance degrades or errors occur (as indicated by metrics and error rates), logs provide the detailed context needed to diagnose the root cause. For example, logs can show specific data points that caused issues, the state of the quantum circuit at the time of failure, or errors in classical tensor operations.\n",
            "    *   **Understanding Data Flow:** Traces, in a more complex pipeline, help visualize the path of a data sample through the VAE and any surrounding processing steps. This is crucial for identifying where unexpected behavior or delays are introduced that might affect the final research outcome.\n",
            "    *   **Ensuring Data Integrity:** Logs and traces can help confirm that data is being processed correctly at each stage, from input to the final VAE output, which is fundamental for the integrity of research findings.\n",
            "\n",
            "In summary, by actively monitoring the Hybrid VAE using a combination of detailed metrics, logs, events, and SRE Golden Signals, we can gain deep insights into its performance and reliability. This operational understanding directly translates into the ability to infer the quality and trustworthiness of the research outcomes derived from the VAE's application in single-cell analysis, ensuring that the model serves as a robust and reliable tool for scientific discovery.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bfd73a9"
      },
      "source": [
        "## Illustrative monitoring setup\n",
        "\n",
        "Conceptual and illustrative code examples for setting up a basic monitoring system (e.g., using a simple logging framework or demonstrating how metrics would be collected). Acknowledge that a full-fledged monitoring setup is beyond the scope of a single notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa2d3304",
        "outputId": "11ed61ac-082c-465a-e021-f7e809872a7f"
      },
      "source": [
        "print(\"### 🛠️ Illustrative Monitoring Setup\")\n",
        "print(\"\")\n",
        "print(\"This section provides conceptual and illustrative code snippets to demonstrate the basic principles of setting up monitoring for the Hybrid VAE within this notebook environment. It's important to note that a full-fledged, production-grade monitoring system involves more complex components like dedicated time-series databases (e.g., Prometheus), visualization dashboards (e.g., Grafana), distributed tracing systems (e.g., Jaeger, Zipkin), and centralized logging platforms (e.g., ELK stack, Splunk). The examples here are simplified for demonstration purposes and do not require setting up these external systems.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### 🛠️ Illustrative Monitoring Setup\n",
            "\n",
            "This section provides conceptual and illustrative code snippets to demonstrate the basic principles of setting up monitoring for the Hybrid VAE within this notebook environment. It's important to note that a full-fledged, production-grade monitoring system involves more complex components like dedicated time-series databases (e.g., Prometheus), visualization dashboards (e.g., Grafana), distributed tracing systems (e.g., Jaeger, Zipkin), and centralized logging platforms (e.g., ELK stack, Splunk). The examples here are simplified for demonstration purposes and do not require setting up these external systems.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e5df8fb",
        "outputId": "2deddcaa-b2df-495c-fa73-e41733d2820e"
      },
      "source": [
        "import logging\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pennylane as qml\n",
        "import mlflow\n",
        "\n",
        "'''\n",
        "Author: Arun Singh | arunsingh.in@gmail.com\n",
        "\n",
        "Python code snippets demonstrating basic logging configuration and usage, how to collect metrics during training\n",
        " and inference using a simple dictionary, and conceptual code showing where to integrate with more advanced systems.\n",
        "\n",
        "'''\n",
        "\n",
        "# Assuming mlflow is already imported and used for experiment tracking\n",
        "\n",
        "# Configure basic logging (if not already done)\n",
        "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Simple dictionary to simulate metrics collection for granular, per-step data\n",
        "# (MLflow is used for higher-level experiment metrics like epoch loss)\n",
        "step_metrics = {\n",
        "    \"train\": [],\n",
        "    \"inference\": []\n",
        "}\n",
        "\n",
        "# --- Illustrative Code Snippets ---\n",
        "\n",
        "# 1. Basic Logging Usage (Reinforcement)\n",
        "logging.info(\"Monitoring setup: Logging configured.\")\n",
        "logging.info(f\"Current time: {time.ctime()}\")\n",
        "\n",
        "# Example within a hypothetical training step (inside the epoch loop)\n",
        "# logging.info(f\"  Processing batch {batch_idx}/{total_batches}\")\n",
        "# logging.debug(f\"  Batch input shape: {inputs.shape}\")\n",
        "# logging.debug(f\"  Loss for batch {batch_idx}: {loss.item():.4f}\")\n",
        "\n",
        "# Example within a hypothetical inference request\n",
        "# logging.info(\"Received inference request.\")\n",
        "# logging.info(f\"  Input data shape: {input_data.shape}\")\n",
        "\n",
        "# 2. Simple Metrics Collection (Manual)\n",
        "# Example within the training loop (collecting metrics per step/batch if applicable)\n",
        "# In our current setup, we process the whole dataset, so step metrics are less relevant\n",
        "# For illustrative purposes, let's imagine collecting time per 'batch' (our full dataset)\n",
        "\n",
        "# Inside the training loop, around the forward/backward pass:\n",
        "# start_step_time = time.time()\n",
        "# recon, mu, log_var = model(X_train_tensor) # Assuming X_train_tensor is the 'batch'\n",
        "# loss = vae_loss(recon, X_train_tensor, mu, log_var)\n",
        "# loss.backward()\n",
        "# optimizer.step()\n",
        "# end_step_time = time.time()\n",
        "# step_metrics[\"train\"].append({\"epoch\": epoch, \"step_duration_s\": end_step_time - start_step_time})\n",
        "\n",
        "# Example within the inference process (already implicitly done in benchmark, but showing manual collection)\n",
        "# start_infer_time = time.time()\n",
        "# ... inference code ...\n",
        "# end_infer_time = time.time()\n",
        "# step_metrics[\"inference\"].append({\"inference_duration_s\": end_infer_time - start_infer_time})\n",
        "# # Also could collect metrics on the output:\n",
        "# # step_metrics[\"inference\"][-1][\"output_mean\"] = reconstructed_test_data.mean().item()\n",
        "# # step_metrics[\"inference\"][-1][\"output_std\"] = reconstructed_test_data.std().item()\n",
        "\n",
        "\n",
        "# 3. Conceptual Integration with Advanced Systems\n",
        "\n",
        "# --- Metrics (e.g., Prometheus Client) ---\n",
        "# import prometheus_client\n",
        "\n",
        "# # Define metrics (gauge, counter, histogram, summary)\n",
        "# INFERENCE_LATENCY = prometheus_client.Summary('inference_latency_seconds', 'Time taken for inference requests')\n",
        "# QUANTUM_CIRCUIT_DURATION = prometheus_client.Histogram('quantum_circuit_duration_seconds', 'Duration of quantum circuit execution')\n",
        "# ERROR_COUNT = prometheus_client.Counter('errors_total', 'Total number of errors')\n",
        "\n",
        "# # In the inference code:\n",
        "# # start_time = time.time()\n",
        "# # try:\n",
        "# #     ... inference process ...\n",
        "# #     end_time = time.time()\n",
        "# #     INFERENCE_LATENCY.observe(end_time - start_time)\n",
        "# # except Exception as e:\n",
        "# #     ERROR_COUNT.inc()\n",
        "# #     logging.error(f\"Inference error: {e}\")\n",
        "\n",
        "# # Around quantum layer call:\n",
        "# # start_q_time = time.time()\n",
        "# # z_quantum_output = model.quantum_layer(z_quantum_input)\n",
        "# # end_q_time = time.time()\n",
        "# # QUANTUM_CIRCUIT_DURATION.observe(end_q_time - start_q_time)\n",
        "\n",
        "# # Start a Prometheus exposition server (typically in a separate service/script)\n",
        "# # prometheus_client.start_http_server(8000)\n",
        "\n",
        "\n",
        "# --- Tracing (e.g., OpenTelemetry) ---\n",
        "# from opentelemetry import trace\n",
        "# from opentelemetry.sdk.trace import TracerProvider\n",
        "# from opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor\n",
        "\n",
        "# # Configure tracer (typically done once at application start)\n",
        "# # provider = TracerProvider()\n",
        "# # processor = SimpleSpanProcessor(ConsoleSpanExporter())\n",
        "# # provider.add_span_processor(processor)\n",
        "# # trace.set_tracer_provider(provider)\n",
        "# # tracer = trace.get_tracer(__name__)\n",
        "\n",
        "# # Around specific operations (e.g., encoding, quantum layer, decoding)\n",
        "# # with tracer.start_as_current_span(\"vae_inference\"):\n",
        "# #     # Inside this span, create child spans\n",
        "# #     with tracer.start_as_current_span(\"classical_encode\"):\n",
        "# #         encoded = model.encoder_classical(x)\n",
        "# #     with tracer.start_as_current_span(\"quantum_layer\"):\n",
        "# #         z_quantum_output = model.quantum_layer(z_quantum_input)\n",
        "# #     with tracer.start_as_current_span(\"classical_decode\"):\n",
        "# #         reconstruction = model.decoder_classical(z)\n",
        "\n",
        "\n",
        "print(\"\\nIllustrative monitoring code snippets added.\")\n",
        "# print(\"Collected step metrics (simulated):\")\n",
        "# print(step_metrics)\n",
        "print(\"Conceptual examples for Prometheus and OpenTelemetry integration are commented out.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Illustrative monitoring code snippets added.\n",
            "Conceptual examples for Prometheus and OpenTelemetry integration are commented out.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b9c8fe9"
      },
      "source": [
        "## Summarize monitoring and observability\n",
        "\n",
        "Summariy of monitoring and observability in MLOps and for understanding model impact.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd093fb5",
        "outputId": "7f1e9c3e-99be-4686-c2d9-ef6abec58501"
      },
      "source": [
        "print(\"### 👁️ The Importance of Monitoring and Observability in MLOps\")\n",
        "print(\"\")\n",
        "print(\"Monitoring and observability are not just operational concerns; they are fundamental pillars of MLOps that are essential for understanding the impact and performance of machine learning models, especially complex ones like the Hybrid Quantum-Classical VAE.\")\n",
        "print(\"\")\n",
        "print(\"For end-user research outcomes, monitoring and observability provide the data needed to:\")\n",
        "print(\"\")\n",
        "print(\"1.  **Ensure Model Reliability and Trust:** By tracking metrics like error rates and reconstruction error, we can verify that the VAE is consistently producing valid and high-quality outputs. This is paramount for researchers who rely on the VAE's output for their analyses. Unreliable models lead to untrustworthy research findings.\")\n",
        "print(\"\")\n",
        "print(\"2.  **Facilitate Debugging and Problem Resolution:** When issues arise (e.g., performance degradation, unexpected output), logs and traces provide the detailed context required to quickly identify the root cause. This minimizes downtime and ensures that researchers can continue their work without significant interruptions.\")\n",
        "print(\"\")\n",
        "print(\"3.  **Understand Performance Characteristics:** Metrics like latency and saturation reveal how the Hybrid VAE performs under different loads and on different hardware. This understanding is crucial for researchers to select appropriate computational resources and anticipate limitations when working with large datasets.\")\n",
        "print(\"\")\n",
        "print(\"4.  **Gain Insights for Model Improvement:** By analyzing trends in metrics and correlating them with events (like model updates), we can identify areas for improvement in the VAE architecture, training process, or the underlying infrastructure. This iterative process of monitoring and improvement is key to developing more effective models and advancing research capabilities.\")\n",
        "print(\"\")\n",
        "print(\"5.  **Validate Research Hypotheses:** In some cases, monitoring the behavior of the VAE's latent space or the impact of the quantum layer (through specific metrics) can directly inform or validate research hypotheses about the structure of the single-cell data.\")\n",
        "print(\"\")\n",
        "print(\"In essence, a robust monitoring and observability strategy transforms the Hybrid VAE from a black box into a transparent and understandable component of the research workflow. It empowers researchers to trust the model's outputs, efficiently troubleshoot issues, and make informed decisions based on real-world performance data, ultimately accelerating scientific discovery and ensuring the validity of their findings.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### 👁️ The Importance of Monitoring and Observability in MLOps\n",
            "\n",
            "Monitoring and observability are not just operational concerns; they are fundamental pillars of MLOps that are essential for understanding the impact and performance of machine learning models, especially complex ones like the Hybrid Quantum-Classical VAE.\n",
            "\n",
            "For end-user research outcomes, monitoring and observability provide the data needed to:\n",
            "\n",
            "1.  **Ensure Model Reliability and Trust:** By tracking metrics like error rates and reconstruction error, we can verify that the VAE is consistently producing valid and high-quality outputs. This is paramount for researchers who rely on the VAE's output for their analyses. Unreliable models lead to untrustworthy research findings.\n",
            "\n",
            "2.  **Facilitate Debugging and Problem Resolution:** When issues arise (e.g., performance degradation, unexpected output), logs and traces provide the detailed context required to quickly identify the root cause. This minimizes downtime and ensures that researchers can continue their work without significant interruptions.\n",
            "\n",
            "3.  **Understand Performance Characteristics:** Metrics like latency and saturation reveal how the Hybrid VAE performs under different loads and on different hardware. This understanding is crucial for researchers to select appropriate computational resources and anticipate limitations when working with large datasets.\n",
            "\n",
            "4.  **Gain Insights for Model Improvement:** By analyzing trends in metrics and correlating them with events (like model updates), we can identify areas for improvement in the VAE architecture, training process, or the underlying infrastructure. This iterative process of monitoring and improvement is key to developing more effective models and advancing research capabilities.\n",
            "\n",
            "5.  **Validate Research Hypotheses:** In some cases, monitoring the behavior of the VAE's latent space or the impact of the quantum layer (through specific metrics) can directly inform or validate research hypotheses about the structure of the single-cell data.\n",
            "\n",
            "In essence, a robust monitoring and observability strategy transforms the Hybrid VAE from a black box into a transparent and understandable component of the research workflow. It empowers researchers to trust the model's outputs, efficiently troubleshoot issues, and make informed decisions based on real-world performance data, ultimately accelerating scientific discovery and ensuring the validity of their findings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "164baded"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   Instrumentation was added to the training and inference loops of the Hybrid VAE, including logging statements to track progress, data shapes, and key events.\n",
        "*   MLflow tracking was successfully integrated into the training loop to log hyperparameters (learning rate, number of epochs, latent dimensions) and the average training loss per epoch.\n",
        "*   Basic metrics collection (inference duration, reconstruction error) was illustrated using a simple Python dictionary.\n",
        "*   The SRE Golden Signals (Latency, Traffic, Errors, Saturation) were explained in detail, relating them to monitoring the Hybrid VAE in a production context and understanding end-user experience.\n",
        "*   The connection between collected monitoring data (metrics, logs, golden signals) and the ability to infer the quality and trustworthiness of end-user research outcomes derived from the VAE's output was discussed.\n",
        "*   Illustrative code snippets were provided to demonstrate basic logging and manual metric collection, alongside conceptual explanations of how to integrate with more advanced monitoring systems like Prometheus and OpenTelemetry.\n",
        "*   The overall importance of monitoring and observability in MLOps for ensuring model reliability, facilitating debugging, understanding performance, guiding improvements, and validating research hypotheses was summarized.\n",
        "*   Initial attempts to create markdown cells directly using `code_block` failed with `SyntaxError`, requiring a workaround to print markdown content as output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5acc37dd"
      },
      "source": [
        "# The Notebook Sum\n",
        "Modify the selected empty cell in the notebook to include a section on advanced monitoring for the Hybrid VAE, detailing metrics collection, automated checks based on SRE Golden Signals, and visualization using Streamlit, and then provide a summary to conclude the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b744aead"
      },
      "source": [
        "## Refine metrics collection\n",
        "\n",
        "Detail the specific metrics to collect for the Hybrid VAE, focusing on quantum layer performance and output characteristics.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf0c3ce3",
        "outputId": "b406e0c8-154e-4524-f42b-71148945aabf"
      },
      "source": [
        "# This is a placeholder code block to output markdown content, as direct markdown cell creation is not supported.\n",
        "# The actual markdown content will be printed as output.\n",
        "\n",
        "print(\"### 🧬 Detailed Metrics for Hybrid VAE Monitoring\")\n",
        "print(\"\")\n",
        "print(\"Beyond the standard training and inference times, a Hybrid Quantum-Classical VAE requires the collection of more detailed and specific metrics to gain a deeper understanding of its performance, behavior, and the contribution of its quantum component. This is particularly crucial for debugging, optimization, and ensuring the reliability of the model in research workflows.\")\n",
        "print(\"\")\n",
        "print(\"Here are specific metrics to collect, categorized by the component of the VAE:\")\n",
        "print(\"\")\n",
        "print(\"#### Classical Encoder Metrics:\")\n",
        "print(\"\")\n",
        "print(\"*   **Processing Time:** The time taken for the classical encoder to process an input sample or batch. This helps identify if the classical encoding is a bottleneck.\")\n",
        "print(\"*   **Latent Space Parameter Distribution (Mean, Variance):** Statistics of the output of the encoder (mean and log-variance for both classical and quantum latent dimensions). Monitoring these distributions helps ensure the encoder is producing meaningful latent representations and that the reparameterization trick is stable.\")\n",
        "print(\"\")\n",
        "print(\"#### Quantum Layer Metrics:\")\n",
        "print(\"\")\n",
        "print(\"*   **Execution Time:** The time taken for the quantum circuit to execute for a single set of inputs. This is a critical metric for understanding the computational cost of the quantum part, especially when using simulators or real quantum hardware.\")\n",
        "print(\"*   **Quantum Expectation Value Distribution (Mean, Variance, Min, Max):** Statistics of the expectation values returned by the quantum circuit. Analyzing this distribution provides insights into the range and variability of the quantum output, which directly contributes to the latent space.\")\n",
        "print(\"*   **Number of Quantum Circuit Evaluations:** The total number of times the quantum circuit is evaluated during training (per epoch or per batch) and inference (per sample or per batch). This is a measure of the quantum computational workload.\")\n",
        "print(\"\")\n",
        "print(\"#### Classical Decoder Metrics:\")\n",
        "print(\"\")\n",
        "print(\"*   **Processing Time:** The time taken for the classical decoder to reconstruct the output from the latent space. Helps identify decoding bottlenecks.\")\n",
        "print(\"*   **Reconstructed Data Distribution (Mean, Variance):** Statistics of the output of the decoder. Comparing this to the input data distribution can indicate how well the VAE is preserving the original data characteristics.\")\n",
        "print(\"\")\n",
        "print(\"#### Overall VAE Metrics:\")\n",
        "print(\"\")\n",
        "print(\"*   **Total Inference Time:** The total time taken for a full forward pass during inference (from input to reconstructed output). This is a key end-user-facing performance metric.\")\n",
        "print(\"*   **Total Training Time per Epoch:** The total time taken to complete one training epoch. Useful for tracking training progress and efficiency.\")\n",
        "print(\"*   **Reconstruction Error:** The difference between the original input and the reconstructed output (e.g., using Mean Squared Error). A primary measure of the VAE's ability to capture data patterns.\")\n",
        "print(\"*   **KL Divergence Loss:** The regularization term in the VAE loss, measuring the divergence between the learned latent distribution and a prior distribution (e.g., a standard normal distribution). Monitoring this helps understand how well the latent space is being regularized.\")\n",
        "print(\"*   **Total VAE Loss:** The combined reconstruction and KL divergence loss. The primary metric for tracking overall model convergence during training.\")\n",
        "print(\"\")\n",
        "print(\"Collecting these detailed metrics provides a comprehensive view of the Hybrid VAE's performance and behavior, allowing for targeted optimization and a better understanding of the quantum layer's impact on the overall model and the resulting research outcomes.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### 🧬 Detailed Metrics for Hybrid VAE Monitoring\n",
            "\n",
            "Beyond the standard training and inference times, a Hybrid Quantum-Classical VAE requires the collection of more detailed and specific metrics to gain a deeper understanding of its performance, behavior, and the contribution of its quantum component. This is particularly crucial for debugging, optimization, and ensuring the reliability of the model in research workflows.\n",
            "\n",
            "Here are specific metrics to collect, categorized by the component of the VAE:\n",
            "\n",
            "#### Classical Encoder Metrics:\n",
            "\n",
            "*   **Processing Time:** The time taken for the classical encoder to process an input sample or batch. This helps identify if the classical encoding is a bottleneck.\n",
            "*   **Latent Space Parameter Distribution (Mean, Variance):** Statistics of the output of the encoder (mean and log-variance for both classical and quantum latent dimensions). Monitoring these distributions helps ensure the encoder is producing meaningful latent representations and that the reparameterization trick is stable.\n",
            "\n",
            "#### Quantum Layer Metrics:\n",
            "\n",
            "*   **Execution Time:** The time taken for the quantum circuit to execute for a single set of inputs. This is a critical metric for understanding the computational cost of the quantum part, especially when using simulators or real quantum hardware.\n",
            "*   **Quantum Expectation Value Distribution (Mean, Variance, Min, Max):** Statistics of the expectation values returned by the quantum circuit. Analyzing this distribution provides insights into the range and variability of the quantum output, which directly contributes to the latent space.\n",
            "*   **Number of Quantum Circuit Evaluations:** The total number of times the quantum circuit is evaluated during training (per epoch or per batch) and inference (per sample or per batch). This is a measure of the quantum computational workload.\n",
            "\n",
            "#### Classical Decoder Metrics:\n",
            "\n",
            "*   **Processing Time:** The time taken for the classical decoder to reconstruct the output from the latent space. Helps identify decoding bottlenecks.\n",
            "*   **Reconstructed Data Distribution (Mean, Variance):** Statistics of the output of the decoder. Comparing this to the input data distribution can indicate how well the VAE is preserving the original data characteristics.\n",
            "\n",
            "#### Overall VAE Metrics:\n",
            "\n",
            "*   **Total Inference Time:** The total time taken for a full forward pass during inference (from input to reconstructed output). This is a key end-user-facing performance metric.\n",
            "*   **Total Training Time per Epoch:** The total time taken to complete one training epoch. Useful for tracking training progress and efficiency.\n",
            "*   **Reconstruction Error:** The difference between the original input and the reconstructed output (e.g., using Mean Squared Error). A primary measure of the VAE's ability to capture data patterns.\n",
            "*   **KL Divergence Loss:** The regularization term in the VAE loss, measuring the divergence between the learned latent distribution and a prior distribution (e.g., a standard normal distribution). Monitoring this helps understand how well the latent space is being regularized.\n",
            "*   **Total VAE Loss:** The combined reconstruction and KL divergence loss. The primary metric for tracking overall model convergence during training.\n",
            "\n",
            "Collecting these detailed metrics provides a comprehensive view of the Hybrid VAE's performance and behavior, allowing for targeted optimization and a better understanding of the quantum layer's impact on the overall model and the resulting research outcomes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01244d87",
        "outputId": "4a7bc92b-4f99-4944-b5dc-a1f04bfd2e95"
      },
      "source": [
        "# This is a placeholder code block to output markdown content, as direct markdown cell creation is not supported.\n",
        "# The actual markdown content will be printed as output.\n",
        "\n",
        "print(\"### ✅ Automated Checks based on SRE Golden Signals\")\n",
        "print(\"\")\n",
        "print(\"Implementing automated checks based on the SRE Golden Signals (Latency, Traffic, Errors, Saturation) is crucial for proactively identifying and responding to issues with the Hybrid VAE in a production-like environment. These checks act as an early warning system, ensuring that the model continues to meet performance and reliability standards, which directly impacts the quality of end-user research outcomes.\")\n",
        "print(\"\")\n",
        "print(\"Here's how automated checks can be applied to the Hybrid VAE:\")\n",
        "print(\"\")\n",
        "print(\"#### Latency Checks:\")\n",
        "print(\"\")\n",
        "print(\"*   **Configuration:** Set thresholds for acceptable inference latency (e.g., average latency, p95 latency). These thresholds should be based on the requirements of the research workflow.\")\n",
        "print(\"*   **Monitoring:** Continuously monitor the actual inference latency using the collected metrics.\")\n",
        "print(\"*   **Alerting:** Trigger alerts (e.g., email, Slack notification, PagerDuty) if the monitored latency exceeds the defined thresholds for a sustained period. This indicates potential performance degradation.\")\n",
        "print(\"*   **Example Check:** Alert if the average inference latency over the last 5 minutes is greater than 1 second.\")\n",
        "print(\"\")\n",
        "print(\"#### Traffic Checks:\")\n",
        "print(\"\")\n",
        "print(\"*   **Configuration:** Define expected traffic patterns (e.g., normal request volume per minute).\")\n",
        "print(\"*   **Monitoring:** Track the incoming inference request rate.\")\n",
        "print(\"*   **Alerting:** Alert on significant deviations from expected traffic patterns. This could include sudden spikes (indicating unexpected load or potential abuse) or drops (indicating issues upstream preventing requests from reaching the VAE).\")\n",
        "print(\"*   **Example Check:** Alert if the inference request rate drops by more than 50% in the last 10 minutes.\")\n",
        "print(\"\")\n",
        "print(\"#### Error Checks:\")\n",
        "print(\"\")\n",
        "print(\"*   **Configuration:** Set a low tolerance for error rates (e.g., percentage of failed inference requests).\")\n",
        "print(\"*   **Monitoring:** Calculate the rate of errors occurring during inference (e.g., exceptions, invalid outputs).\")\n",
        "print(\"*   **Alerting:** Immediately trigger high-priority alerts if the error rate exceeds the defined threshold. This indicates a critical issue impacting the VAE's functionality.\")\n",
        "print(\"*   **Example Check:** Alert if the error rate for inference requests is greater than 1% over the last 1 minute.\")\n",
        "print(\"\")\n",
        "print(\"#### Saturation Checks:\")\n",
        "print(\"\")\n",
        "print(\"*   **Configuration:** Define thresholds for resource utilization metrics (e.g., CPU usage, GPU utilization, memory usage, quantum device queue length if applicable).\")\n",
        "print(\"*   **Monitoring:** Continuously monitor resource utilization.\")\n",
        "print(\"*   **Alerting:** Trigger alerts if resource utilization exceeds warning or critical thresholds. This indicates that the system is becoming saturated and may soon experience performance degradation or failures.\")\n",
        "print(\"*   **Example Check:** Alert if GPU utilization is consistently above 90% for more than 15 minutes.\")\n",
        "print(\"\")\n",
        "print(\"#### Combining Checks and Context:\")\n",
        "print(\"\")\n",
        "print(\"Automated checks are most effective when combined with context from logs and events. When an alert is triggered, the monitoring system should provide links to relevant logs and events to help quickly diagnose the root cause. For instance, a latency alert might be correlated with logs showing increased time spent in the quantum layer or events indicating recent infrastructure changes.\")\n",
        "print(\"\")\n",
        "print(\"By implementing these automated checks, we can move from reactive problem-solving to proactive issue detection, ensuring the Hybrid VAE remains a reliable and performant tool for end-user research.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### ✅ Automated Checks based on SRE Golden Signals\n",
            "\n",
            "Implementing automated checks based on the SRE Golden Signals (Latency, Traffic, Errors, Saturation) is crucial for proactively identifying and responding to issues with the Hybrid VAE in a production-like environment. These checks act as an early warning system, ensuring that the model continues to meet performance and reliability standards, which directly impacts the quality of end-user research outcomes.\n",
            "\n",
            "Here's how automated checks can be applied to the Hybrid VAE:\n",
            "\n",
            "#### Latency Checks:\n",
            "\n",
            "*   **Configuration:** Set thresholds for acceptable inference latency (e.g., average latency, p95 latency). These thresholds should be based on the requirements of the research workflow.\n",
            "*   **Monitoring:** Continuously monitor the actual inference latency using the collected metrics.\n",
            "*   **Alerting:** Trigger alerts (e.g., email, Slack notification, PagerDuty) if the monitored latency exceeds the defined thresholds for a sustained period. This indicates potential performance degradation.\n",
            "*   **Example Check:** Alert if the average inference latency over the last 5 minutes is greater than 1 second.\n",
            "\n",
            "#### Traffic Checks:\n",
            "\n",
            "*   **Configuration:** Define expected traffic patterns (e.g., normal request volume per minute).\n",
            "*   **Monitoring:** Track the incoming inference request rate.\n",
            "*   **Alerting:** Alert on significant deviations from expected traffic patterns. This could include sudden spikes (indicating unexpected load or potential abuse) or drops (indicating issues upstream preventing requests from reaching the VAE).\n",
            "*   **Example Check:** Alert if the inference request rate drops by more than 50% in the last 10 minutes.\n",
            "\n",
            "#### Error Checks:\n",
            "\n",
            "*   **Configuration:** Set a low tolerance for error rates (e.g., percentage of failed inference requests).\n",
            "*   **Monitoring:** Calculate the rate of errors occurring during inference (e.g., exceptions, invalid outputs).\n",
            "*   **Alerting:** Immediately trigger high-priority alerts if the error rate exceeds the defined threshold. This indicates a critical issue impacting the VAE's functionality.\n",
            "*   **Example Check:** Alert if the error rate for inference requests is greater than 1% over the last 1 minute.\n",
            "\n",
            "#### Saturation Checks:\n",
            "\n",
            "*   **Configuration:** Define thresholds for resource utilization metrics (e.g., CPU usage, GPU utilization, memory usage, quantum device queue length if applicable).\n",
            "*   **Monitoring:** Continuously monitor resource utilization.\n",
            "*   **Alerting:** Trigger alerts if resource utilization exceeds warning or critical thresholds. This indicates that the system is becoming saturated and may soon experience performance degradation or failures.\n",
            "*   **Example Check:** Alert if GPU utilization is consistently above 90% for more than 15 minutes.\n",
            "\n",
            "#### Combining Checks and Context:\n",
            "\n",
            "Automated checks are most effective when combined with context from logs and events. When an alert is triggered, the monitoring system should provide links to relevant logs and events to help quickly diagnose the root cause. For instance, a latency alert might be correlated with logs showing increased time spent in the quantum layer or events indicating recent infrastructure changes.\n",
            "\n",
            "By implementing these automated checks, we can move from reactive problem-solving to proactive issue detection, ensuring the Hybrid VAE remains a reliable and performant tool for end-user research.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "34680117",
        "outputId": "6e429cb0-fd0e-4114-facf-5382d7163d12"
      },
      "source": [
        "# This is a placeholder code block to output markdown content, as direct markdown cell creation is supported.\n",
        "# The actual markdown content will be printed as output.\n",
        "\n",
        "print(\"### 📈 Visualizing Monitoring Data with Streamlit (Illustrative)\")\n",
        "print(\"\")\n",
        "print(\"Visualizing the collected metrics, logs, and alerts is essential for easily understanding the health and performance of the Hybrid VAE and the impact on research outcomes. While full-fledged monitoring dashboards like Grafana are common in production, tools like Streamlit can be used to create interactive dashboards for visualizing monitoring data in a more research-focused or development environment.\")\n",
        "print(\"\")\n",
        "print(\"Streamlit allows quickly building web applications using pure Python. Here's a conceptual outline of how Streamlit could be used to visualize monitoring data for the Hybrid VAE:\")\n",
        "print(\"\")\n",
        "print(\"#### 1. Data Source:\")\n",
        "print(\"\")\n",
        "print(\"The Streamlit application would need access to the collected monitoring data. This could be:\")\n",
        "print(\"*   **MLflow Tracking Server:** Retrieve logged metrics, parameters, and artifacts directly from MLflow.\")\n",
        "print(\"*   **Centralized Logging/Metrics System:** Connect to a database or service where detailed metrics and logs are stored (e.g., a simple file for this notebook's illustration, or a time-series database like Prometheus in a more robust setup).\")\n",
        "print(\"\")\n",
        "print(\"#### 2. Dashboard Components:\")\n",
        "print(\"\")\n",
        "print(\"A Streamlit dashboard could include various components to visualize different aspects of the monitoring data:\")\n",
        "print(\"\")\n",
        "print(\"*   **Key Performance Indicators (KPIs):** Display the current status of key metrics like average inference latency, error rate, and recent training loss.\")\n",
        "print(\"*   **Metric Plots:\n",
        "    *   Line plots showing the trend of training loss, reconstruction error, and KL divergence over epochs.\n",
        "    *   Time-series plots of inference latency and traffic over time.\n",
        "    *   Histograms or distribution plots for quantum expectation values to understand their range and variability.\n",
        "    *   Resource utilization plots (CPU, GPU, memory).\")\n",
        "print(\"*   **Alerts Summary:** Display a list of recent alerts triggered based on the SRE Golden Signals, including the alert type, time, and triggering value.\")\n",
        "print(\"*   **Log Viewer:** Integrate a simple log viewer to display recent log entries, potentially with filtering capabilities.\")\n",
        "print(\"*   **Experiment Comparison:** Allow users to select different MLflow runs and compare their metrics (e.g., loss curves, benchmark times) side-by-side.\")\n",
        "print(\"\")\n",
        "print(\"#### 3. Interactivity:\")\n",
        "print(\"\")\n",
        "print(\"Streamlit's interactive widgets can enhance the dashboard:\")\n",
        "print(\"\")\n",
        "print(\"*   **Time Range Selector:** Allow users to select a specific time range for viewing metrics and logs.\")\n",
        "print(\"*   **Model Version Selector:** If multiple models are tracked in MLflow, allow selecting a specific model version.\")\n",
        "print(\"*   **Filtering:** Provide options to filter logs by severity level or keywords.\")\n",
        "print(\"\")\n",
        "print(\"#### Conceptual Streamlit Code Snippet:\")\n",
        "print(\"\")\n",
        "print(\"```python\")\n",
        "print(\"# Example (conceptual) Streamlit app structure\")\n",
        "print(\"\")\n",
        "print(\"import streamlit as st\")\n",
        "print(\"import pandas as pd\")\n",
        "print(\"import matplotlib.pyplot as plt\")\n",
        "print(\"# import mlflow # Uncomment if fetching data from MLflow\")\n",
        "print(\"\")\n",
        "print(\"st.title('Hybrid VAE Monitoring Dashboard')\")\n",
        "print(\"\")\n",
        "print(\"# --- Load Data (Illustrative - replace with actual data loading) ---\")\n",
        "print(\"# For this example, we'll use the 'metrics' dictionary from the notebook\")\n",
        "print(\"import __main__ # Access variables from the main notebook script\")\n",
        "print(\"monitoring_data = __main__.metrics\") # Access the 'metrics' dictionary\n",
        "\n",
        "print(\"# Convert training loss to a DataFrame\")\n",
        "print(\"train_loss_data = [{'epoch': e, 'loss': d['train_loss']} for e, d in monitoring_data.items() if isinstance(e, int)]\")\n",
        "print(\"train_loss_df = pd.DataFrame(train_loss_data)\")\n",
        "\n",
        "print(\"# Get inference metrics\")\n",
        "print(\"inference_duration = monitoring_data.get('inference_duration', 0)\")\n",
        "print(\"reconstruction_error = monitoring_data.get('inference_reconstruction_error', 0)\")\n",
        "\n",
        "\n",
        "print(\"# --- Display KPIs ---\")\n",
        "print(\"st.header('Key Performance Indicators')\")\n",
        "print(f\"**Last Recorded Inference Duration:** {inference_duration:.4f} seconds\")\n",
        "print(f\"**Last Recorded Reconstruction Error (MSE):** {reconstruction_error:.4f}\")\n",
        "print(f\"**Last Training Epoch Loss:** {train_loss_df['loss'].iloc[-1]:.4f}\" if not train_loss_df.empty else \"**Last Training Epoch Loss:** N/A\")\n",
        "\n",
        "print(\"\")\n",
        "print(\"#### Training Loss Over Epochs\")\n",
        "print(\"if not train_loss_df.empty:\")\n",
        "print(\"    fig, ax = plt.subplots()\")\n",
        "print(\"    ax.plot(train_loss_df['epoch'], train_loss_df['loss'])\")\n",
        "print(\"    ax.set_xlabel('Epoch')\")\n",
        "print(\"    ax.set_ylabel('Training Loss')\")\n",
        "print(\"    st.pyplot(fig)\")\n",
        "print(\"else:\")\n",
        "print(\"    st.write('No training loss data available.')\")\n",
        "\n",
        "print(\"# --- Conceptual Plot for Quantum Expectation Values (requires collecting this data) ---\")\n",
        "print(\"# st.header('Quantum Layer Metrics')\")\n",
        "print(\"# if 'quantum_expval_distribution' in monitoring_data:\")\n",
        "print(\"#     st.subheader('Quantum Expectation Value Distribution')\")\n",
        "print(\"#     fig, ax = plt.subplots()\")\n",
        "print(\"#     ax.hist(monitoring_data['quantum_expval_distribution'], bins=20)\")\n",
        "print(\"#     ax.set_xlabel('Expectation Value')\")\n",
        "print(\"#     ax.set_ylabel('Frequency')\")\n",
        "print(\"#     st.pyplot(fig)\")\n",
        "print(\"# else:\")\n",
        "print(\"#     st.write('Quantum expectation value distribution data not available.')\")\n",
        "\n",
        "\n",
        "print(\"# --- Conceptual Log Viewer (requires collecting logs to a file or database) ---\")\n",
        "print(\"# st.header('Recent Logs')\")\n",
        "print(\"# try:\")\n",
        "print(\"#     with open('app.log', 'r') as f:\")\n",
        "print(\"#         logs = f.readlines()\")\n",
        "print(\"#     for log_entry in logs[-20:]: # Display last 20 logs\")\n",
        "print(\"#         st.text(log_entry.strip())\")\n",
        "print(\"# except FileNotFoundError:\")\n",
        "print(\"#     st.write('Log file not found.')\")\n",
        "\n",
        "print(\"```\")\n",
        "print(\"\")\n",
        "print(\"To run a Streamlit app, you would typically save the code to a `.py` file (e.g., `monitor_app.py`) and run it from your terminal using `streamlit run monitor_app.py`. In this notebook context, this is illustrative.\")\n",
        "print(\"\")\n",
        "print(\"Visualizing monitoring data makes it significantly easier to track the performance and health of the Hybrid VAE, identify issues quickly, and gain insights that can inform both model development and scientific conclusions.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 21) (ipython-input-3109441435.py, line 21)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3109441435.py\"\u001b[0;36m, line \u001b[0;32m21\u001b[0m\n\u001b[0;31m    print(\"*   **Metric Plots:\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DLYdSKXqUFP6",
        "outputId": "2e834080-8505-4eae-a618-b0f681fb1be2"
      },
      "source": [
        "# This is a placeholder code block to output markdown content, as direct markdown cell creation is not supported.\n",
        "# The actual markdown content will be printed as output.\n",
        "\n",
        "print(\"### 📈 Visualizing Monitoring Data with Streamlit (Illustrative)\")\n",
        "print(\"\")\n",
        "print(\"Visualizing the collected metrics, logs, and alerts is essential for easily understanding the health and performance of the Hybrid VAE and the impact on research outcomes. While full-fledged monitoring dashboards like Grafana are common in production, tools like Streamlit can be used to create interactive dashboards for visualizing monitoring data in a more research-focused or development environment.\")\n",
        "print(\"\")\n",
        "print(\"Streamlit allows quickly building web applications using pure Python. Here's a conceptual outline of how Streamlit could be used to visualize monitoring data for the Hybrid VAE:\")\n",
        "print(\"\")\n",
        "print(\"#### 1. Data Source:\")\n",
        "print(\"\")\n",
        "print(\"The Streamlit application would need access to the collected monitoring data. This could be:\")\n",
        "print(\"*   **MLflow Tracking Server:** Retrieve logged metrics, parameters, and artifacts directly from MLflow.\")\n",
        "print(\"*   **Centralized Logging/Metrics System:** Connect to a database or service where detailed metrics and logs are stored (e.g., a simple file for this notebook's illustration, or a time-series database like Prometheus in a more robust setup).\")\n",
        "print(\"\")\n",
        "print(\"#### 2. Dashboard Components:\")\n",
        "print(\"\")\n",
        "print(\"A Streamlit dashboard could include various components to visualize different aspects of the monitoring data:\")\n",
        "print(\"\")\n",
        "print(\"*   **Key Performance Indicators (KPIs):** Display the current status of key metrics like average inference latency, error rate, and recent training loss.\")\n",
        "print(\"*   **Metric Plots:**\")\n",
        "print(\"    *   Line plots showing the trend of training loss, reconstruction error, and KL divergence over epochs.\")\n",
        "print(\"    *   Time-series plots of inference latency and traffic over time.\")\n",
        "print(\"    *   Histograms or distribution plots for quantum expectation values to understand their range and variability.\")\n",
        "print(\"    *   Resource utilization plots (CPU, GPU, memory).\")\n",
        "print(\"*   **Alerts Summary:** Display a list of recent alerts triggered based on the SRE Golden Signals, including the alert type, time, and triggering value.\")\n",
        "print(\"*   **Log Viewer:** Integrate a simple log viewer to display recent log entries, potentially with filtering capabilities.\")\n",
        "print(\"*   **Experiment Comparison:** Allow users to select different MLflow runs and compare their metrics (e.g., loss curves, benchmark times) side-by-side.\")\n",
        "print(\"\")\n",
        "print(\"#### 3. Interactivity:\")\n",
        "print(\"\")\n",
        "print(\"Streamlit's interactive widgets can enhance the dashboard:\")\n",
        "print(\"\")\n",
        "print(\"*   **Time Range Selector:** Allow users to select a specific time range for viewing metrics and logs.\")\n",
        "print(\"*   **Model Version Selector:** If multiple models are tracked in MLflow, allow selecting a specific model version.\")\n",
        "print(\"*   **Filtering:** Provide options to filter logs by severity level or keywords.\")\n",
        "print(\"\")\n",
        "print(\"#### Conceptual Streamlit Code Snippet:\")\n",
        "print(\"\")\n",
        "print(\"```python\")\n",
        "print(\"# Example (conceptual) Streamlit app structure\")\n",
        "print(\"\")\n",
        "print(\"import streamlit as st\")\n",
        "print(\"import pandas as pd\")\n",
        "print(\"import matplotlib.pyplot as plt\")\n",
        "print(\"# import mlflow # Uncomment if fetching data from MLflow\")\n",
        "print(\"\")\n",
        "print(\"st.title('Hybrid VAE Monitoring Dashboard')\")\n",
        "print(\"\")\n",
        "print(\"# --- Load Data (Illustrative - replace with actual data loading) ---\")\n",
        "print(\"# For this example, we'll use the 'metrics' dictionary from the notebook\")\n",
        "print(\"import __main__ # Access variables from the main notebook script\")\n",
        "print(\"monitoring_data = __main__.metrics\") # Access the 'metrics' dictionary\n",
        "\n",
        "print(\"# Convert training loss to a DataFrame\")\n",
        "print(\"train_loss_data = [{'epoch': e, 'loss': d['train_loss']} for e, d in monitoring_data.items() if isinstance(e, int)]\")\n",
        "print(\"train_loss_df = pd.DataFrame(train_loss_data)\")\n",
        "\n",
        "print(\"# Get inference metrics\")\n",
        "print(\"inference_duration = monitoring_data.get('inference_duration', 0)\")\n",
        "print(\"reconstruction_error = monitoring_data.get('inference_reconstruction_error', 0)\")\n",
        "\n",
        "\n",
        "print(\"# --- Display KPIs ---\")\n",
        "print(\"st.header('Key Performance Indicators')\")\n",
        "print(f\"**Last Recorded Inference Duration:** {inference_duration:.4f} seconds\")\n",
        "print(f\"**Last Recorded Reconstruction Error (MSE):** {reconstruction_error:.4f}\")\n",
        "print(f\"**Last Training Epoch Loss:** {train_loss_df['loss'].iloc[-1]:.4f}\" if not train_loss_df.empty else \"**Last Training Epoch Loss:** N/A\")\n",
        "\n",
        "print(\"\")\n",
        "print(\"#### Training Loss Over Epochs\")\n",
        "print(\"if not train_loss_df.empty:\")\n",
        "print(\"    fig, ax = plt.subplots()\")\n",
        "print(\"    ax.plot(train_loss_df['epoch'], train_loss_df['loss'])\")\n",
        "print(\"    ax.set_xlabel('Epoch')\")\n",
        "print(\"    ax.set_ylabel('Training Loss')\")\n",
        "print(\"    st.pyplot(fig)\")\n",
        "print(\"else:\")\n",
        "print(\"    st.write('No training loss data available.')\")\n",
        "\n",
        "print(\"# --- Conceptual Plot for Quantum Expectation Values (requires collecting this data) ---\")\n",
        "print(\"# st.header('Quantum Layer Metrics')\")\n",
        "print(\"# if 'quantum_expval_distribution' in monitoring_data:\")\n",
        "print(\"#     st.subheader('Quantum Expectation Value Distribution')\")\n",
        "print(\"#     fig, ax = plt.subplots()\")\n",
        "print(\"#     ax.hist(monitoring_data['quantum_expval_distribution'], bins=20)\")\n",
        "print(\"#     ax.set_xlabel('Expectation Value')\")\n",
        "print(\"#     ax.set_ylabel('Frequency')\")\n",
        "print(\"#     st.pyplot(fig)\")\n",
        "print(\"# else:\")\n",
        "print(\"#     st.write('Quantum expectation value distribution data not available.')\")\n",
        "\n",
        "\n",
        "print(\"# --- Conceptual Log Viewer (requires collecting logs to a file or database) ---\")\n",
        "print(\"# st.header('Recent Logs')\")\n",
        "print(\"# try:\")\n",
        "print(\"#     with open('app.log', 'r') as f:\")\n",
        "print(\"#         logs = f.readlines()\")\n",
        "print(\"#     for log_entry in logs[-20:]: # Display last 20 logs\")\n",
        "print(\"#         st.text(log_entry.strip())\")\n",
        "print(\"# except FileNotFoundError:\")\n",
        "print(\"#     st.write('Log file not found.')\")\n",
        "\n",
        "print(\"```\")\n",
        "print(\"\")\n",
        "print(\"To run a Streamlit app, you would typically save the code to a `.py` file (e.g., `monitor_app.py`) and run it from your terminal using `streamlit run monitor_app.py`. In this notebook context, this is illustrative.\")\n",
        "print(\"\")\n",
        "print(\"Visualizing monitoring data makes it significantly easier to track the performance and health of the Hybrid VAE, identify issues quickly, and gain insights that can inform both model development and scientific conclusions.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### 📈 Visualizing Monitoring Data with Streamlit (Illustrative)\n",
            "\n",
            "Visualizing the collected metrics, logs, and alerts is essential for easily understanding the health and performance of the Hybrid VAE and the impact on research outcomes. While full-fledged monitoring dashboards like Grafana are common in production, tools like Streamlit can be used to create interactive dashboards for visualizing monitoring data in a more research-focused or development environment.\n",
            "\n",
            "Streamlit allows quickly building web applications using pure Python. Here's a conceptual outline of how Streamlit could be used to visualize monitoring data for the Hybrid VAE:\n",
            "\n",
            "#### 1. Data Source:\n",
            "\n",
            "The Streamlit application would need access to the collected monitoring data. This could be:\n",
            "*   **MLflow Tracking Server:** Retrieve logged metrics, parameters, and artifacts directly from MLflow.\n",
            "*   **Centralized Logging/Metrics System:** Connect to a database or service where detailed metrics and logs are stored (e.g., a simple file for this notebook's illustration, or a time-series database like Prometheus in a more robust setup).\n",
            "\n",
            "#### 2. Dashboard Components:\n",
            "\n",
            "A Streamlit dashboard could include various components to visualize different aspects of the monitoring data:\n",
            "\n",
            "*   **Key Performance Indicators (KPIs):** Display the current status of key metrics like average inference latency, error rate, and recent training loss.\n",
            "*   **Metric Plots:**\n",
            "    *   Line plots showing the trend of training loss, reconstruction error, and KL divergence over epochs.\n",
            "    *   Time-series plots of inference latency and traffic over time.\n",
            "    *   Histograms or distribution plots for quantum expectation values to understand their range and variability.\n",
            "    *   Resource utilization plots (CPU, GPU, memory).\n",
            "*   **Alerts Summary:** Display a list of recent alerts triggered based on the SRE Golden Signals, including the alert type, time, and triggering value.\n",
            "*   **Log Viewer:** Integrate a simple log viewer to display recent log entries, potentially with filtering capabilities.\n",
            "*   **Experiment Comparison:** Allow users to select different MLflow runs and compare their metrics (e.g., loss curves, benchmark times) side-by-side.\n",
            "\n",
            "#### 3. Interactivity:\n",
            "\n",
            "Streamlit's interactive widgets can enhance the dashboard:\n",
            "\n",
            "*   **Time Range Selector:** Allow users to select a specific time range for viewing metrics and logs.\n",
            "*   **Model Version Selector:** If multiple models are tracked in MLflow, allow selecting a specific model version.\n",
            "*   **Filtering:** Provide options to filter logs by severity level or keywords.\n",
            "\n",
            "#### Conceptual Streamlit Code Snippet:\n",
            "\n",
            "```python\n",
            "# Example (conceptual) Streamlit app structure\n",
            "\n",
            "import streamlit as st\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt\n",
            "# import mlflow # Uncomment if fetching data from MLflow\n",
            "\n",
            "st.title('Hybrid VAE Monitoring Dashboard')\n",
            "\n",
            "# --- Load Data (Illustrative - replace with actual data loading) ---\n",
            "# For this example, we'll use the 'metrics' dictionary from the notebook\n",
            "import __main__ # Access variables from the main notebook script\n",
            "monitoring_data = __main__.metrics\n",
            "# Convert training loss to a DataFrame\n",
            "train_loss_data = [{'epoch': e, 'loss': d['train_loss']} for e, d in monitoring_data.items() if isinstance(e, int)]\n",
            "train_loss_df = pd.DataFrame(train_loss_data)\n",
            "# Get inference metrics\n",
            "inference_duration = monitoring_data.get('inference_duration', 0)\n",
            "reconstruction_error = monitoring_data.get('inference_reconstruction_error', 0)\n",
            "# --- Display KPIs ---\n",
            "st.header('Key Performance Indicators')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'inference_duration' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3571939552.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"# --- Display KPIs ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"st.header('Key Performance Indicators')\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"**Last Recorded Inference Duration:** {inference_duration:.4f} seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"**Last Recorded Reconstruction Error (MSE):** {reconstruction_error:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"**Last Training Epoch Loss:** {train_loss_df['loss'].iloc[-1]:.4f}\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrain_loss_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"**Last Training Epoch Loss:** N/A\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'inference_duration' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzV3hs3DULjE",
        "outputId": "b90d0075-c471-4879-ae3c-c94429c7601a"
      },
      "source": [
        "# This is a placeholder code block to output markdown content, as direct markdown cell creation is not supported.\n",
        "# The actual markdown content will be printed as output.\n",
        "\n",
        "print(\"### 📈 Visualizing Monitoring Data with Streamlit (Illustrative)\")\n",
        "print(\"\")\n",
        "print(\"Visualizing the collected metrics, logs, and alerts is essential for easily understanding the health and performance of the Hybrid VAE and the impact on research outcomes. While full-fledged monitoring dashboards like Grafana are common in production, tools like Streamlit can be used to create interactive dashboards for visualizing monitoring data in a more research-focused or development environment.\")\n",
        "print(\"\")\n",
        "print(\"Streamlit allows quickly building web applications using pure Python. Here's a conceptual outline of how Streamlit could be used to visualize monitoring data for the Hybrid VAE:\")\n",
        "print(\"\")\n",
        "print(\"#### 1. Data Source:\")\n",
        "print(\"\")\n",
        "print(\"The Streamlit application would need access to the collected monitoring data. This could be:\")\n",
        "print(\"*   **MLflow Tracking Server:** Retrieve logged metrics, parameters, and artifacts directly from MLflow.\")\n",
        "print(\"*   **Centralized Logging/Metrics System:** Connect to a database or service where detailed metrics and logs are stored (e.g., a simple file for this notebook's illustration, or a time-series database like Prometheus in a more robust setup).\")\n",
        "print(\"\")\n",
        "print(\"#### 2. Dashboard Components:\")\n",
        "print(\"\")\n",
        "print(\"A Streamlit dashboard could include various components to visualize different aspects of the monitoring data:\")\n",
        "print(\"\")\n",
        "print(\"*   **Key Performance Indicators (KPIs):** Display the current status of key metrics like average inference latency, error rate, and recent training loss.\")\n",
        "print(\"*   **Metric Plots:**\")\n",
        "print(\"    *   Line plots showing the trend of training loss, reconstruction error, and KL divergence over epochs.\")\n",
        "print(\"    *   Time-series plots of inference latency and traffic over time.\")\n",
        "print(\"    *   Histograms or distribution plots for quantum expectation values to understand their range and variability.\")\n",
        "print(\"    *   Resource utilization plots (CPU, GPU, memory).\")\n",
        "print(\"*   **Alerts Summary:** Display a list of recent alerts triggered based on the SRE Golden Signals, including the alert type, time, and triggering value.\")\n",
        "print(\"*   **Log Viewer:** Integrate a simple log viewer to display recent log entries, potentially with filtering capabilities.\")\n",
        "print(\"*   **Experiment Comparison:** Allow users to select different MLflow runs and compare their metrics (e.g., loss curves, benchmark times) side-by-side.\")\n",
        "print(\"\")\n",
        "print(\"#### 3. Interactivity:\")\n",
        "print(\"\")\n",
        "print(\"Streamlit's interactive widgets can enhance the dashboard:\")\n",
        "print(\"\")\n",
        "print(\"*   **Time Range Selector:** Allow users to select a specific time range for viewing metrics and logs.\")\n",
        "print(\"*   **Model Version Selector:** If multiple models are tracked in MLflow, allow selecting a specific model version.\")\n",
        "print(\"*   **Filtering:** Provide options to filter logs by severity level or keywords.\")\n",
        "print(\"\")\n",
        "print(\"#### Conceptual Streamlit Code Snippet:\")\n",
        "print(\"\")\n",
        "print(\"```python\")\n",
        "print(\"# Example (conceptual) Streamlit app structure\")\n",
        "print(\"\")\n",
        "print(\"import streamlit as st\")\n",
        "print(\"import pandas as pd\")\n",
        "print(\"import matplotlib.pyplot as plt\")\n",
        "print(\"# import mlflow # Uncomment if fetching data from MLflow\")\n",
        "print(\"\")\n",
        "print(\"st.title('Hybrid VAE Monitoring Dashboard')\")\n",
        "print(\"\")\n",
        "print(\"# --- Load Data (Illustrative - replace with actual data loading) ---\")\n",
        "print(\"# For this example, we'll use the 'metrics' dictionary from the notebook\")\n",
        "print(\"# import __main__ # Access variables from the main notebook script (Not possible in a real Streamlit app)\")\n",
        "print(\"# monitoring_data = __main__.metrics\") # Access the 'metrics' dictionary\n",
        "\n",
        "print(\"# In a real Streamlit app, load data from your source (MLflow, database, etc.)\")\n",
        "print(\"# Example: Load data from a simulated source or MLflow run\")\n",
        "print(\"monitoring_data = {\")\n",
        "print(\"    0: {'train_loss': 1560.5},\")\n",
        "print(\"    1: {'train_loss': 1553.1},\")\n",
        "print(\"    9: {'train_loss': 1507.0},\")\n",
        "print(\"    'inference_duration': 0.0108,\")\n",
        "print(\"    'inference_reconstruction_error': 15.446\")\n",
        "print(\"}\")\n",
        "\n",
        "\n",
        "print(\"# Convert training loss to a DataFrame\")\n",
        "print(\"train_loss_data = [{'epoch': e, 'loss': d['train_loss']} for e, d in monitoring_data.items() if isinstance(e, int)]\")\n",
        "print(\"train_loss_df = pd.DataFrame(train_loss_data)\")\n",
        "\n",
        "print(\"# Get inference metrics\")\n",
        "print(\"inference_duration = monitoring_data.get('inference_duration', 0)\")\n",
        "print(\"reconstruction_error = monitoring_data.get('inference_reconstruction_error', 0)\")\n",
        "\n",
        "\n",
        "print(\"# --- Display KPIs ---\")\n",
        "print(\"st.header('Key Performance Indicators')\")\n",
        "print(\"st.write(f'**Last Recorded Inference Duration:** {inference_duration:.4f} seconds')\") # Use st.write\n",
        "print(\"st.write(f'**Last Recorded Reconstruction Error (MSE):** {reconstruction_error:.4f}')\") # Use st.write\n",
        "print(\"st.write(f\\\"**Last Training Epoch Loss:** {train_loss_df['loss'].iloc[-1]:.4f}\\\" if not train_loss_df.empty else \\\"**Last Training Epoch Loss:** N/A\\\")\") # Use st.write\n",
        "\n",
        "\n",
        "print(\"\")\n",
        "print(\"#### Training Loss Over Epochs\")\n",
        "print(\"if not train_loss_df.empty:\")\n",
        "print(\"    fig, ax = plt.subplots()\")\n",
        "print(\"    ax.plot(train_loss_df['epoch'], train_loss_df['loss'])\")\n",
        "print(\"    ax.set_xlabel('Epoch')\")\n",
        "print(\"    ax.set_ylabel('Training Loss')\")\n",
        "print(\"    st.pyplot(fig)\")\n",
        "print(\"else:\")\n",
        "print(\"    st.write('No training loss data available.')\")\n",
        "\n",
        "print(\"# --- Conceptual Plot for Quantum Expectation Values (requires collecting this data) ---\")\n",
        "print(\"# st.header('Quantum Layer Metrics')\")\n",
        "print(\"# if 'quantum_expval_distribution' in monitoring_data:\")\n",
        "print(\"#     st.subheader('Quantum Expectation Value Distribution')\")\n",
        "print(\"#     fig, ax = plt.subplots()\")\n",
        "print(\"#     ax.hist(monitoring_data['quantum_expval_distribution'], bins=20)\")\n",
        "print(\"#     ax.set_xlabel('Expectation Value')\")\n",
        "print(\"#     ax.set_ylabel('Frequency')\")\n",
        "print(\"#     st.pyplot(fig)\")\n",
        "print(\"# else:\")\n",
        "print(\"#     st.write('Quantum expectation value distribution data not available.')\")\n",
        "\n",
        "\n",
        "print(\"# --- Conceptual Log Viewer (requires collecting logs to a file or database) ---\")\n",
        "print(\"# st.header('Recent Logs')\")\n",
        "print(\"# try:\")\n",
        "print(\"#     with open('app.log', 'r') as f:\")\n",
        "print(\"#         logs = f.readlines()\")\n",
        "print(\"#     for log_entry in logs[-20:]: # Display last 20 logs\")\n",
        "print(\"#         st.text(log_entry.strip())\")\n",
        "print(\"# except FileNotFoundError:\")\n",
        "print(\"#     st.write('Log file not found.')\")\n",
        "\n",
        "print(\"```\")\n",
        "print(\"\")\n",
        "print(\"To run a Streamlit app, you would typically save the code to a `.py` file (e.g., `monitor_app.py`) and run it from your terminal using `streamlit run monitor_app.py`. In this notebook context, this is illustrative.\")\n",
        "print(\"\")\n",
        "print(\"Visualizing monitoring data makes it significantly easier to track the performance and health of the Hybrid VAE, identify issues quickly, and gain insights that can inform both model development and scientific conclusions.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### 📈 Visualizing Monitoring Data with Streamlit (Illustrative)\n",
            "\n",
            "Visualizing the collected metrics, logs, and alerts is essential for easily understanding the health and performance of the Hybrid VAE and the impact on research outcomes. While full-fledged monitoring dashboards like Grafana are common in production, tools like Streamlit can be used to create interactive dashboards for visualizing monitoring data in a more research-focused or development environment.\n",
            "\n",
            "Streamlit allows quickly building web applications using pure Python. Here's a conceptual outline of how Streamlit could be used to visualize monitoring data for the Hybrid VAE:\n",
            "\n",
            "#### 1. Data Source:\n",
            "\n",
            "The Streamlit application would need access to the collected monitoring data. This could be:\n",
            "*   **MLflow Tracking Server:** Retrieve logged metrics, parameters, and artifacts directly from MLflow.\n",
            "*   **Centralized Logging/Metrics System:** Connect to a database or service where detailed metrics and logs are stored (e.g., a simple file for this notebook's illustration, or a time-series database like Prometheus in a more robust setup).\n",
            "\n",
            "#### 2. Dashboard Components:\n",
            "\n",
            "A Streamlit dashboard could include various components to visualize different aspects of the monitoring data:\n",
            "\n",
            "*   **Key Performance Indicators (KPIs):** Display the current status of key metrics like average inference latency, error rate, and recent training loss.\n",
            "*   **Metric Plots:**\n",
            "    *   Line plots showing the trend of training loss, reconstruction error, and KL divergence over epochs.\n",
            "    *   Time-series plots of inference latency and traffic over time.\n",
            "    *   Histograms or distribution plots for quantum expectation values to understand their range and variability.\n",
            "    *   Resource utilization plots (CPU, GPU, memory).\n",
            "*   **Alerts Summary:** Display a list of recent alerts triggered based on the SRE Golden Signals, including the alert type, time, and triggering value.\n",
            "*   **Log Viewer:** Integrate a simple log viewer to display recent log entries, potentially with filtering capabilities.\n",
            "*   **Experiment Comparison:** Allow users to select different MLflow runs and compare their metrics (e.g., loss curves, benchmark times) side-by-side.\n",
            "\n",
            "#### 3. Interactivity:\n",
            "\n",
            "Streamlit's interactive widgets can enhance the dashboard:\n",
            "\n",
            "*   **Time Range Selector:** Allow users to select a specific time range for viewing metrics and logs.\n",
            "*   **Model Version Selector:** If multiple models are tracked in MLflow, allow selecting a specific model version.\n",
            "*   **Filtering:** Provide options to filter logs by severity level or keywords.\n",
            "\n",
            "#### Conceptual Streamlit Code Snippet:\n",
            "\n",
            "```python\n",
            "# Example (conceptual) Streamlit app structure\n",
            "\n",
            "import streamlit as st\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt\n",
            "# import mlflow # Uncomment if fetching data from MLflow\n",
            "\n",
            "st.title('Hybrid VAE Monitoring Dashboard')\n",
            "\n",
            "# --- Load Data (Illustrative - replace with actual data loading) ---\n",
            "# For this example, we'll use the 'metrics' dictionary from the notebook\n",
            "# import __main__ # Access variables from the main notebook script (Not possible in a real Streamlit app)\n",
            "# monitoring_data = __main__.metrics\n",
            "# In a real Streamlit app, load data from your source (MLflow, database, etc.)\n",
            "# Example: Load data from a simulated source or MLflow run\n",
            "monitoring_data = {\n",
            "    0: {'train_loss': 1560.5},\n",
            "    1: {'train_loss': 1553.1},\n",
            "    9: {'train_loss': 1507.0},\n",
            "    'inference_duration': 0.0108,\n",
            "    'inference_reconstruction_error': 15.446\n",
            "}\n",
            "# Convert training loss to a DataFrame\n",
            "train_loss_data = [{'epoch': e, 'loss': d['train_loss']} for e, d in monitoring_data.items() if isinstance(e, int)]\n",
            "train_loss_df = pd.DataFrame(train_loss_data)\n",
            "# Get inference metrics\n",
            "inference_duration = monitoring_data.get('inference_duration', 0)\n",
            "reconstruction_error = monitoring_data.get('inference_reconstruction_error', 0)\n",
            "# --- Display KPIs ---\n",
            "st.header('Key Performance Indicators')\n",
            "st.write(f'**Last Recorded Inference Duration:** {inference_duration:.4f} seconds')\n",
            "st.write(f'**Last Recorded Reconstruction Error (MSE):** {reconstruction_error:.4f}')\n",
            "st.write(f\"**Last Training Epoch Loss:** {train_loss_df['loss'].iloc[-1]:.4f}\" if not train_loss_df.empty else \"**Last Training Epoch Loss:** N/A\")\n",
            "\n",
            "#### Training Loss Over Epochs\n",
            "if not train_loss_df.empty:\n",
            "    fig, ax = plt.subplots()\n",
            "    ax.plot(train_loss_df['epoch'], train_loss_df['loss'])\n",
            "    ax.set_xlabel('Epoch')\n",
            "    ax.set_ylabel('Training Loss')\n",
            "    st.pyplot(fig)\n",
            "else:\n",
            "    st.write('No training loss data available.')\n",
            "# --- Conceptual Plot for Quantum Expectation Values (requires collecting this data) ---\n",
            "# st.header('Quantum Layer Metrics')\n",
            "# if 'quantum_expval_distribution' in monitoring_data:\n",
            "#     st.subheader('Quantum Expectation Value Distribution')\n",
            "#     fig, ax = plt.subplots()\n",
            "#     ax.hist(monitoring_data['quantum_expval_distribution'], bins=20)\n",
            "#     ax.set_xlabel('Expectation Value')\n",
            "#     ax.set_ylabel('Frequency')\n",
            "#     st.pyplot(fig)\n",
            "# else:\n",
            "#     st.write('Quantum expectation value distribution data not available.')\n",
            "# --- Conceptual Log Viewer (requires collecting logs to a file or database) ---\n",
            "# st.header('Recent Logs')\n",
            "# try:\n",
            "#     with open('app.log', 'r') as f:\n",
            "#         logs = f.readlines()\n",
            "#     for log_entry in logs[-20:]: # Display last 20 logs\n",
            "#         st.text(log_entry.strip())\n",
            "# except FileNotFoundError:\n",
            "#     st.write('Log file not found.')\n",
            "```\n",
            "\n",
            "To run a Streamlit app, you would typically save the code to a `.py` file (e.g., `monitor_app.py`) and run it from your terminal using `streamlit run monitor_app.py`. In this notebook context, this is illustrative.\n",
            "\n",
            "Visualizing monitoring data makes it significantly easier to track the performance and health of the Hybrid VAE, identify issues quickly, and gain insights that can inform both model development and scientific conclusions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcb18f2a"
      },
      "source": [
        "## Refine metrics collection\n",
        "\n",
        "Detail the specific metrics to collect for the Hybrid VAE, focusing on quantum layer performance and output characteristics.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a89d037",
        "outputId": "bd851771-062e-42e1-ec2a-9ba54ce2bbdc"
      },
      "source": [
        "# This is a placeholder code block to output markdown content, as direct markdown cell creation is not supported.\n",
        "# The actual markdown content will be printed as output.\n",
        "\n",
        "print(\"### 🧬 Detailed Metrics for Hybrid VAE Monitoring\")\n",
        "print(\"\")\n",
        "print(\"Beyond the standard training and inference times, a Hybrid Quantum-Classical VAE requires the collection of more detailed and specific metrics to gain a deeper understanding of its performance, behavior, and the contribution of its quantum component. This is particularly crucial for debugging, optimization, and ensuring the reliability of the model in research workflows.\")\n",
        "print(\"\")\n",
        "print(\"Here are specific metrics to collect, categorized by the component of the VAE:\")\n",
        "print(\"\")\n",
        "print(\"#### Classical Encoder Metrics:\")\n",
        "print(\"\")\n",
        "print(\"*   **Processing Time:** The time taken for the classical encoder to process an input sample or batch. This helps identify if the classical encoding is a bottleneck.\")\n",
        "print(\"*   **Latent Space Parameter Distribution (Mean, Variance):** Statistics of the output of the encoder (mean and log-variance for both classical and quantum latent dimensions). Monitoring these distributions helps ensure the encoder is producing meaningful latent representations and that the reparameterization trick is stable.\")\n",
        "print(\"\")\n",
        "print(\"#### Quantum Layer Metrics:\")\n",
        "print(\"\")\n",
        "print(\"*   **Execution Time:** The time taken for the quantum circuit to execute for a single set of inputs. This is a critical metric for understanding the computational cost of the quantum part, especially when using simulators or real quantum hardware.\")\n",
        "print(\"*   **Quantum Expectation Value Distribution (Mean, Variance, Min, Max):** Statistics of the expectation values returned by the quantum circuit. Analyzing this distribution provides insights into the range and variability of the quantum output, which directly contributes to the latent space.\")\n",
        "print(\"*   **Number of Quantum Circuit Evaluations:** The total number of times the quantum circuit is evaluated during training (per epoch or per batch) and inference (per sample or per batch). This is a measure of the quantum computational workload.\")\n",
        "print(\"\")\n",
        "print(\"#### Classical Decoder Metrics:\")\n",
        "print(\"\")\n",
        "print(\"*   **Processing Time:** The time taken for the classical decoder to reconstruct the output from the latent space. Helps identify decoding bottlenecks.\")\n",
        "print(\"*   **Reconstructed Data Distribution (Mean, Variance):** Statistics of the output of the decoder. Comparing this to the input data distribution can indicate how well the VAE is preserving the original data characteristics.\")\n",
        "print(\"\")\n",
        "print(\"#### Overall VAE Metrics:\")\n",
        "print(\"\")\n",
        "print(\"*   **Total Inference Time:** The total time taken for a full forward pass during inference (from input to reconstructed output). This is a key end-user-facing performance metric.\")\n",
        "print(\"*   **Total Training Time per Epoch:** The total time taken to complete one training epoch. Useful for tracking training progress and efficiency.\")\n",
        "print(\"*   **Reconstruction Error:** The difference between the original input and the reconstructed output (e.g., using Mean Squared Error). A primary measure of the VAE's ability to capture data patterns.\")\n",
        "print(\"*   **KL Divergence Loss:** The regularization term in the VAE loss, measuring the divergence between the learned latent distribution and a prior distribution (e.g., a standard normal distribution). Monitoring this helps understand how well the latent space is being regularized.\")\n",
        "print(\"*   **Total VAE Loss:** The combined reconstruction and KL divergence loss. The primary metric for tracking overall model convergence during training.\")\n",
        "print(\"\")\n",
        "print(\"Collecting these detailed metrics provides a comprehensive view of the Hybrid VAE's performance and behavior, allowing for targeted optimization and a better understanding of the quantum layer's impact on the overall model and the resulting research outcomes.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### 🧬 Detailed Metrics for Hybrid VAE Monitoring\n",
            "\n",
            "Beyond the standard training and inference times, a Hybrid Quantum-Classical VAE requires the collection of more detailed and specific metrics to gain a deeper understanding of its performance, behavior, and the contribution of its quantum component. This is particularly crucial for debugging, optimization, and ensuring the reliability of the model in research workflows.\n",
            "\n",
            "Here are specific metrics to collect, categorized by the component of the VAE:\n",
            "\n",
            "#### Classical Encoder Metrics:\n",
            "\n",
            "*   **Processing Time:** The time taken for the classical encoder to process an input sample or batch. This helps identify if the classical encoding is a bottleneck.\n",
            "*   **Latent Space Parameter Distribution (Mean, Variance):** Statistics of the output of the encoder (mean and log-variance for both classical and quantum latent dimensions). Monitoring these distributions helps ensure the encoder is producing meaningful latent representations and that the reparameterization trick is stable.\n",
            "\n",
            "#### Quantum Layer Metrics:\n",
            "\n",
            "*   **Execution Time:** The time taken for the quantum circuit to execute for a single set of inputs. This is a critical metric for understanding the computational cost of the quantum part, especially when using simulators or real quantum hardware.\n",
            "*   **Quantum Expectation Value Distribution (Mean, Variance, Min, Max):** Statistics of the expectation values returned by the quantum circuit. Analyzing this distribution provides insights into the range and variability of the quantum output, which directly contributes to the latent space.\n",
            "*   **Number of Quantum Circuit Evaluations:** The total number of times the quantum circuit is evaluated during training (per epoch or per batch) and inference (per sample or per batch). This is a measure of the quantum computational workload.\n",
            "\n",
            "#### Classical Decoder Metrics:\n",
            "\n",
            "*   **Processing Time:** The time taken for the classical decoder to reconstruct the output from the latent space. Helps identify decoding bottlenecks.\n",
            "*   **Reconstructed Data Distribution (Mean, Variance):** Statistics of the output of the decoder. Comparing this to the input data distribution can indicate how well the VAE is preserving the original data characteristics.\n",
            "\n",
            "#### Overall VAE Metrics:\n",
            "\n",
            "*   **Total Inference Time:** The total time taken for a full forward pass during inference (from input to reconstructed output). This is a key end-user-facing performance metric.\n",
            "*   **Total Training Time per Epoch:** The total time taken to complete one training epoch. Useful for tracking training progress and efficiency.\n",
            "*   **Reconstruction Error:** The difference between the original input and the reconstructed output (e.g., using Mean Squared Error). A primary measure of the VAE's ability to capture data patterns.\n",
            "*   **KL Divergence Loss:** The regularization term in the VAE loss, measuring the divergence between the learned latent distribution and a prior distribution (e.g., a standard normal distribution). Monitoring this helps understand how well the latent space is being regularized.\n",
            "*   **Total VAE Loss:** The combined reconstruction and KL divergence loss. The primary metric for tracking overall model convergence during training.\n",
            "\n",
            "Collecting these detailed metrics provides a comprehensive view of the Hybrid VAE's performance and behavior, allowing for targeted optimization and a better understanding of the quantum layer's impact on the overall model and the resulting research outcomes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18f07542",
        "outputId": "e1a4d41c-1cb1-48f8-f978-53522da689c0"
      },
      "source": [
        "# This is a placeholder code block to output markdown content, as direct markdown cell creation is not supported.\n",
        "# The actual markdown content will be printed as output.\n",
        "\n",
        "print(\"### ✅ Automated Checks based on SRE Golden Signals\")\n",
        "print(\"\")\n",
        "print(\"Implementing automated checks based on the SRE Golden Signals (Latency, Traffic, Errors, Saturation) is crucial for proactively identifying and responding to issues with the Hybrid VAE in a production-like environment. These checks act as an early warning system, ensuring that the model continues to meet performance and reliability standards, which directly impacts the quality of end-user research outcomes.\")\n",
        "print(\"\")\n",
        "print(\"Here's how automated checks can be applied to the Hybrid VAE:\")\n",
        "print(\"\")\n",
        "print(\"#### Latency Checks:\")\n",
        "print(\"\")\n",
        "print(\"*   **Configuration:** Set thresholds for acceptable inference latency (e.g., average latency, p95 latency). These thresholds should be based on the requirements of the research workflow.\")\n",
        "print(\"*   **Monitoring:** Continuously monitor the actual inference latency using the collected metrics.\")\n",
        "print(\"*   **Alerting:** Trigger alerts (e.g., email, Slack notification, PagerDuty) if the monitored latency exceeds the defined thresholds for a sustained period. This indicates potential performance degradation.\")\n",
        "print(\"*   **Example Check:** Alert if the average inference latency over the last 5 minutes is greater than 1 second.\")\n",
        "print(\"\")\n",
        "print(\"#### Traffic Checks:\")\n",
        "print(\"\")\n",
        "print(\"*   **Configuration:** Define expected traffic patterns (e.g., normal request volume per minute).\")\n",
        "print(\"*   **Monitoring:** Track the incoming inference request rate.\")\n",
        "print(\"*   **Alerting:** Alert on significant deviations from expected traffic patterns. This could include sudden spikes (indicating unexpected load or potential abuse) or drops (indicating issues upstream preventing requests from reaching the VAE).\")\n",
        "print(\"*   **Example Check:** Alert if the inference request rate drops by more than 50% in the last 10 minutes.\")\n",
        "print(\"\")\n",
        "print(\"#### Error Checks:\")\n",
        "print(\"\")\n",
        "print(\"*   **Configuration:** Set a low tolerance for error rates (e.g., percentage of failed inference requests).\")\n",
        "print(\"*   **Monitoring:** Calculate the rate of errors occurring during inference (e.g., exceptions, invalid outputs).\")\n",
        "print(\"*   **Alerting:** Immediately trigger high-priority alerts if the error rate exceeds the defined threshold. This indicates a critical issue impacting the VAE's functionality.\")\n",
        "print(\"*   **Example Check:** Alert if the error rate for inference requests is greater than 1% over the last 1 minute.\")\n",
        "print(\"\")\n",
        "print(\"#### Saturation Checks:\")\n",
        "print(\"\")\n",
        "print(\"*   **Configuration:** Define thresholds for resource utilization metrics (e.g., CPU usage, GPU utilization, memory usage, quantum device queue length if applicable).\")\n",
        "print(\"*   **Monitoring:** Continuously monitor resource utilization.\")\n",
        "print(\"*   **Alerting:** Trigger alerts if resource utilization exceeds warning or critical thresholds. This indicates that the system is becoming saturated and may soon experience performance degradation or failures.\")\n",
        "print(\"*   **Example Check:** Alert if GPU utilization is consistently above 90% for more than 15 minutes.\")\n",
        "print(\"\")\n",
        "print(\"#### Combining Checks and Context:\")\n",
        "print(\"\")\n",
        "print(\"Automated checks are most effective when combined with context from logs and events. When an alert is triggered, the monitoring system should provide links to relevant logs and events to help quickly diagnose the root cause. For instance, a latency alert might be correlated with logs showing increased time spent in the quantum layer or events indicating recent infrastructure changes.\")\n",
        "print(\"\")\n",
        "print(\"By implementing these automated checks, we can move from reactive problem-solving to proactive issue detection, ensuring the Hybrid VAE remains a reliable and performant tool for end-user research.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### ✅ Automated Checks based on SRE Golden Signals\n",
            "\n",
            "Implementing automated checks based on the SRE Golden Signals (Latency, Traffic, Errors, Saturation) is crucial for proactively identifying and responding to issues with the Hybrid VAE in a production-like environment. These checks act as an early warning system, ensuring that the model continues to meet performance and reliability standards, which directly impacts the quality of end-user research outcomes.\n",
            "\n",
            "Here's how automated checks can be applied to the Hybrid VAE:\n",
            "\n",
            "#### Latency Checks:\n",
            "\n",
            "*   **Configuration:** Set thresholds for acceptable inference latency (e.g., average latency, p95 latency). These thresholds should be based on the requirements of the research workflow.\n",
            "*   **Monitoring:** Continuously monitor the actual inference latency using the collected metrics.\n",
            "*   **Alerting:** Trigger alerts (e.g., email, Slack notification, PagerDuty) if the monitored latency exceeds the defined thresholds for a sustained period. This indicates potential performance degradation.\n",
            "*   **Example Check:** Alert if the average inference latency over the last 5 minutes is greater than 1 second.\n",
            "\n",
            "#### Traffic Checks:\n",
            "\n",
            "*   **Configuration:** Define expected traffic patterns (e.g., normal request volume per minute).\n",
            "*   **Monitoring:** Track the incoming inference request rate.\n",
            "*   **Alerting:** Alert on significant deviations from expected traffic patterns. This could include sudden spikes (indicating unexpected load or potential abuse) or drops (indicating issues upstream preventing requests from reaching the VAE).\n",
            "*   **Example Check:** Alert if the inference request rate drops by more than 50% in the last 10 minutes.\n",
            "\n",
            "#### Error Checks:\n",
            "\n",
            "*   **Configuration:** Set a low tolerance for error rates (e.g., percentage of failed inference requests).\n",
            "*   **Monitoring:** Calculate the rate of errors occurring during inference (e.g., exceptions, invalid outputs).\n",
            "*   **Alerting:** Immediately trigger high-priority alerts if the error rate exceeds the defined threshold. This indicates a critical issue impacting the VAE's functionality.\n",
            "*   **Example Check:** Alert if the error rate for inference requests is greater than 1% over the last 1 minute.\n",
            "\n",
            "#### Saturation Checks:\n",
            "\n",
            "*   **Configuration:** Define thresholds for resource utilization metrics (e.g., CPU usage, GPU utilization, memory usage, quantum device queue length if applicable).\n",
            "*   **Monitoring:** Continuously monitor resource utilization.\n",
            "*   **Alerting:** Trigger alerts if resource utilization exceeds warning or critical thresholds. This indicates that the system is becoming saturated and may soon experience performance degradation or failures.\n",
            "*   **Example Check:** Alert if GPU utilization is consistently above 90% for more than 15 minutes.\n",
            "\n",
            "#### Combining Checks and Context:\n",
            "\n",
            "Automated checks are most effective when combined with context from logs and events. When an alert is triggered, the monitoring system should provide links to relevant logs and events to help quickly diagnose the root cause. For instance, a latency alert might be correlated with logs showing increased time spent in the quantum layer or events indicating recent infrastructure changes.\n",
            "\n",
            "By implementing these automated checks, we can move from reactive problem-solving to proactive issue detection, ensuring the Hybrid VAE remains a reliable and performant tool for end-user research.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30c01c39",
        "outputId": "6c8bf364-8a43-4e74-a8db-83d86b001dd4"
      },
      "source": [
        "# This is a placeholder code block to output markdown content, as direct markdown cell creation is not supported.\n",
        "# The actual markdown content will be printed as output.\n",
        "\n",
        "print(\"### 📈 Visualizing Monitoring Data with Streamlit (Illustrative)\")\n",
        "print(\"\")\n",
        "print(\"Visualizing the collected metrics, logs, and alerts is essential for easily understanding the health and performance of the Hybrid VAE and the impact on research outcomes. While full-fledged monitoring dashboards like Grafana are common in production, tools like Streamlit can be used to create interactive dashboards for visualizing monitoring data in a more research-focused or development environment.\")\n",
        "print(\"\")\n",
        "print(\"Streamlit allows quickly building web applications using pure Python. Here's a conceptual outline of how Streamlit could be used to visualize monitoring data for the Hybrid VAE:\")\n",
        "print(\"\")\n",
        "print(\"#### 1. Data Source:\")\n",
        "print(\"\")\n",
        "print(\"The Streamlit application would need access to the collected monitoring data. This could be:\")\n",
        "print(\"*   **MLflow Tracking Server:** Retrieve logged metrics, parameters, and artifacts directly from MLflow.\")\n",
        "print(\"*   **Centralized Logging/Metrics System:** Connect to a database or service where detailed metrics and logs are stored (e.g., a simple file for this notebook's illustration, or a time-series database like Prometheus in a more robust setup).\")\n",
        "print(\"\")\n",
        "print(\"#### 2. Dashboard Components:\")\n",
        "print(\"\")\n",
        "print(\"A Streamlit dashboard could include various components to visualize different aspects of the monitoring data:\")\n",
        "print(\"\")\n",
        "print(\"*   **Key Performance Indicators (KPIs):** Display the current status of key metrics like average inference latency, error rate, and recent training loss.\")\n",
        "print(\"*   **Metric Plots:**\")\n",
        "print(\"    *   Line plots showing the trend of training loss, reconstruction error, and KL divergence over epochs.\")\n",
        "print(\"    *   Time-series plots of inference latency and traffic over time.\")\n",
        "print(\"    *   Histograms or distribution plots for quantum expectation values to understand their range and variability.\")\n",
        "print(\"    *   Resource utilization plots (CPU, GPU, memory).\")\n",
        "print(\"*   **Alerts Summary:** Display a list of recent alerts triggered based on the SRE Golden Signals, including the alert type, time, and triggering value.\")\n",
        "print(\"*   **Log Viewer:** Integrate a simple log viewer to display recent log entries, potentially with filtering capabilities.\")\n",
        "print(\"*   **Experiment Comparison:** Allow users to select different MLflow runs and compare their metrics (e.g., loss curves, benchmark times) side-by-side.\")\n",
        "print(\"\")\n",
        "print(\"#### 3. Interactivity:\")\n",
        "print(\"\")\n",
        "print(\"Streamlit's interactive widgets can enhance the dashboard:\")\n",
        "print(\"\")\n",
        "print(\"*   **Time Range Selector:** Allow users to select a specific time range for viewing metrics and logs.\")\n",
        "print(\"*   **Model Version Selector:** If multiple models are tracked in MLflow, allow selecting a specific model version.\")\n",
        "print(\"*   **Filtering:** Provide options to filter logs by severity level or keywords.\")\n",
        "print(\"\")\n",
        "print(\"#### Conceptual Streamlit Code Snippet:\")\n",
        "print(\"\")\n",
        "print(\"```python\")\n",
        "print(\"# Example (conceptual) Streamlit app structure\")\n",
        "print(\"\")\n",
        "print(\"import streamlit as st\")\n",
        "print(\"import pandas as pd\")\n",
        "print(\"import matplotlib.pyplot as plt\")\n",
        "print(\"# import mlflow # Uncomment if fetching data from MLflow\")\n",
        "print(\"\")\n",
        "print(\"st.title('Hybrid VAE Monitoring Dashboard')\")\n",
        "print(\"\")\n",
        "print(\"# --- Load Data (Illustrative - replace with actual data loading) ---\")\n",
        "print(\"# For this example, we'll use the 'metrics' dictionary from the notebook\")\n",
        "print(\"# import __main__ # Access variables from the main notebook script (Not possible in a real Streamlit app)\")\n",
        "print(\"# monitoring_data = __main__.metrics\") # Access the 'metrics' dictionary\")\n",
        "\n",
        "print(\"# In a real Streamlit app, load data from your source (MLflow, database, etc.)\")\n",
        "print(\"# Example: Load data from a simulated source or MLflow run\")\n",
        "print(\"monitoring_data = {\")\n",
        "print(\"    0: {'train_loss': 1560.5},\")\n",
        "print(\"    1: {'train_loss': 1553.1},\")\n",
        "print(\"    9: {'train_loss': 1507.0},\")\n",
        "print(\"    'inference_duration': 0.0108,\")\n",
        "print(\"    'inference_reconstruction_error': 15.446\")\n",
        "print(\"}\")\n",
        "\n",
        "\n",
        "print(\"# Convert training loss to a DataFrame\")\n",
        "print(\"train_loss_data = [{'epoch': e, 'loss': d['train_loss']} for e, d in monitoring_data.items() if isinstance(e, int)]\")\n",
        "print(\"train_loss_df = pd.DataFrame(train_loss_data)\")\n",
        "\n",
        "print(\"# Get inference metrics\")\n",
        "print(\"inference_duration = monitoring_data.get('inference_duration', 0)\")\n",
        "print(\"reconstruction_error = monitoring_data.get('inference_reconstruction_error', 0)\")\n",
        "\n",
        "\n",
        "print(\"# --- Display KPIs ---\")\n",
        "print(\"st.header('Key Performance Indicators')\")\n",
        "print(\"st.write(f'**Last Recorded Inference Duration:** {inference_duration:.4f} seconds')\") # Use st.write\n",
        "print(\"st.write(f'**Last Recorded Reconstruction Error (MSE):** {reconstruction_error:.4f}')\") # Use st.write\n",
        "print(\"st.write(f\\\"**Last Training Epoch Loss:** {train_loss_df['loss'].iloc[-1]:.4f}\\\" if not train_loss_df.empty else \\\"**Last Training Epoch Loss:** N/A\\\")\") # Use st.write\n",
        "\n",
        "\n",
        "print(\"\")\n",
        "print(\"#### Training Loss Over Epochs\")\n",
        "print(\"if not train_loss_df.empty:\")\n",
        "print(\"    fig, ax = plt.subplots()\")\n",
        "print(\"    ax.plot(train_loss_df['epoch'], train_loss_df['loss'])\")\n",
        "print(\"    ax.set_xlabel('Epoch')\")\n",
        "print(\"    ax.set_ylabel('Training Loss')\")\n",
        "print(\"    st.pyplot(fig)\")\n",
        "print(\"else:\")\n",
        "print(\"    st.write('No training loss data available.')\")\n",
        "\n",
        "print(\"# --- Conceptual Plot for Quantum Expectation Values (requires collecting this data) ---\")\n",
        "print(\"# st.header('Quantum Layer Metrics')\")\n",
        "print(\"# if 'quantum_expval_distribution' in monitoring_data:\")\n",
        "print(\"#     st.subheader('Quantum Expectation Value Distribution')\")\n",
        "print(\"#     fig, ax = plt.subplots()\")\n",
        "print(\"#     ax.hist(monitoring_data['quantum_expval_distribution'], bins=20)\")\n",
        "print(\"#     ax.set_xlabel('Expectation Value')\")\n",
        "print(\"#     ax.set_ylabel('Frequency')\")\n",
        "print(\"#     st.pyplot(fig)\")\n",
        "print(\"# else:\")\n",
        "print(\"#     st.write('Quantum expectation value distribution data not available.')\")\n",
        "\n",
        "\n",
        "print(\"# --- Conceptual Log Viewer (requires collecting logs to a file or database) ---\")\n",
        "print(\"# st.header('Recent Logs')\")\n",
        "print(\"# try:\")\n",
        "print(\"#     with open('app.log', 'r') as f:\")\n",
        "print(\"#         logs = f.readlines()\")\n",
        "print(\"#     for log_entry in logs[-20:]: # Display last 20 logs\")\n",
        "print(\"#         st.text(log_entry.strip())\")\n",
        "print(\"# except FileNotFoundError:\")\n",
        "print(\"#     st.write('Log file not found.')\")\n",
        "\n",
        "print(\"```\")\n",
        "print(\"\")\n",
        "print(\"To run a Streamlit app, you would typically save the code to a `.py` file (e.g., `monitor_app.py`) and run it from your terminal using `streamlit run monitor_app.py`. In this notebook context, this is illustrative.\")\n",
        "print(\"\")\n",
        "print(\"Visualizing monitoring data makes it significantly easier to track the performance and health of the Hybrid VAE, identify issues quickly, and gain insights that can inform both model development and scientific conclusions.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### 📈 Visualizing Monitoring Data with Streamlit (Illustrative)\n",
            "\n",
            "Visualizing the collected metrics, logs, and alerts is essential for easily understanding the health and performance of the Hybrid VAE and the impact on research outcomes. While full-fledged monitoring dashboards like Grafana are common in production, tools like Streamlit can be used to create interactive dashboards for visualizing monitoring data in a more research-focused or development environment.\n",
            "\n",
            "Streamlit allows quickly building web applications using pure Python. Here's a conceptual outline of how Streamlit could be used to visualize monitoring data for the Hybrid VAE:\n",
            "\n",
            "#### 1. Data Source:\n",
            "\n",
            "The Streamlit application would need access to the collected monitoring data. This could be:\n",
            "*   **MLflow Tracking Server:** Retrieve logged metrics, parameters, and artifacts directly from MLflow.\n",
            "*   **Centralized Logging/Metrics System:** Connect to a database or service where detailed metrics and logs are stored (e.g., a simple file for this notebook's illustration, or a time-series database like Prometheus in a more robust setup).\n",
            "\n",
            "#### 2. Dashboard Components:\n",
            "\n",
            "A Streamlit dashboard could include various components to visualize different aspects of the monitoring data:\n",
            "\n",
            "*   **Key Performance Indicators (KPIs):** Display the current status of key metrics like average inference latency, error rate, and recent training loss.\n",
            "*   **Metric Plots:**\n",
            "    *   Line plots showing the trend of training loss, reconstruction error, and KL divergence over epochs.\n",
            "    *   Time-series plots of inference latency and traffic over time.\n",
            "    *   Histograms or distribution plots for quantum expectation values to understand their range and variability.\n",
            "    *   Resource utilization plots (CPU, GPU, memory).\n",
            "*   **Alerts Summary:** Display a list of recent alerts triggered based on the SRE Golden Signals, including the alert type, time, and triggering value.\n",
            "*   **Log Viewer:** Integrate a simple log viewer to display recent log entries, potentially with filtering capabilities.\n",
            "*   **Experiment Comparison:** Allow users to select different MLflow runs and compare their metrics (e.g., loss curves, benchmark times) side-by-side.\n",
            "\n",
            "#### 3. Interactivity:\n",
            "\n",
            "Streamlit's interactive widgets can enhance the dashboard:\n",
            "\n",
            "*   **Time Range Selector:** Allow users to select a specific time range for viewing metrics and logs.\n",
            "*   **Model Version Selector:** If multiple models are tracked in MLflow, allow selecting a specific model version.\n",
            "*   **Filtering:** Provide options to filter logs by severity level or keywords.\n",
            "\n",
            "#### Conceptual Streamlit Code Snippet:\n",
            "\n",
            "```python\n",
            "# Example (conceptual) Streamlit app structure\n",
            "\n",
            "import streamlit as st\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt\n",
            "# import mlflow # Uncomment if fetching data from MLflow\n",
            "\n",
            "st.title('Hybrid VAE Monitoring Dashboard')\n",
            "\n",
            "# --- Load Data (Illustrative - replace with actual data loading) ---\n",
            "# For this example, we'll use the 'metrics' dictionary from the notebook\n",
            "# import __main__ # Access variables from the main notebook script (Not possible in a real Streamlit app)\n",
            "# monitoring_data = __main__.metrics\n",
            "# In a real Streamlit app, load data from your source (MLflow, database, etc.)\n",
            "# Example: Load data from a simulated source or MLflow run\n",
            "monitoring_data = {\n",
            "    0: {'train_loss': 1560.5},\n",
            "    1: {'train_loss': 1553.1},\n",
            "    9: {'train_loss': 1507.0},\n",
            "    'inference_duration': 0.0108,\n",
            "    'inference_reconstruction_error': 15.446\n",
            "}\n",
            "# Convert training loss to a DataFrame\n",
            "train_loss_data = [{'epoch': e, 'loss': d['train_loss']} for e, d in monitoring_data.items() if isinstance(e, int)]\n",
            "train_loss_df = pd.DataFrame(train_loss_data)\n",
            "# Get inference metrics\n",
            "inference_duration = monitoring_data.get('inference_duration', 0)\n",
            "reconstruction_error = monitoring_data.get('inference_reconstruction_error', 0)\n",
            "# --- Display KPIs ---\n",
            "st.header('Key Performance Indicators')\n",
            "st.write(f'**Last Recorded Inference Duration:** {inference_duration:.4f} seconds')\n",
            "st.write(f'**Last Recorded Reconstruction Error (MSE):** {reconstruction_error:.4f}')\n",
            "st.write(f\"**Last Training Epoch Loss:** {train_loss_df['loss'].iloc[-1]:.4f}\" if not train_loss_df.empty else \"**Last Training Epoch Loss:** N/A\")\n",
            "\n",
            "#### Training Loss Over Epochs\n",
            "if not train_loss_df.empty:\n",
            "    fig, ax = plt.subplots()\n",
            "    ax.plot(train_loss_df['epoch'], train_loss_df['loss'])\n",
            "    ax.set_xlabel('Epoch')\n",
            "    ax.set_ylabel('Training Loss')\n",
            "    st.pyplot(fig)\n",
            "else:\n",
            "    st.write('No training loss data available.')\n",
            "# --- Conceptual Plot for Quantum Expectation Values (requires collecting this data) ---\n",
            "# st.header('Quantum Layer Metrics')\n",
            "# if 'quantum_expval_distribution' in monitoring_data:\n",
            "#     st.subheader('Quantum Expectation Value Distribution')\n",
            "#     fig, ax = plt.subplots()\n",
            "#     ax.hist(monitoring_data['quantum_expval_distribution'], bins=20)\n",
            "#     ax.set_xlabel('Expectation Value')\n",
            "#     ax.set_ylabel('Frequency')\n",
            "#     st.pyplot(fig)\n",
            "# else:\n",
            "#     st.write('Quantum expectation value distribution data not available.')\n",
            "# --- Conceptual Log Viewer (requires collecting logs to a file or database) ---\n",
            "# st.header('Recent Logs')\n",
            "# try:\n",
            "#     with open('app.log', 'r') as f:\n",
            "#         logs = f.readlines()\n",
            "#     for log_entry in logs[-20:]: # Display last 20 logs\n",
            "#         st.text(log_entry.strip())\n",
            "# except FileNotFoundError:\n",
            "#     st.write('Log file not found.')\n",
            "```\n",
            "\n",
            "To run a Streamlit app, you would typically save the code to a `.py` file (e.g., `monitor_app.py`) and run it from your terminal using `streamlit run monitor_app.py`. In this notebook context, this is illustrative.\n",
            "\n",
            "Visualizing monitoring data makes it significantly easier to track the performance and health of the Hybrid VAE, identify issues quickly, and gain insights that can inform both model development and scientific conclusions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d7b8106"
      },
      "source": [
        "## Choose and integrate a metrics library\n",
        "\n",
        "Select a suitable Python library for metrics collection (e.g., `prometheus_client`, `opentelemetry-api`) and integrate it into the VAE code.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "9374e5f5",
        "outputId": "661bf26e-cb51-48a7-f258-0cac3d29a859"
      },
      "source": [
        "import prometheus_client\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pennylane as qml\n",
        "import mlflow\n",
        "\n",
        "# Define Prometheus metrics (Illustrative)\n",
        "# Resetting registry for notebook re-runs\n",
        "prometheus_client.REGISTRY.clear()\n",
        "\n",
        "# Classical Encoder Metrics\n",
        "ENCODER_PROCESSING_TIME = prometheus_client.Summary('encoder_processing_time_seconds', 'Time taken for classical encoder processing')\n",
        "LATENT_MU = prometheus_client.Gauge('latent_mu', 'Mean of the latent space mu values', ['latent_dim_type'])\n",
        "LATENT_LOG_VAR = prometheus_client.Gauge('latent_log_var', 'Mean of the latent space log_var values', ['latent_dim_type'])\n",
        "\n",
        "\n",
        "# Quantum Layer Metrics\n",
        "QUANTUM_CIRCUIT_DURATION = prometheus_client.Summary('quantum_circuit_duration_seconds', 'Duration of quantum circuit execution')\n",
        "QUANTUM_EXPVAL_MEAN = prometheus_client.Gauge('quantum_expval_mean', 'Mean of quantum expectation values')\n",
        "QUANTUM_EXPVAL_STD = prometheus_client.Gauge('quantum_expval_std', 'Standard deviation of quantum expectation values')\n",
        "QUANTUM_CIRCUIT_EVALUATIONS_TOTAL = prometheus_client.Counter('quantum_circuit_evaluations_total', 'Total number of quantum circuit evaluations')\n",
        "\n",
        "\n",
        "# Classical Decoder Metrics\n",
        "DECODER_PROCESSING_TIME = prometheus_client.Summary('decoder_processing_time_seconds', 'Time taken for classical decoder processing')\n",
        "\n",
        "# Overall VAE Metrics\n",
        "TOTAL_INFERENCE_TIME = prometheus_client.Summary('total_inference_time_seconds', 'Total time taken for inference requests')\n",
        "TRAINING_TIME_PER_EPOCH = prometheus_client.Summary('training_time_per_epoch_seconds', 'Total time taken for one training epoch')\n",
        "RECONSTRUCTION_ERROR_MSE = prometheus_client.Gauge('reconstruction_error_mse', 'Mean Squared Error of reconstruction')\n",
        "KL_DIVERGENCE_LOSS = prometheus_client.Gauge('kl_divergence_loss', 'Kullback-Leibler Divergence loss')\n",
        "TOTAL_VAE_LOSS = prometheus_client.Gauge('total_vae_loss', 'Total VAE loss (Reconstruction + KL)')\n",
        "\n",
        "\n",
        "# Define the Hybrid VAE model again to ensure it's accessible\n",
        "class HybridVAE(nn.Module):\n",
        "    def __init__(self, input_dim=100, latent_dim_classical=32, latent_dim_quantum=4):\n",
        "        super().__init__()\n",
        "        self.latent_dim_classical = latent_dim_classical\n",
        "        self.latent_dim_quantum = latent_dim_quantum\n",
        "\n",
        "        # Classical Encoder\n",
        "        self.encoder_classical = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 2 * (latent_dim_classical + latent_dim_quantum)) # Output 2x for mu and log_var\n",
        "        )\n",
        "\n",
        "        # Quantum Layer\n",
        "        self.dev = qml.device(\"default.qubit\", wires=latent_dim_quantum)\n",
        "\n",
        "        @qml.qnode(self.dev, interface=\"torch\")\n",
        "        def quantum_circuit(inputs):\n",
        "            # Simple variational quantum circuit\n",
        "            # inputs should have shape (batch_size, latent_dim_quantum)\n",
        "            for i in range(self.latent_dim_quantum):\n",
        "                # Apply RY gate to each qubit with the corresponding input from the latent space\n",
        "                qml.RY(inputs[:, i], wires=i)\n",
        "            for i in range(self.latent_dim_quantum - 1):\n",
        "                qml.CZ(wires=[i, i+1])\n",
        "            # Return a list of expectation values. PennyLane with torch interface\n",
        "            # should handle batching and convert this list of measurement processes to a tensor\n",
        "            # of shape (batch_size, latent_dim_quantum).\n",
        "            return [qml.expval(qml.PauliZ(i)) for i in range(self.latent_dim_quantum)]\n",
        "\n",
        "        self.quantum_layer = quantum_circuit\n",
        "\n",
        "        # Classical Decoder\n",
        "        self.decoder_classical = nn.Sequential(\n",
        "            nn.Linear(latent_dim_classical + latent_dim_quantum, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, input_dim)\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Classical Encoder forward pass\n",
        "        start_encode_time = time.time()\n",
        "        encoded = self.encoder_classical(x)\n",
        "        end_encode_time = time.time()\n",
        "        ENCODER_PROCESSING_TIME.observe(end_encode_time - start_encode_time)\n",
        "\n",
        "        # Reshape to separate mu and log_var for the combined latent space\n",
        "        mu_log_var = encoded.view(-1, 2, self.latent_dim_classical + self.latent_dim_quantum)\n",
        "        mu = mu_log_var[:, 0, :]\n",
        "        log_var = mu_log_var[:, 1, :]\n",
        "\n",
        "        # Log latent space parameter means\n",
        "        LATENT_MU.labels(latent_dim_type='classical').set(mu[:, :self.latent_dim_classical].mean().item())\n",
        "        LATENT_LOG_VAR.labels(latent_dim_type='classical').set(log_var[:, :self.latent_dim_classical].mean().item())\n",
        "        LATENT_MU.labels(latent_dim_type='quantum').set(mu[:, self.latent_dim_classical:].mean().item())\n",
        "        LATENT_LOG_VAR.labels(latent_dim_type='quantum').set(log_var[:, self.latent_dim_classical:].mean().item())\n",
        "\n",
        "        # Split latent space for classical and quantum parts\n",
        "        mu_classical = mu[:, :self.latent_dim_classical]\n",
        "        log_var_classical = log_var[:, :self.latent_dim_classical]\n",
        "        mu_quantum = mu[:, self.latent_dim_classical:]\n",
        "        log_var_quantum = log_var[:, self.latent_dim_classical:]\n",
        "\n",
        "        # Reparameterization trick for classical latent space\n",
        "        z_classical = self.reparameterize(mu_classical, log_var_classical)\n",
        "\n",
        "        # Pass quantum latent space through quantum layer\n",
        "        # Apply reparameterization trick for quantum latent space inputs to the quantum circuit\n",
        "        z_quantum_input = self.reparameterize(mu_quantum, log_var_quantum)\n",
        "\n",
        "        start_quantum_time = time.time()\n",
        "        # Pass the entire batch of quantum latent inputs to the quantum layer\n",
        "        z_quantum_output = self.quantum_layer(z_quantum_input)\n",
        "        end_quantum_time = time.time()\n",
        "        QUANTUM_CIRCUIT_DURATION.observe(end_quantum_time - start_quantum_time)\n",
        "        QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.inc(z_quantum_input.size(0)) # Increment by batch size\n",
        "\n",
        "        # If the output is a list of tensors, stack it. If it's already a single tensor, keep it.\n",
        "        if isinstance(z_quantum_output, list):\n",
        "             z_quantum_output = torch.stack(z_quantum_output, dim=1)\n",
        "\n",
        "        # Ensure data types match before concatenation\n",
        "        z_quantum_output = z_quantum_output.float()\n",
        "\n",
        "        # Log quantum expectation value statistics\n",
        "        QUANTUM_EXPVAL_MEAN.set(z_quantum_output.mean().item())\n",
        "        QUANTUM_EXPVAL_STD.set(z_quantum_output.std().item())\n",
        "\n",
        "\n",
        "        # Concatenate classical and quantum latent representations\n",
        "        z = torch.cat((z_classical, z_quantum_output), dim=1)\n",
        "\n",
        "        # Classical Decoder forward pass\n",
        "        start_decode_time = time.time()\n",
        "        reconstruction = self.decoder_classical(z)\n",
        "        end_decode_time = time.time()\n",
        "        DECODER_PROCESSING_TIME.observe(end_decode_time - start_decode_time)\n",
        "\n",
        "\n",
        "        return reconstruction, mu, log_var\n",
        "\n",
        "\n",
        "# Define the loss function (VAE loss: Reconstruction + KL Divergence)\n",
        "def vae_loss(recon_x, x, mu, log_var):\n",
        "    BCE = F.mse_loss(recon_x, x, reduction='sum') # Using MSE for reconstruction loss\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "\n",
        "    # Log loss components\n",
        "    RECONSTRUCTION_ERROR_MSE.set(BCE.item() / x.size(0)) # Log mean MSE\n",
        "    KL_DIVERGENCE_LOSS.set(KLD.item())\n",
        "    TOTAL_VAE_LOSS.set((BCE + KLD).item())\n",
        "\n",
        "    return BCE + KLD\n",
        "\n",
        "# Example usage in a training loop (adapted from previous cells)\n",
        "# Assuming X_train_tensor and X_test_tensor are already defined\n",
        "\n",
        "# Start an MLflow run\n",
        "with mlflow.start_run(run_name=\"Hybrid VAE Training with Metrics\"):\n",
        "    # Log hyperparameters\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 10\n",
        "    classical_latent_dim = 32\n",
        "    quantum_latent_dim = 4\n",
        "\n",
        "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
        "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
        "    mlflow.log_param(\"classical_latent_dim\", classical_latent_dim)\n",
        "    mlflow.log_param(\"quantum_latent_dim\", quantum_latent_dim)\n",
        "\n",
        "    model = HybridVAE(input_dim=X_train_tensor.shape[1],\n",
        "                      latent_dim_classical=classical_latent_dim,\n",
        "                      latent_dim_quantum=quantum_latent_dim)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    print(\"Starting Hybrid VAE training with Prometheus metrics...\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_epoch_time = time.time()\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Process the entire training tensor as a single batch\n",
        "        recon, mu, log_var = model(X_train_tensor)\n",
        "        loss = vae_loss(recon, X_train_tensor, mu, log_var)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        end_epoch_time = time.time()\n",
        "        epoch_duration = end_epoch_time - start_epoch_time\n",
        "        TRAINING_TIME_PER_EPOCH.observe(epoch_duration)\n",
        "\n",
        "\n",
        "        avg_train_loss = train_loss / X_train_tensor.size(0)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Log training loss for the epoch to MLflow\n",
        "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "        # Log Prometheus metrics related to the epoch (loss, etc.)\n",
        "        mlflow.log_metric(\"total_vae_loss_epoch\", TOTAL_VAE_LOSS.collect()[0].samples[0].value, step=epoch)\n",
        "        mlflow.log_metric(\"reconstruction_error_mse_epoch\", RECONSTRUCTION_ERROR_MSE.collect()[0].samples[0].value, step=epoch)\n",
        "        mlflow.log_metric(\"kl_divergence_loss_epoch\", KL_DIVERGENCE_LOSS.collect()[0].samples[0].value, step=epoch)\n",
        "        mlflow.log_metric(\"training_time_per_epoch_s\", epoch_duration, step=epoch)\n",
        "        mlflow.log_metric(\"quantum_circuit_evaluations_total\", QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.collect()[0].samples[0].value, step=epoch)\n",
        "\n",
        "\n",
        "    print(\"Hybrid VAE training finished.\")\n",
        "\n",
        "    # Log the trained model\n",
        "    mlflow.pytorch.log_model(model, \"hybrid_vae_model\")\n",
        "\n",
        "# The MLflow run automatically ends when exiting the 'with' block\n",
        "\n",
        "# Example usage in an inference scenario\n",
        "print(\"\\nStarting Hybrid VAE inference with Prometheus metrics...\")\n",
        "\n",
        "model.eval() # Set the model to evaluation mode\n",
        "with torch.no_grad(): # Disable gradient calculation for inference\n",
        "    start_total_inference_time = time.time()\n",
        "\n",
        "    # Encode\n",
        "    start_encode_time = time.time()\n",
        "    encoded_test_data = model.encoder_classical(X_test_tensor)\n",
        "    end_encode_time = time.time()\n",
        "    ENCODER_PROCESSING_TIME.observe(end_encode_time - start_encode_time)\n",
        "\n",
        "    mu_log_var_test = encoded_test_data.view(-1, 2, model.latent_dim_classical + model.latent_dim_quantum)\n",
        "    mu_test = mu_log_var_test[:, 0, :]\n",
        "    log_var_test = mu_log_var_test[:, 1, :]\n",
        "\n",
        "    # Log latent space parameter means for inference\n",
        "    LATENT_MU.labels(latent_dim_type='classical').set(mu_test[:, :model.latent_dim_classical].mean().item())\n",
        "    LATENT_LOG_VAR.labels(latent_dim_type='classical').set(log_var_test[:, :model.latent_dim_classical].mean().item())\n",
        "    LATENT_MU.labels(latent_dim_type='quantum').set(mu_test[:, model.latent_dim_classical:].mean().item())\n",
        "    LATENT_LOG_VAR.labels(latent_dim_type='quantum').set(log_var_test[:, model.latent_dim_classical:].mean().item())\n",
        "\n",
        "\n",
        "    mu_classical_test = mu_test[:, :model.latent_dim_classical]\n",
        "    log_var_classical_test = log_var_test[:, :model.latent_dim_classical]\n",
        "    mu_quantum_test = mu_test[:, model.latent_dim_classical:]\n",
        "    log_var_quantum_test = log_var_test[:, model.latent_dim_classical:]\n",
        "\n",
        "    z_classical_test = model.reparameterize(mu_classical_test, log_var_classical_test)\n",
        "    z_quantum_input_test = model.reparameterize(mu_quantum_test, log_var_quantum_test)\n",
        "\n",
        "    # Quantum Layer\n",
        "    start_quantum_time = time.time()\n",
        "    z_quantum_output_test = model.quantum_layer(z_quantum_input_test)\n",
        "    end_quantum_time = time.time()\n",
        "    QUANTUM_CIRCUIT_DURATION.observe(end_quantum_time - start_quantum_time)\n",
        "    QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.inc(z_quantum_input_test.size(0)) # Increment by batch size\n",
        "\n",
        "    if isinstance(z_quantum_output_test, list):\n",
        "         z_quantum_output_test = torch.stack(z_quantum_output_test, dim=1)\n",
        "\n",
        "    z_quantum_output_test = z_quantum_output_test.float()\n",
        "\n",
        "    # Log quantum expectation value statistics for inference\n",
        "    QUANTUM_EXPVAL_MEAN.set(z_quantum_output_test.mean().item())\n",
        "    QUANTUM_EXPVAL_STD.set(z_quantum_output_test.std().item())\n",
        "\n",
        "\n",
        "    z_test = torch.cat((z_classical_test, z_quantum_output_test), dim=1)\n",
        "\n",
        "    # Decode\n",
        "    start_decode_time = time.time()\n",
        "    reconstructed_test_data = model.decoder_classical(z_test)\n",
        "    end_decode_time = time.time()\n",
        "    DECODER_PROCESSING_TIME.observe(end_decode_time - start_decode_time)\n",
        "\n",
        "    end_total_inference_time = time.time()\n",
        "    TOTAL_INFERENCE_TIME.observe(end_total_inference_time - start_total_inference_time)\n",
        "\n",
        "\n",
        "    # Calculate and log inference reconstruction error\n",
        "    inference_reconstruction_error = F.mse_loss(reconstructed_test_data, X_test_tensor, reduction='mean').item()\n",
        "    RECONSTRUCTION_ERROR_MSE.set(inference_reconstruction_error) # Set for inference\n",
        "\n",
        "\n",
        "print(\"\\nPrometheus metrics integrated into training and inference.\")\n",
        "print(\"Metrics can be accessed via `prometheus_client.REGISTRY.collect()` (e.g., for exposition).\")\n",
        "\n",
        "# Illustrative: Print some collected metrics\n",
        "print(\"\\nIllustrative collected metrics:\")\n",
        "for metric in prometheus_client.REGISTRY.collect():\n",
        "    print(f\"Metric: {metric.name}\")\n",
        "    for sample in metric.samples:\n",
        "        print(f\"  Sample: {sample.name}, Labels: {sample.labels}, Value: {sample.value}\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pennylane'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-787999106.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpennylane\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mqml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pennylane'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "tgpgO9s1UpEH",
        "outputId": "0f6d0811-65c5-45b3-a4fb-c3ccb8dc40df"
      },
      "source": [
        "import prometheus_client\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pennylane as qml\n",
        "import mlflow\n",
        "\n",
        "\n",
        "'''\n",
        "Fixing error: The error `AttributeError: 'CollectorRegistry' object has no attribute 'clear'` indicates that\n",
        "`prometheus_client.REGISTRY` does not have a `clear` method. To reset the default registry in Prometheus client\n",
        " versions that don't support `clear`, a new default registry can be created and set. The code needs to be\n",
        " regenerated with this fix to allow for rerunning the cell in a notebook environment.\n",
        "'''\n",
        "\n",
        "# Define Prometheus metrics (Illustrative)\n",
        "# Resetting default registry for notebook re-runs\n",
        "# In newer versions, REGISTRY.clear() might work, but this approach is more compatible\n",
        "prometheus_client.core._REGISTRY = prometheus_client.CollectorRegistry()\n",
        "prometheus_client.set_default_registry(prometheus_client.core._REGISTRY)\n",
        "\n",
        "\n",
        "# Classical Encoder Metrics\n",
        "ENCODER_PROCESSING_TIME = prometheus_client.Summary('encoder_processing_time_seconds', 'Time taken for classical encoder processing')\n",
        "LATENT_MU = prometheus_client.Gauge('latent_mu', 'Mean of the latent space mu values', ['latent_dim_type'])\n",
        "LATENT_LOG_VAR = prometheus_client.Gauge('latent_log_var', 'Mean of the latent space log_var values', ['latent_dim_type'])\n",
        "\n",
        "\n",
        "# Quantum Layer Metrics\n",
        "QUANTUM_CIRCUIT_DURATION = prometheus_client.Summary('quantum_circuit_duration_seconds', 'Duration of quantum circuit execution')\n",
        "QUANTUM_EXPVAL_MEAN = prometheus_client.Gauge('quantum_expval_mean', 'Mean of quantum expectation values')\n",
        "QUANTUM_EXPVAL_STD = prometheus_client.Gauge('quantum_expval_std', 'Standard deviation of quantum expectation values')\n",
        "QUANTUM_CIRCUIT_EVALUATIONS_TOTAL = prometheus_client.Counter('quantum_circuit_evaluations_total', 'Total number of quantum circuit evaluations')\n",
        "\n",
        "\n",
        "# Classical Decoder Metrics\n",
        "DECODER_PROCESSING_TIME = prometheus_client.Summary('decoder_processing_time_seconds', 'Time taken for classical decoder processing')\n",
        "\n",
        "# Overall VAE Metrics\n",
        "TOTAL_INFERENCE_TIME = prometheus_client.Summary('total_inference_time_seconds', 'Total time taken for inference requests')\n",
        "TRAINING_TIME_PER_EPOCH = prometheus_client.Summary('training_time_per_epoch_seconds', 'Total time taken for one training epoch')\n",
        "RECONSTRUCTION_ERROR_MSE = prometheus_client.Gauge('reconstruction_error_mse', 'Mean Squared Error of reconstruction')\n",
        "KL_DIVERGENCE_LOSS = prometheus_client.Gauge('kl_divergence_loss', 'Kullback-Leibler Divergence loss')\n",
        "TOTAL_VAE_LOSS = prometheus_client.Gauge('total_vae_loss', 'Total VAE loss (Reconstruction + KL)')\n",
        "\n",
        "\n",
        "# Define the Hybrid VAE model again to ensure it's accessible\n",
        "class HybridVAE(nn.Module):\n",
        "    def __init__(self, input_dim=100, latent_dim_classical=32, latent_dim_quantum=4):\n",
        "        super().__init__()\n",
        "        self.latent_dim_classical = latent_dim_classical\n",
        "        self.latent_dim_quantum = latent_dim_quantum\n",
        "\n",
        "        # Classical Encoder\n",
        "        self.encoder_classical = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 2 * (latent_dim_classical + latent_dim_quantum)) # Output 2x for mu and log_var\n",
        "        )\n",
        "\n",
        "        # Quantum Layer\n",
        "        self.dev = qml.device(\"default.qubit\", wires=latent_dim_quantum)\n",
        "\n",
        "        @qml.qnode(self.dev, interface=\"torch\")\n",
        "        def quantum_circuit(inputs):\n",
        "            # Simple variational quantum circuit\n",
        "            # inputs should have shape (batch_size, latent_dim_quantum)\n",
        "            for i in range(self.latent_dim_quantum):\n",
        "                # Apply RY gate to each qubit with the corresponding input from the latent space\n",
        "                qml.RY(inputs[:, i], wires=i)\n",
        "            for i in range(self.latent_dim_quantum - 1):\n",
        "                qml.CZ(wires=[i, i+1])\n",
        "            # Return a list of expectation values. PennyLane with torch interface\n",
        "            # should handle batching and convert this list of measurement processes to a tensor\n",
        "            # of shape (batch_size, latent_dim_quantum).\n",
        "            return [qml.expval(qml.PauliZ(i)) for i in range(self.latent_dim_quantum)]\n",
        "\n",
        "        self.quantum_layer = quantum_circuit\n",
        "\n",
        "        # Classical Decoder\n",
        "        self.decoder_classical = nn.Sequential(\n",
        "            nn.Linear(latent_dim_classical + latent_dim_quantum, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, input_dim)\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Classical Encoder forward pass\n",
        "        start_encode_time = time.time()\n",
        "        encoded = self.encoder_classical(x)\n",
        "        end_encode_time = time.time()\n",
        "        ENCODER_PROCESSING_TIME.observe(end_encode_time - start_encode_time)\n",
        "\n",
        "        # Reshape to separate mu and log_var for the combined latent space\n",
        "        mu_log_var = encoded.view(-1, 2, self.latent_dim_classical + self.latent_dim_quantum)\n",
        "        mu = mu_log_var[:, 0, :]\n",
        "        log_var = mu_log_var[:, 1, :]\n",
        "\n",
        "        # Log latent space parameter means\n",
        "        LATENT_MU.labels(latent_dim_type='classical').set(mu[:, :self.latent_dim_classical].mean().item())\n",
        "        LATENT_LOG_VAR.labels(latent_dim_type='classical').set(log_var[:, :self.latent_dim_classical].mean().item())\n",
        "        LATENT_MU.labels(latent_dim_type='quantum').set(mu[:, self.latent_dim_classical:].mean().item())\n",
        "        LATENT_LOG_VAR.labels(latent_dim_type='quantum').set(log_var[:, self.latent_dim_classical:].mean().item())\n",
        "\n",
        "        # Split latent space for classical and quantum parts\n",
        "        mu_classical = mu[:, :self.latent_dim_classical]\n",
        "        log_var_classical = log_var[:, :self.latent_dim_classical]\n",
        "        mu_quantum = mu[:, self.latent_dim_classical:]\n",
        "        log_var_quantum = log_var[:, self.latent_dim_classical:]\n",
        "\n",
        "        # Reparameterization trick for classical latent space\n",
        "        z_classical = self.reparameterize(mu_classical, log_var_classical)\n",
        "\n",
        "        # Pass quantum latent space through quantum layer\n",
        "        # Apply reparameterization trick for quantum latent space inputs to the quantum circuit\n",
        "        z_quantum_input = self.reparameterize(mu_quantum, log_var_quantum)\n",
        "\n",
        "        start_quantum_time = time.time()\n",
        "        # Pass the entire batch of quantum latent inputs to the quantum layer\n",
        "        z_quantum_output = self.quantum_layer(z_quantum_input)\n",
        "        end_quantum_time = time.time()\n",
        "        QUANTUM_CIRCUIT_DURATION.observe(end_quantum_time - start_quantum_time)\n",
        "        QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.inc(z_quantum_input.size(0)) # Increment by batch size\n",
        "\n",
        "        # If the output is a list of tensors, stack it. If it's already a single tensor, keep it.\n",
        "        if isinstance(z_quantum_output, list):\n",
        "             z_quantum_output = torch.stack(z_quantum_output, dim=1)\n",
        "\n",
        "        # Ensure data types match before concatenation\n",
        "        z_quantum_output = z_quantum_output.float()\n",
        "\n",
        "        # Log quantum expectation value statistics\n",
        "        QUANTUM_EXPVAL_MEAN.set(z_quantum_output.mean().item())\n",
        "        QUANTUM_EXPVAL_STD.set(z_quantum_output.std().item())\n",
        "\n",
        "\n",
        "        # Concatenate classical and quantum latent representations\n",
        "        z = torch.cat((z_classical, z_quantum_output), dim=1)\n",
        "\n",
        "        # Classical Decoder forward pass\n",
        "        start_decode_time = time.time()\n",
        "        reconstruction = self.decoder_classical(z)\n",
        "        end_decode_time = time.time()\n",
        "        DECODER_PROCESSING_TIME.observe(end_decode_time - start_decode_time)\n",
        "\n",
        "\n",
        "        return reconstruction, mu, log_var\n",
        "\n",
        "\n",
        "# Define the loss function (VAE loss: Reconstruction + KL Divergence)\n",
        "def vae_loss(recon_x, x, mu, log_var):\n",
        "    BCE = F.mse_loss(recon_x, x, reduction='sum') # Using MSE for reconstruction loss\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "\n",
        "    # Log loss components\n",
        "    RECONSTRUCTION_ERROR_MSE.set(BCE.item() / x.size(0)) # Log mean MSE\n",
        "    KL_DIVERGENCE_LOSS.set(KLD.item())\n",
        "    TOTAL_VAE_LOSS.set((BCE + KLD).item())\n",
        "\n",
        "    return BCE + KLD\n",
        "\n",
        "# Example usage in a training loop (adapted from previous cells)\n",
        "# Assuming X_train_tensor and X_test_tensor are already defined\n",
        "\n",
        "# Start an MLflow run\n",
        "with mlflow.start_run(run_name=\"Hybrid VAE Training with Metrics\"):\n",
        "    # Log hyperparameters\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 10\n",
        "    classical_latent_dim = 32\n",
        "    quantum_latent_dim = 4\n",
        "\n",
        "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
        "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
        "    mlflow.log_param(\"classical_latent_dim\", classical_latent_dim)\n",
        "    mlflow.log_param(\"quantum_latent_dim\", quantum_latent_dim)\n",
        "\n",
        "    model = HybridVAE(input_dim=X_train_tensor.shape[1],\n",
        "                      latent_dim_classical=classical_latent_dim,\n",
        "                      latent_dim_quantum=quantum_latent_dim)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    print(\"Starting Hybrid VAE training with Prometheus metrics...\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_epoch_time = time.time()\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Process the entire training tensor as a single batch\n",
        "        recon, mu, log_var = model(X_train_tensor)\n",
        "        loss = vae_loss(recon, X_train_tensor, mu, log_var)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        end_epoch_time = time.time()\n",
        "        epoch_duration = end_epoch_time - start_epoch_time\n",
        "        TRAINING_TIME_PER_EPOCH.observe(epoch_duration)\n",
        "\n",
        "\n",
        "        avg_train_loss = train_loss / X_train_tensor.size(0)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Log training loss for the epoch to MLflow\n",
        "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "        # Log Prometheus metrics related to the epoch (loss, etc.)\n",
        "        mlflow.log_metric(\"total_vae_loss_epoch\", TOTAL_VAE_LOSS.collect()[0].samples[0].value, step=epoch)\n",
        "        mlflow.log_metric(\"reconstruction_error_mse_epoch\", RECONSTRUCTION_ERROR_MSE.collect()[0].samples[0].value, step=epoch)\n",
        "        mlflow.log_metric(\"kl_divergence_loss_epoch\", KL_DIVERGENCE_LOSS.collect()[0].samples[0].value, step=epoch)\n",
        "        mlflow.log_metric(\"training_time_per_epoch_s\", epoch_duration, step=epoch)\n",
        "        mlflow.log_metric(\"quantum_circuit_evaluations_total\", QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.collect()[0].samples[0].value, step=epoch)\n",
        "\n",
        "\n",
        "    print(\"Hybrid VAE training finished.\")\n",
        "\n",
        "    # Log the trained model\n",
        "    mlflow.pytorch.log_model(model, \"hybrid_vae_model\")\n",
        "\n",
        "# The MLflow run automatically ends when exiting the 'with' block\n",
        "\n",
        "# Example usage in an inference scenario\n",
        "print(\"\\nStarting Hybrid VAE inference with Prometheus metrics...\")\n",
        "\n",
        "model.eval() # Set the model to evaluation mode\n",
        "with torch.no_grad(): # Disable gradient calculation for inference\n",
        "    start_total_inference_time = time.time()\n",
        "\n",
        "    # Encode\n",
        "    start_encode_time = time.time()\n",
        "    encoded_test_data = model.encoder_classical(X_test_tensor)\n",
        "    end_encode_time = time.time()\n",
        "    ENCODER_PROCESSING_TIME.observe(end_encode_time - start_encode_time)\n",
        "\n",
        "    mu_log_var_test = encoded_test_data.view(-1, 2, model.latent_dim_classical + model.latent_dim_quantum)\n",
        "    mu_test = mu_log_var_test[:, 0, :]\n",
        "    log_var_test = mu_log_var_test[:, 1, :]\n",
        "\n",
        "    # Log latent space parameter means for inference\n",
        "    LATENT_MU.labels(latent_dim_type='classical').set(mu_test[:, :model.latent_dim_classical].mean().item())\n",
        "    LATENT_LOG_VAR.labels(latent_dim_type='classical').set(log_var_test[:, :model.latent_dim_classical].mean().item())\n",
        "    LATENT_MU.labels(latent_dim_type='quantum').set(mu_test[:, model.latent_dim_classical:].mean().item())\n",
        "    LATENT_LOG_VAR.labels(latent_dim_type='quantum').set(log_var_test[:, model.latent_dim_classical:].mean().item())\n",
        "\n",
        "\n",
        "    mu_classical_test = mu_test[:, :model.latent_dim_classical]\n",
        "    log_var_classical_test = log_var_test[:, :model.latent_dim_classical]\n",
        "    mu_quantum_test = mu_test[:, model.latent_dim_classical:]\n",
        "    log_var_quantum_test = log_var_test[:, model.latent_dim_classical:]\n",
        "\n",
        "    z_classical_test = model.reparameterize(mu_classical_test, log_var_classical_test)\n",
        "    z_quantum_input_test = model.reparameterize(mu_quantum_test, log_var_quantum_test)\n",
        "\n",
        "    # Quantum Layer\n",
        "    start_quantum_time = time.time()\n",
        "    z_quantum_output_test = model.quantum_layer(z_quantum_input_test)\n",
        "    end_quantum_time = time.time()\n",
        "    QUANTUM_CIRCUIT_DURATION.observe(end_quantum_time - start_quantum_time)\n",
        "    QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.inc(z_quantum_input_test.size(0)) # Increment by batch size\n",
        "\n",
        "    if isinstance(z_quantum_output_test, list):\n",
        "         z_quantum_output_test = torch.stack(z_quantum_output_test, dim=1)\n",
        "\n",
        "    z_quantum_output_test = z_quantum_output_test.float()\n",
        "\n",
        "    # Log quantum expectation value statistics for inference\n",
        "    QUANTUM_EXPVAL_MEAN.set(z_quantum_output_test.mean().item())\n",
        "    QUANTUM_EXPVAL_STD.set(z_quantum_output_test.std().item())\n",
        "\n",
        "\n",
        "    z_test = torch.cat((z_classical_test, z_quantum_output_test), dim=1)\n",
        "\n",
        "    # Decode\n",
        "    start_decode_time = time.time()\n",
        "    reconstructed_test_data = model.decoder_classical(z_test)\n",
        "    end_decode_time = time.time()\n",
        "    DECODER_PROCESSING_TIME.observe(end_decode_time - start_decode_time)\n",
        "\n",
        "    end_total_inference_time = time.time()\n",
        "    TOTAL_INFERENCE_TIME.observe(end_total_inference_time - start_total_inference_time)\n",
        "\n",
        "\n",
        "    # Calculate and log inference reconstruction error\n",
        "    inference_reconstruction_error = F.mse_loss(reconstructed_test_data, X_test_tensor, reduction='mean').item()\n",
        "    RECONSTRUCTION_ERROR_MSE.set(inference_reconstruction_error) # Set for inference\n",
        "\n",
        "\n",
        "print(\"\\nPrometheus metrics integrated into training and inference.\")\n",
        "print(\"Metrics can be accessed via `prometheus_client.REGISTRY.collect()` (e.g., for exposition).\")\n",
        "\n",
        "# Illustrative: Print some collected metrics\n",
        "print(\"\\nIllustrative collected metrics:\")\n",
        "for metric in prometheus_client.REGISTRY.collect():\n",
        "    print(f\"Metric: {metric.name}\")\n",
        "    for sample in metric.samples:\n",
        "        print(f\"  Sample: {sample.name}, Labels: {sample.labels}, Value: {sample.value}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'prometheus_client' has no attribute 'core'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3184171778.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Resetting default registry for notebook re-runs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# In newer versions, REGISTRY.clear() might work, but this approach is more compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprometheus_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_REGISTRY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprometheus_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCollectorRegistry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprometheus_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_registry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprometheus_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_REGISTRY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'prometheus_client' has no attribute 'core'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "hB1deF9UUxBG",
        "outputId": "3959d88f-5fa1-417f-b18b-9fdb81049222"
      },
      "source": [
        "import prometheus_client\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pennylane as qml\n",
        "import mlflow\n",
        "\n",
        "\n",
        "'''\n",
        "Error Fix: The error `AttributeError: 'CollectorRegistry' object has no attribute 'clear'` indicates that\n",
        "`prometheus_client.REGISTRY` does not have a `clear` method. To reset the default registry in Prometheus client\n",
        " versions that don't support `clear`, a new default registry can be created\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "# Define Prometheus metrics (Illustrative)\n",
        "# Removing the explicit registry reset attempt to avoid AttributeError\n",
        "\n",
        "# Classical Encoder Metrics\n",
        "ENCODER_PROCESSING_TIME = prometheus_client.Summary('encoder_processing_time_seconds', 'Time taken for classical encoder processing')\n",
        "LATENT_MU = prometheus_client.Gauge('latent_mu', 'Mean of the latent space mu values', ['latent_dim_type'])\n",
        "LATENT_LOG_VAR = prometheus_client.Gauge('latent_log_var', 'Mean of the latent space log_var values', ['latent_dim_type'])\n",
        "\n",
        "\n",
        "# Quantum Layer Metrics\n",
        "QUANTUM_CIRCUIT_DURATION = prometheus_client.Summary('quantum_circuit_duration_seconds', 'Duration of quantum circuit execution')\n",
        "QUANTUM_EXPVAL_MEAN = prometheus_client.Gauge('quantum_expval_mean', 'Mean of quantum expectation values')\n",
        "QUANTUM_EXPVAL_STD = prometheus_client.Gauge('quantum_expval_std', 'Standard deviation of quantum expectation values')\n",
        "QUANTUM_CIRCUIT_EVALUATIONS_TOTAL = prometheus_client.Counter('quantum_circuit_evaluations_total', 'Total number of quantum circuit evaluations')\n",
        "\n",
        "\n",
        "# Classical Decoder Metrics\n",
        "DECODER_PROCESSING_TIME = prometheus_client.Summary('decoder_processing_time_seconds', 'Time taken for classical decoder processing')\n",
        "\n",
        "# Overall VAE Metrics\n",
        "TOTAL_INFERENCE_TIME = prometheus_client.Summary('total_inference_time_seconds', 'Total time taken for inference requests')\n",
        "TRAINING_TIME_PER_EPOCH = prometheus_client.Summary('training_time_per_epoch_seconds', 'Total time taken for one training epoch')\n",
        "RECONSTRUCTION_ERROR_MSE = prometheus_client.Gauge('reconstruction_error_mse', 'Mean Squared Error of reconstruction')\n",
        "KL_DIVERGENCE_LOSS = prometheus_client.Gauge('kl_divergence_loss', 'Kullback-Leibler Divergence loss')\n",
        "TOTAL_VAE_LOSS = prometheus_client.Gauge('total_vae_loss', 'Total VAE loss (Reconstruction + KL)')\n",
        "\n",
        "\n",
        "# Define the Hybrid VAE model again to ensure it's accessible\n",
        "class HybridVAE(nn.Module):\n",
        "    def __init__(self, input_dim=100, latent_dim_classical=32, latent_dim_quantum=4):\n",
        "        super().__init__()\n",
        "        self.latent_dim_classical = latent_dim_classical\n",
        "        self.latent_dim_quantum = latent_dim_quantum\n",
        "\n",
        "        # Classical Encoder\n",
        "        self.encoder_classical = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 2 * (latent_dim_classical + latent_dim_quantum)) # Output 2x for mu and log_var\n",
        "        )\n",
        "\n",
        "        # Quantum Layer\n",
        "        self.dev = qml.device(\"default.qubit\", wires=latent_dim_quantum)\n",
        "\n",
        "        @qml.qnode(self.dev, interface=\"torch\")\n",
        "        def quantum_circuit(inputs):\n",
        "            # Simple variational quantum circuit\n",
        "            # inputs should have shape (batch_size, latent_dim_quantum)\n",
        "            for i in range(self.latent_dim_quantum):\n",
        "                # Apply RY gate to each qubit with the corresponding input from the latent space\n",
        "                qml.RY(inputs[:, i], wires=i)\n",
        "            for i in range(self.latent_dim_quantum - 1):\n",
        "                qml.CZ(wires=[i, i+1])\n",
        "            # Return a list of expectation values. PennyLane with torch interface\n",
        "            # should handle batching and convert this list of measurement processes to a tensor\n",
        "            # of shape (batch_size, latent_dim_quantum).\n",
        "            return [qml.expval(qml.PauliZ(i)) for i in range(self.latent_dim_quantum)]\n",
        "\n",
        "        self.quantum_layer = quantum_circuit\n",
        "\n",
        "        # Classical Decoder\n",
        "        self.decoder_classical = nn.Sequential(\n",
        "            nn.Linear(latent_dim_classical + latent_dim_quantum, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, input_dim)\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Classical Encoder forward pass\n",
        "        start_encode_time = time.time()\n",
        "        encoded = self.encoder_classical(x)\n",
        "        end_encode_time = time.time()\n",
        "        ENCODER_PROCESSING_TIME.observe(end_encode_time - start_encode_time)\n",
        "\n",
        "        # Reshape to separate mu and log_var for the combined latent space\n",
        "        mu_log_var = encoded.view(-1, 2, self.latent_dim_classical + self.latent_dim_quantum)\n",
        "        mu = mu_log_var[:, 0, :]\n",
        "        log_var = mu_log_var[:, 1, :]\n",
        "\n",
        "        # Log latent space parameter means\n",
        "        LATENT_MU.labels(latent_dim_type='classical').set(mu[:, :self.latent_dim_classical].mean().item())\n",
        "        LATENT_LOG_VAR.labels(latent_dim_type='classical').set(log_var[:, :self.latent_dim_classical].mean().item())\n",
        "        LATENT_MU.labels(latent_dim_type='quantum').set(mu[:, self.latent_dim_classical:].mean().item())\n",
        "        LATENT_LOG_VAR.labels(latent_dim_type='quantum').set(log_var[:, self.latent_dim_classical:].mean().item())\n",
        "\n",
        "\n",
        "        # Split latent space for classical and quantum parts\n",
        "        mu_classical = mu[:, :self.latent_dim_classical]\n",
        "        log_var_classical = log_var[:, :self.latent_dim_classical]\n",
        "        mu_quantum = mu[:, self.latent_dim_classical:]\n",
        "        log_var_quantum = log_var[:, self.latent_dim_classical:]\n",
        "\n",
        "        # Reparameterization trick for classical latent space\n",
        "        z_classical = self.reparameterize(mu_classical, log_var_classical)\n",
        "\n",
        "        # Pass quantum latent space through quantum layer\n",
        "        # Apply reparameterization trick for quantum latent space inputs to the quantum circuit\n",
        "        z_quantum_input = self.reparameterize(mu_quantum, log_var_quantum)\n",
        "\n",
        "        start_quantum_time = time.time()\n",
        "        # Pass the entire batch of quantum latent inputs to the quantum layer\n",
        "        z_quantum_output = self.quantum_layer(z_quantum_input)\n",
        "        end_quantum_time = time.time()\n",
        "        QUANTUM_CIRCUIT_DURATION.observe(end_quantum_time - start_quantum_time)\n",
        "        QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.inc(z_quantum_input.size(0)) # Increment by batch size\n",
        "\n",
        "        # If the output is a list of tensors, stack it. If it's already a single tensor, keep it.\n",
        "        if isinstance(z_quantum_output, list):\n",
        "             z_quantum_output = torch.stack(z_quantum_output, dim=1)\n",
        "\n",
        "        # Ensure data types match before concatenation\n",
        "        z_quantum_output = z_quantum_output.float()\n",
        "\n",
        "        # Log quantum expectation value statistics\n",
        "        QUANTUM_EXPVAL_MEAN.set(z_quantum_output.mean().item())\n",
        "        QUANTUM_EXPVAL_STD.set(z_quantum_output.std().item())\n",
        "\n",
        "\n",
        "        # Concatenate classical and quantum latent representations\n",
        "        z = torch.cat((z_classical, z_quantum_output), dim=1)\n",
        "\n",
        "        # Classical Decoder forward pass\n",
        "        start_decode_time = time.time()\n",
        "        reconstruction = self.decoder_classical(z)\n",
        "        end_decode_time = time.time()\n",
        "        DECODER_PROCESSING_TIME.observe(end_decode_time - start_decode_time)\n",
        "\n",
        "\n",
        "        return reconstruction, mu, log_var\n",
        "\n",
        "\n",
        "# Define the loss function (VAE loss: Reconstruction + KL Divergence)\n",
        "def vae_loss(recon_x, x, mu, log_var):\n",
        "    BCE = F.mse_loss(recon_x, x, reduction='sum') # Using MSE for reconstruction loss\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "\n",
        "    # Log loss components\n",
        "    RECONSTRUCTION_ERROR_MSE.set(BCE.item() / x.size(0)) # Log mean MSE\n",
        "    KL_DIVERGENCE_LOSS.set(KLD.item())\n",
        "    TOTAL_VAE_LOSS.set((BCE + KLD).item())\n",
        "\n",
        "    return BCE + KLD\n",
        "\n",
        "# Example usage in a training loop (adapted from previous cells)\n",
        "# Assuming X_train_tensor and X_test_tensor are already defined\n",
        "\n",
        "# Start an MLflow run\n",
        "with mlflow.start_run(run_name=\"Hybrid VAE Training with Metrics\"):\n",
        "    # Log hyperparameters\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 10\n",
        "    classical_latent_dim = 32\n",
        "    quantum_latent_dim = 4\n",
        "\n",
        "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
        "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
        "    mlflow.log_param(\"classical_latent_dim\", classical_latent_dim)\n",
        "    mlflow.log_param(\"quantum_latent_dim\", quantum_latent_dim)\n",
        "\n",
        "    model = HybridVAE(input_dim=X_train_tensor.shape[1],\n",
        "                      latent_dim_classical=classical_latent_dim,\n",
        "                      latent_dim_quantum=quantum_latent_dim)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    print(\"Starting Hybrid VAE training with Prometheus metrics...\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_epoch_time = time.time()\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Process the entire training tensor as a single batch\n",
        "        recon, mu, log_var = model(X_train_tensor)\n",
        "        loss = vae_loss(recon, X_train_tensor, mu, log_var)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        end_epoch_time = time.time()\n",
        "        epoch_duration = end_epoch_time - start_epoch_time\n",
        "        TRAINING_TIME_PER_EPOCH.observe(epoch_duration)\n",
        "\n",
        "\n",
        "        avg_train_loss = train_loss / X_train_tensor.size(0)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Log training loss for the epoch to MLflow\n",
        "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "        # Log Prometheus metrics related to the epoch (loss, etc.)\n",
        "        # Note: These logs to MLflow are for demonstration. In a real system,\n",
        "        # Prometheus would scrape a metrics endpoint.\n",
        "        try:\n",
        "            mlflow.log_metric(\"total_vae_loss_epoch\", TOTAL_VAE_LOSS.collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"reconstruction_error_mse_epoch\", RECONSTRUCTION_ERROR_MSE.collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"kl_divergence_loss_epoch\", KL_DIVERGENCE_LOSS.collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"training_time_per_epoch_s\", epoch_duration, step=epoch)\n",
        "            mlflow.log_metric(\"quantum_circuit_evaluations_total\", QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.collect()[0].samples[0].value, step=epoch)\n",
        "             # Log latent space and quantum metrics to MLflow\n",
        "            mlflow.log_metric(\"latent_mu_classical_mean\", LATENT_MU.labels(latent_dim_type='classical').collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"latent_log_var_classical_mean\", LATENT_LOG_VAR.labels(latent_dim_type='classical').collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"latent_mu_quantum_mean\", LATENT_MU.labels(latent_dim_type='quantum').collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"latent_log_var_quantum_mean\", LATENT_LOG_VAR.labels(latent_dim_type='quantum').collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"quantum_expval_mean\", QUANTUM_EXPVAL_MEAN.collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"quantum_expval_std\", QUANTUM_EXPVAL_STD.collect()[0].samples[0].value, step=epoch)\n",
        "            # Note: Summary metrics (like duration) from Prometheus client are Histograms.\n",
        "            # Logging their current observation count/sum might be less informative for trend over epoch.\n",
        "            # Logging average duration per epoch might be more suitable here, but requires calculation.\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"Error logging Prometheus metrics to MLflow: {e}\")\n",
        "\n",
        "\n",
        "    print(\"Hybrid VAE training finished.\")\n",
        "\n",
        "    # Log the trained model\n",
        "    mlflow.pytorch.log_model(model, \"hybrid_vae_model\")\n",
        "\n",
        "# The MLflow run automatically ends when exiting the 'with' block\n",
        "\n",
        "# Example usage in an inference scenario\n",
        "print(\"\\nStarting Hybrid VAE inference with Prometheus metrics...\")\n",
        "\n",
        "# Start a new MLflow run for inference benchmarking/logging\n",
        "with mlflow.start_run(run_name=\"Hybrid VAE Inference with Metrics\"):\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation for inference\n",
        "        start_total_inference_time = time.time()\n",
        "\n",
        "        # Encode\n",
        "        start_encode_time = time.time()\n",
        "        encoded_test_data = model.encoder_classical(X_test_tensor)\n",
        "        end_encode_time = time.time()\n",
        "        ENCODER_PROCESSING_TIME.observe(end_encode_time - start_encode_time)\n",
        "\n",
        "        mu_log_var_test = encoded_test_data.view(-1, 2, model.latent_dim_classical + model.latent_dim_quantum)\n",
        "        mu_test = mu_log_var_test[:, 0, :]\n",
        "        log_var_test = mu_log_var_test[:, 1, :]\n",
        "\n",
        "        # Log latent space parameter means for inference\n",
        "        LATENT_MU.labels(latent_dim_type='classical').set(mu_test[:, :model.latent_dim_classical].mean().item())\n",
        "        LATENT_LOG_VAR.labels(latent_dim_type='classical').set(log_var_test[:, :model.latent_dim_classical].mean().item())\n",
        "        LATENT_MU.labels(latent_dim_type='quantum').set(mu_test[:, model.latent_dim_classical:].mean().item())\n",
        "        LATENT_LOG_VAR.labels(latent_dim_type='quantum').set(log_var_test[:, model.latent_dim_classical:].mean().item())\n",
        "\n",
        "\n",
        "        mu_classical_test = mu_test[:, :model.latent_dim_classical]\n",
        "        log_var_classical_test = log_var_test[:, :model.latent_dim_classical]\n",
        "        mu_quantum_test = mu_test[:, model.latent_dim_classical:]\n",
        "        log_var_quantum_test = log_var_test[:, model.latent_dim_classical:]\n",
        "\n",
        "        z_classical_test = model.reparameterize(mu_classical_test, log_var_classical_test)\n",
        "        z_quantum_input_test = model.reparameterize(mu_quantum_test, log_var_quantum_test)\n",
        "\n",
        "        # Quantum Layer\n",
        "        start_quantum_time = time.time()\n",
        "        z_quantum_output_test = model.quantum_layer(z_quantum_input_test)\n",
        "        end_quantum_time = time.time()\n",
        "        QUANTUM_CIRCUIT_DURATION.observe(end_quantum_time - start_quantum_time)\n",
        "        QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.inc(z_quantum_input_test.size(0)) # Increment by batch size\n",
        "\n",
        "        if isinstance(z_quantum_output_test, list):\n",
        "             z_quantum_output_test = torch.stack(z_quantum_output_test, dim=1)\n",
        "\n",
        "        z_quantum_output_test = z_quantum_output_test.float()\n",
        "\n",
        "        # Log quantum expectation value statistics for inference\n",
        "        QUANTUM_EXPVAL_MEAN.set(z_quantum_output_test.mean().item())\n",
        "        QUANTUM_EXPVAL_STD.set(z_quantum_output_test.std().item())\n",
        "\n",
        "\n",
        "        z_test = torch.cat((z_classical_test, z_quantum_output_test), dim=1)\n",
        "\n",
        "        # Decode\n",
        "        start_decode_time = time.time()\n",
        "        reconstructed_test_data = model.decoder_classical(z_test)\n",
        "        end_decode_time = time.time()\n",
        "        DECODER_PROCESSING_TIME.observe(end_decode_time - start_decode_time)\n",
        "\n",
        "        end_total_inference_time = time.time()\n",
        "        total_inference_duration = end_total_inference_time - start_total_inference_time\n",
        "        TOTAL_INFERENCE_TIME.observe(total_inference_duration)\n",
        "\n",
        "\n",
        "        # Calculate and log inference reconstruction error\n",
        "        inference_reconstruction_error = F.mse_loss(reconstructed_test_data, X_test_tensor, reduction='mean').item()\n",
        "        RECONSTRUCTION_ERROR_MSE.set(inference_reconstruction_error) # Set for inference\n",
        "\n",
        "        # Log inference-specific metrics to MLflow\n",
        "        mlflow.log_metric(\"total_inference_time_s\", total_inference_duration)\n",
        "        mlflow.log_metric(\"inference_reconstruction_error_mse\", inference_reconstruction_error)\n",
        "        # Log component durations (these are Summaries, so logging their sum/count or calculating avg here)\n",
        "        # For simplicity, logging the *current* observation from the Summary which is less meaningful for a single run,\n",
        "        # but demonstrates logging. A real setup would scrape Prometheus.\n",
        "        try:\n",
        "            mlflow.log_metric(\"encoder_processing_time_s_sum\", ENCODER_PROCESSING_TIME.collect()[0].samples[1].value)\n",
        "            mlflow.log_metric(\"quantum_circuit_duration_s_sum\", QUANTUM_CIRCUIT_DURATION.collect()[0].samples[1].value)\n",
        "            mlflow.log_metric(\"decoder_processing_time_s_sum\", DECODER_PROCESSING_TIME.collect()[0].samples[1].value)\n",
        "            mlflow.log_metric(\"inference_quantum_circuit_evaluations\", QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_latent_mu_classical_mean\", LATENT_MU.labels(latent_dim_type='classical').collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_latent_log_var_classical_mean\", LATENT_LOG_VAR.labels(latent_dim_type='classical').collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_latent_mu_quantum_mean\", LATENT_MU.labels(latent_dim_type='quantum').collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_latent_log_var_quantum_mean\", LATENT_LOG_VAR.labels(latent_dim_type='quantum').collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_quantum_expval_mean\", QUANTUM_EXPVAL_MEAN.collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_quantum_expval_std\", QUANTUM_EXPVAL_STD.collect()[0].samples[0].value)\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"Error logging Prometheus metrics to MLflow during inference: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nPrometheus metrics integrated into training and inference.\")\n",
        "print(\"Metrics can be accessed via `prometheus_client.REGISTRY.collect()` (e.g., for exposition).\")\n",
        "\n",
        "# Illustrative: Print some collected metrics\n",
        "print(\"\\nIllustrative collected metrics:\")\n",
        "for metric in prometheus_client.REGISTRY.collect():\n",
        "    print(f\"Metric: {metric.name}\")\n",
        "    for sample in metric.samples:\n",
        "        print(f\"  Sample: {sample.name}, Labels: {sample.labels}, Value: {sample.value}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Hybrid VAE training with Prometheus metrics...\n",
            "Epoch [1/10], Loss: 1554.1306\n",
            "Epoch [2/10], Loss: 1546.7583\n",
            "Epoch [3/10], Loss: 1538.9165\n",
            "Epoch [4/10], Loss: 1532.9200\n",
            "Epoch [5/10], Loss: 1527.1137\n",
            "Epoch [6/10], Loss: 1521.8830\n",
            "Epoch [7/10], Loss: 1516.9719\n",
            "Epoch [8/10], Loss: 1512.5186\n",
            "Epoch [9/10], Loss: 1507.8349\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/04 18:23:34 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Loss: 1502.6196\n",
            "Hybrid VAE training finished.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "cannot pickle '_thread.lock' object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2182802487.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;31m# Log the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hybrid_vae_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;31m# The MLflow run automatically ends when exiting the 'with' block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/pytorch/__init__.py\u001b[0m in \u001b[0;36mlog_model\u001b[0;34m(pytorch_model, artifact_path, conda_env, code_paths, pickle_module, registered_model_name, signature, input_example, await_registration_for, extra_files, pip_requirements, extra_pip_requirements, metadata, name, params, tags, model_type, step, model_id, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \"\"\"\n\u001b[1;32m    287\u001b[0m     \u001b[0mpickle_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmlflow_pytorch_pickle_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m     return Model.log(\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0martifact_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0martifact_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/models/model.py\u001b[0m in \u001b[0;36mlog\u001b[0;34m(cls, artifact_path, flavor, registered_model_name, await_registration_for, metadata, run_id, resources, auth_policy, prompts, name, model_type, params, tags, step, model_id, **kwargs)\u001b[0m\n\u001b[1;32m   1205\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 )\n\u001b[0;32m-> 1207\u001b[0;31m                 \u001b[0mflavor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlflow_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlflow_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m                 \u001b[0;31m# `save_model` calls `load_model` to infer the model requirements, which may result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m                 \u001b[0;31m# in __pycache__ directories being created in the model directory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/pytorch/__init__.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(pytorch_model, path, conda_env, mlflow_model, code_paths, pickle_module, signature, input_example, extra_files, pip_requirements, extra_pip_requirements, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScriptModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytorch_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytorch_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0mtorchserve_artifacts_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                 \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0mpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyTorchPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m     \u001b[0mdata_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m     \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cloudpickle/cloudpickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m   1301\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"recursion\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot pickle '_thread.lock' object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "rohi8DFVU44C",
        "outputId": "ad5d251e-3853-45b6-8bce-e09ec9b78f15"
      },
      "source": [
        "import prometheus_client\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pennylane as qml\n",
        "import mlflow\n",
        "\n",
        "\n",
        "'''\n",
        "Fixing Error: The error `AttributeError: 'CollectorRegistry' object has no attribute 'clear'` indicates that\n",
        "`prometheus_client.REGISTRY` does not have a `clear` method. To reset the default registry in Prometheus client\n",
        " versions that don't support `clear`, a new default registry can be created\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "# Define Prometheus metrics (Illustrative)\n",
        "# Removing the explicit registry reset attempt to avoid AttributeError\n",
        "\n",
        "# Classical Encoder Metrics\n",
        "try:\n",
        "    ENCODER_PROCESSING_TIME = prometheus_client.Summary('encoder_processing_time_seconds', 'Time taken for classical encoder processing')\n",
        "    LATENT_MU = prometheus_client.Gauge('latent_mu', 'Mean of the latent space mu values', ['latent_dim_type'])\n",
        "    LATENT_LOG_VAR = prometheus_client.Gauge('latent_log_var', 'Mean of the latent space log_var values', ['latent_dim_type'])\n",
        "\n",
        "\n",
        "    # Quantum Layer Metrics\n",
        "    QUANTUM_CIRCUIT_DURATION = prometheus_client.Summary('quantum_circuit_duration_seconds', 'Duration of quantum circuit execution')\n",
        "    QUANTUM_EXPVAL_MEAN = prometheus_client.Gauge('quantum_expval_mean', 'Mean of quantum expectation values')\n",
        "    QUANTUM_EXPVAL_STD = prometheus_client.Gauge('quantum_expval_std', 'Standard deviation of quantum expectation values')\n",
        "    QUANTUM_CIRCUIT_EVALUATIONS_TOTAL = prometheus_client.Counter('quantum_circuit_evaluations_total', 'Total number of quantum circuit evaluations')\n",
        "\n",
        "\n",
        "    # Classical Decoder Metrics\n",
        "    DECODER_PROCESSING_TIME = prometheus_client.Summary('decoder_processing_time_seconds', 'Time taken for classical decoder processing')\n",
        "\n",
        "    # Overall VAE Metrics\n",
        "    TOTAL_INFERENCE_TIME = prometheus_client.Summary('total_inference_time_seconds', 'Total time taken for inference requests')\n",
        "    TRAINING_TIME_PER_EPOCH = prometheus_client.Summary('training_time_per_epoch_seconds', 'Total time taken for one training epoch')\n",
        "    RECONSTRUCTION_ERROR_MSE = prometheus_client.Gauge('reconstruction_error_mse', 'Mean Squared Error of reconstruction')\n",
        "    KL_DIVERGENCE_LOSS = prometheus_client.Gauge('kl_divergence_loss', 'Kullback-Leibler Divergence loss')\n",
        "    TOTAL_VAE_LOSS = prometheus_client.Gauge('total_vae_loss', 'Total VAE loss (Reconstruction + KL)')\n",
        "except ValueError as e:\n",
        "    print(f\"Metrics already registered: {e}. Skipping metric definition.\")\n",
        "\n",
        "\n",
        "# Define the Quantum Device and QNode outside the Module\n",
        "latent_dim_quantum = 4\n",
        "dev = qml.device(\"default.qubit\", wires=latent_dim_quantum)\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def quantum_circuit(inputs):\n",
        "    # Simple variational quantum circuit\n",
        "    # inputs should have shape (batch_size, latent_dim_quantum)\n",
        "    for i in range(latent_dim_quantum):\n",
        "        # Apply RY gate to each qubit with the corresponding input from the latent space\n",
        "        qml.RY(inputs[:, i], wires=i)\n",
        "    for i in range(latent_dim_quantum - 1):\n",
        "        qml.CZ(wires=[i, i+1])\n",
        "    # Return a list of expectation values. PennyLane with torch interface\n",
        "    # should handle batching and convert this list of measurement processes to a tensor\n",
        "    # of shape (batch_size, latent_dim_quantum).\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(latent_dim_quantum)]\n",
        "\n",
        "\n",
        "# Define the Hybrid VAE model, accepting the QNode as an argument\n",
        "class HybridVAE(nn.Module):\n",
        "    def __init__(self, input_dim=100, latent_dim_classical=32, quantum_layer=None):\n",
        "        super().__init__()\n",
        "        self.latent_dim_classical = latent_dim_classical\n",
        "        # Assuming quantum_layer is a QNode and implicitly knows its output dim\n",
        "        self.latent_dim_quantum = quantum_layer.func.num_wires # Get quantum dim from QNode wires\n",
        "\n",
        "        # Classical Encoder\n",
        "        self.encoder_classical = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 2 * (self.latent_dim_classical + self.latent_dim_quantum)) # Output 2x for mu and log_var\n",
        "        )\n",
        "\n",
        "        self.quantum_layer = quantum_layer # Store the passed QNode\n",
        "\n",
        "        # Classical Decoder\n",
        "        self.decoder_classical = nn.Sequential(\n",
        "            nn.Linear(self.latent_dim_classical + self.latent_dim_quantum, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, input_dim)\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Classical Encoder forward pass\n",
        "        start_encode_time = time.time()\n",
        "        encoded = self.encoder_classical(x)\n",
        "        end_encode_time = time.time()\n",
        "        ENCODER_PROCESSING_TIME.observe(end_encode_time - start_encode_time)\n",
        "\n",
        "        # Reshape to separate mu and log_var for the combined latent space\n",
        "        mu_log_var = encoded.view(-1, 2, self.latent_dim_classical + self.latent_dim_quantum)\n",
        "        mu = mu_log_var[:, 0, :]\n",
        "        log_var = mu_log_var[:, 1, :]\n",
        "\n",
        "        # Log latent space parameter means\n",
        "        LATENT_MU.labels(latent_dim_type='classical').set(mu[:, :self.latent_dim_classical].mean().item())\n",
        "        LATENT_LOG_VAR.labels(latent_dim_type='classical').set(log_var[:, :self.latent_dim_classical].mean().item())\n",
        "        LATENT_MU.labels(latent_dim_type='quantum').set(mu[:, self.latent_dim_classical:].mean().item())\n",
        "        LATENT_LOG_VAR.labels(latent_dim_type='quantum').set(log_var[:, self.latent_dim_classical:].mean().item())\n",
        "\n",
        "\n",
        "        # Split latent space for classical and quantum parts\n",
        "        mu_classical = mu[:, :self.latent_dim_classical]\n",
        "        log_var_classical = log_var[:, :self.latent_dim_classical]\n",
        "        mu_quantum = mu[:, self.latent_dim_classical:]\n",
        "        log_var_quantum = log_var[:, self.latent_dim_quantum:] # Use quantum log_var here\n",
        "\n",
        "        # Reparameterization trick for classical latent space\n",
        "        z_classical = self.reparameterize(mu_classical, log_var_classical)\n",
        "\n",
        "        # Pass quantum latent space through quantum layer\n",
        "        # Apply reparameterization trick for quantum latent space inputs to the quantum circuit\n",
        "        z_quantum_input = self.reparameterize(mu_quantum, log_var_quantum)\n",
        "\n",
        "        start_quantum_time = time.time()\n",
        "        # Pass the entire batch of quantum latent inputs to the quantum layer\n",
        "        z_quantum_output = self.quantum_layer(z_quantum_input)\n",
        "        end_quantum_time = time.time()\n",
        "        QUANTUM_CIRCUIT_DURATION.observe(end_quantum_time - start_quantum_time)\n",
        "        QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.inc(z_quantum_input.size(0)) # Increment by batch size\n",
        "\n",
        "        # If the output is a list of tensors, stack it. If it's already a single tensor, keep it.\n",
        "        if isinstance(z_quantum_output, list):\n",
        "             z_quantum_output = torch.stack(z_quantum_output, dim=1)\n",
        "\n",
        "        # Ensure data types match before concatenation\n",
        "        z_quantum_output = z_quantum_output.float()\n",
        "\n",
        "        # Log quantum expectation value statistics\n",
        "        QUANTUM_EXPVAL_MEAN.set(z_quantum_output.mean().item())\n",
        "        QUANTUM_EXPVAL_STD.set(z_quantum_output.std().item())\n",
        "\n",
        "\n",
        "        # Concatenate classical and quantum latent representations\n",
        "        z = torch.cat((z_classical, z_quantum_output), dim=1)\n",
        "\n",
        "        # Classical Decoder forward pass\n",
        "        start_decode_time = time.time()\n",
        "        reconstruction = self.decoder_classical(z)\n",
        "        end_decode_time = time.time()\n",
        "        DECODER_PROCESSING_TIME.observe(end_decode_time - start_decode_time)\n",
        "\n",
        "\n",
        "        return reconstruction, mu, log_var\n",
        "\n",
        "\n",
        "# Define the loss function (VAE loss: Reconstruction + KL Divergence)\n",
        "def vae_loss(recon_x, x, mu, log_var):\n",
        "    BCE = F.mse_loss(recon_x, x, reduction='sum') # Using MSE for reconstruction loss\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "\n",
        "    # Log loss components\n",
        "    RECONSTRUCTION_ERROR_MSE.set(BCE.item() / x.size(0)) # Log mean MSE\n",
        "    KL_DIVERGENCE_LOSS.set(KLD.item())\n",
        "    TOTAL_VAE_LOSS.set((BCE + KLD).item())\n",
        "\n",
        "    return BCE + KLD\n",
        "\n",
        "# Example usage in a training loop (adapted from previous cells)\n",
        "# Assuming X_train_tensor and X_test_tensor are already defined\n",
        "\n",
        "# Start an MLflow run\n",
        "with mlflow.start_run(run_name=\"Hybrid VAE Training with Metrics\"):\n",
        "    # Log hyperparameters\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 10\n",
        "    classical_latent_dim = 32\n",
        "    # quantum_latent_dim is now determined by the QNode passed to the model\n",
        "\n",
        "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
        "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
        "    mlflow.log_param(\"classical_latent_dim\", classical_latent_dim)\n",
        "    mlflow.log_param(\"quantum_latent_dim\", latent_dim_quantum) # Log the actual quantum dim\n",
        "\n",
        "\n",
        "    # Instantiate the model, passing the globally defined QNode\n",
        "    model = HybridVAE(input_dim=X_train_tensor.shape[1],\n",
        "                      latent_dim_classical=classical_latent_dim,\n",
        "                      quantum_layer=quantum_circuit)\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    print(\"Starting Hybrid VAE training with Prometheus metrics...\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_epoch_time = time.time()\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Process the entire training tensor as a single batch\n",
        "        recon, mu, log_var = model(X_train_tensor)\n",
        "        loss = vae_loss(recon, X_train_tensor, mu, log_var)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        end_epoch_time = time.time()\n",
        "        epoch_duration = end_epoch_time - start_epoch_time\n",
        "        TRAINING_TIME_PER_EPOCH.observe(epoch_duration)\n",
        "\n",
        "\n",
        "        avg_train_loss = train_loss / X_train_tensor.size(0)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Log training loss for the epoch to MLflow\n",
        "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "        # Log Prometheus metrics related to the epoch (loss, etc.)\n",
        "        # Note: These logs to MLflow are for demonstration. In a real system,\n",
        "        # Prometheus would scrape a metrics endpoint.\n",
        "        try:\n",
        "            mlflow.log_metric(\"total_vae_loss_epoch\", TOTAL_VAE_LOSS.collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"reconstruction_error_mse_epoch\", RECONSTRUCTION_ERROR_MSE.collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"kl_divergence_loss_epoch\", KL_DIVERGENCE_LOSS.collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"training_time_per_epoch_s\", epoch_duration, step=epoch)\n",
        "            mlflow.log_metric(\"quantum_circuit_evaluations_total\", QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.collect()[0].samples[0].value, step=epoch)\n",
        "             # Log latent space and quantum metrics to MLflow\n",
        "            mlflow.log_metric(\"latent_mu_classical_mean\", LATENT_MU.labels(latent_dim_type='classical').collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"latent_log_var_classical_mean\", LATENT_LOG_VAR.labels(latent_dim_type='classical').collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"latent_mu_quantum_mean\", LATENT_MU.labels(latent_dim_type='quantum').collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"latent_log_var_quantum_mean\", LATENT_LOG_VAR.labels(latent_dim_type='quantum').collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"quantum_expval_mean\", QUANTUM_EXPVAL_MEAN.collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"quantum_expval_std\", QUANTUM_EXPVAL_STD.collect()[0].samples[0].value, step=epoch)\n",
        "            # Note: Summary metrics (like duration) from Prometheus client are Histograms.\n",
        "            # Logging their current observation count/sum might be less informative for trend over epoch.\n",
        "            # Logging average duration per epoch might be more suitable here, but requires calculation.\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"Error logging Prometheus metrics to MLflow: {e}\")\n",
        "\n",
        "\n",
        "    print(\"Hybrid VAE training finished.\")\n",
        "\n",
        "    # Log the trained model\n",
        "    mlflow.pytorch.log_model(model, \"hybrid_vae_model\")\n",
        "\n",
        "# The MLflow run automatically ends when exiting the 'with' block\n",
        "\n",
        "# Example usage in an inference scenario\n",
        "print(\"\\nStarting Hybrid VAE inference with Prometheus metrics...\")\n",
        "\n",
        "# Start a new MLflow run for inference benchmarking/logging\n",
        "with mlflow.start_run(run_name=\"Hybrid VAE Inference with Metrics\"):\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation for inference\n",
        "        start_total_inference_time = time.time()\n",
        "\n",
        "        # Encode\n",
        "        start_encode_time = time.time()\n",
        "        encoded_test_data = model.encoder_classical(X_test_tensor)\n",
        "        end_encode_time = time.time()\n",
        "        ENCODER_PROCESSING_TIME.observe(end_encode_time - start_encode_time)\n",
        "\n",
        "        mu_log_var_test = encoded_test_data.view(-1, 2, model.latent_dim_classical + model.latent_dim_quantum)\n",
        "        mu_test = mu_log_var_test[:, 0, :]\n",
        "        log_var_test = mu_log_var_test[:, 1, :]\n",
        "\n",
        "        # Log latent space parameter means for inference\n",
        "        LATENT_MU.labels(latent_dim_type='classical').set(mu_test[:, :model.latent_dim_classical].mean().item())\n",
        "        LATENT_LOG_VAR.labels(latent_dim_type='classical').set(log_var_test[:, :model.latent_dim_classical].mean().item())\n",
        "        LATENT_MU.labels(latent_dim_type='quantum').set(mu_test[:, model.latent_dim_classical:].mean().item())\n",
        "        LATENT_LOG_VAR.labels(latent_dim_type='quantum').set(log_var_test[:, model.latent_dim_classical:].mean().item())\n",
        "\n",
        "\n",
        "        mu_classical_test = mu_test[:, :model.latent_dim_classical]\n",
        "        log_var_classical_test = log_var_test[:, :model.latent_dim_classical]\n",
        "        mu_quantum_test = mu_test[:, model.latent_dim_classical:]\n",
        "        log_var_quantum_test = log_var_test[:, model.latent_dim_classical:]\n",
        "\n",
        "        z_classical_test = model.reparameterize(mu_classical_test, log_var_classical_test)\n",
        "        z_quantum_input_test = model.reparameterize(mu_quantum_test, log_var_quantum_test)\n",
        "\n",
        "        # Quantum Layer\n",
        "        start_quantum_time = time.time()\n",
        "        z_quantum_output_test = model.quantum_layer(z_quantum_input_test)\n",
        "        end_quantum_time = time.time()\n",
        "        QUANTUM_CIRCUIT_DURATION.observe(end_quantum_time - start_quantum_time)\n",
        "        QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.inc(z_quantum_input_test.size(0)) # Increment by batch size\n",
        "\n",
        "        if isinstance(z_quantum_output_test, list):\n",
        "             z_quantum_output_test = torch.stack(z_quantum_output_test, dim=1)\n",
        "\n",
        "        z_quantum_output_test = z_quantum_output_test.float()\n",
        "\n",
        "        # Log quantum expectation value statistics for inference\n",
        "        QUANTUM_EXPVAL_MEAN.set(z_quantum_output_test.mean().item())\n",
        "        QUANTUM_EXPVAL_STD.set(z_quantum_output_test.std().item())\n",
        "\n",
        "\n",
        "        z_test = torch.cat((z_classical_test, z_quantum_output_test), dim=1)\n",
        "\n",
        "        # Decode\n",
        "        start_decode_time = time.time()\n",
        "        reconstructed_test_data = model.decoder_classical(z_test)\n",
        "        end_decode_time = time.time()\n",
        "        DECODER_PROCESSING_TIME.observe(end_decode_time - start_decode_time)\n",
        "\n",
        "        end_total_inference_time = time.time()\n",
        "        total_inference_duration = end_total_inference_time - start_total_inference_time\n",
        "        TOTAL_INFERENCE_TIME.observe(total_inference_duration)\n",
        "\n",
        "\n",
        "        # Calculate and log inference reconstruction error\n",
        "        inference_reconstruction_error = F.mse_loss(reconstructed_test_data, X_test_tensor, reduction='mean').item()\n",
        "        RECONSTRUCTION_ERROR_MSE.set(inference_reconstruction_error) # Set for inference\n",
        "\n",
        "        # Log inference-specific metrics to MLflow\n",
        "        mlflow.log_metric(\"total_inference_time_s\", total_inference_duration)\n",
        "        mlflow.log_metric(\"inference_reconstruction_error_mse\", inference_reconstruction_error)\n",
        "        # Log component durations (these are Summaries, so logging their sum/count or calculating avg here)\n",
        "        # For simplicity, logging the *current* observation from the Summary which is less meaningful for a single run,\n",
        "        # but demonstrates logging. A real setup would scrape Prometheus.\n",
        "        try:\n",
        "            mlflow.log_metric(\"encoder_processing_time_s_sum\", ENCODER_PROCESSING_TIME.collect()[0].samples[1].value)\n",
        "            mlflow.log_metric(\"quantum_circuit_duration_s_sum\", QUANTUM_CIRCUIT_DURATION.collect()[0].samples[1].value)\n",
        "            mlflow.log_metric(\"decoder_processing_time_s_sum\", DECODER_PROCESSING_TIME.collect()[0].samples[1].value)\n",
        "            mlflow.log_metric(\"inference_quantum_circuit_evaluations\", QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_latent_mu_classical_mean\", LATENT_MU.labels(latent_dim_type='classical').collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_latent_log_var_classical_mean\", LATENT_LOG_VAR.labels(latent_dim_type='classical').collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_latent_mu_quantum_mean\", LATENT_MU.labels(latent_dim_type='quantum').collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_latent_log_var_quantum_mean\", LATENT_LOG_VAR.labels(latent_dim_type='quantum').collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_quantum_expval_mean\", QUANTUM_EXPVAL_MEAN.collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_quantum_expval_std\", QUANTUM_EXPVAL_STD.collect()[0].samples[0].value)\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"Error logging Prometheus metrics to MLflow during inference: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nPrometheus metrics integrated into training and inference.\")\n",
        "print(\"Metrics can be accessed via `prometheus_client.REGISTRY.collect()` (e.g., for exposition).\")\n",
        "\n",
        "# Illustrative: Print some collected metrics\n",
        "print(\"\\nIllustrative collected metrics:\")\n",
        "for metric in prometheus_client.REGISTRY.collect():\n",
        "    print(f\"Metric: {metric.name}\")\n",
        "    for sample in metric.samples:\n",
        "        print(f\"  Sample: {sample.name}, Labels: {sample.labels}, Value: {sample.value}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics already registered: Duplicated timeseries in CollectorRegistry: {'encoder_processing_time_seconds_sum', 'encoder_processing_time_seconds_created', 'encoder_processing_time_seconds', 'encoder_processing_time_seconds_count'}. Skipping metric definition.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'function' object has no attribute 'num_wires'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3003270388.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;31m# Instantiate the model, passing the globally defined QNode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     model = HybridVAE(input_dim=X_train_tensor.shape[1],\n\u001b[0m\u001b[1;32m    183\u001b[0m                       \u001b[0mlatent_dim_classical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclassical_latent_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                       quantum_layer=quantum_circuit)\n",
            "\u001b[0;32m/tmp/ipython-input-3003270388.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, latent_dim_classical, quantum_layer)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_dim_classical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlatent_dim_classical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# Assuming quantum_layer is a QNode and implicitly knows its output dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_dim_quantum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquantum_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_wires\u001b[0m \u001b[0;31m# Get quantum dim from QNode wires\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# Classical Encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'num_wires'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "GfOI-267VApZ",
        "outputId": "23f170d7-61c6-462b-8226-ae6f7a6db37d"
      },
      "source": [
        "import prometheus_client\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pennylane as qml\n",
        "import mlflow\n",
        "\n",
        "'''\n",
        "Error FIx:\n",
        "\n",
        "The AttributeError: 'function' object has no attribute 'num_wires' occurs because quantum_circuit is a decorated function\n",
        "(a QNode), and accessing quantum_layer.func.num_wires inside the HybridVAE's __init__ expects quantum_layer.func to be a\n",
        " QNode object with a num_wires attribute, but it seems to be a regular function here. The number of quantum wires\n",
        " (and thus the quantum latent dimension) is a property of the PennyLane device, not the QNode function itself in this context.\n",
        " The quantum latent dimension was already defined globally as latent_dim_quantum. The HybridVAE should use this global variable\n",
        " directly. The ValueError: Duplicated timeseries in CollectorRegistry is expected when rerunning the cell without clearing the\n",
        "  registry; the try...except ValueError block is already in place to handle this by skipping metric definition on subsequent runs.\n",
        "'''\n",
        "\n",
        "# Define Prometheus metrics (Illustrative)\n",
        "# Removing the explicit registry reset attempt to avoid AttributeError\n",
        "\n",
        "# Classical Encoder Metrics\n",
        "try:\n",
        "    ENCODER_PROCESSING_TIME = prometheus_client.Summary('encoder_processing_time_seconds', 'Time taken for classical encoder processing')\n",
        "    LATENT_MU = prometheus_client.Gauge('latent_mu', 'Mean of the latent space mu values', ['latent_dim_type'])\n",
        "    LATENT_LOG_VAR = prometheus_client.Gauge('latent_log_var', 'Mean of the latent space log_var values', ['latent_dim_type'])\n",
        "\n",
        "\n",
        "    # Quantum Layer Metrics\n",
        "    QUANTUM_CIRCUIT_DURATION = prometheus_client.Summary('quantum_circuit_duration_seconds', 'Duration of quantum circuit execution')\n",
        "    QUANTUM_EXPVAL_MEAN = prometheus_client.Gauge('quantum_expval_mean', 'Mean of quantum expectation values')\n",
        "    QUANTUM_EXPVAL_STD = prometheus_client.Gauge('quantum_expval_std', 'Standard deviation of quantum expectation values')\n",
        "    QUANTUM_CIRCUIT_EVALUATIONS_TOTAL = prometheus_client.Counter('quantum_circuit_evaluations_total', 'Total number of quantum circuit evaluations')\n",
        "\n",
        "\n",
        "    # Classical Decoder Metrics\n",
        "    DECODER_PROCESSING_TIME = prometheus_client.Summary('decoder_processing_time_seconds', 'Time taken for classical decoder processing')\n",
        "\n",
        "    # Overall VAE Metrics\n",
        "    TOTAL_INFERENCE_TIME = prometheus_client.Summary('total_inference_time_seconds', 'Total time taken for inference requests')\n",
        "    TRAINING_TIME_PER_EPOCH = prometheus_client.Summary('training_time_per_epoch_seconds', 'Total time taken for one training epoch')\n",
        "    RECONSTRUCTION_ERROR_MSE = prometheus_client.Gauge('reconstruction_error_mse', 'Mean Squared Error of reconstruction')\n",
        "    KL_DIVERGENCE_LOSS = prometheus_client.Gauge('kl_divergence_loss', 'Kullback-Leibler Divergence loss')\n",
        "    TOTAL_VAE_LOSS = prometheus_client.Gauge('total_vae_loss', 'Total VAE loss (Reconstruction + KL)')\n",
        "except ValueError as e:\n",
        "    print(f\"Metrics already registered: {e}. Skipping metric definition.\")\n",
        "\n",
        "\n",
        "# Define the Quantum Device and QNode outside the Module\n",
        "latent_dim_quantum = 4\n",
        "dev = qml.device(\"default.qubit\", wires=latent_dim_quantum)\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def quantum_circuit(inputs):\n",
        "    # Simple variational quantum circuit\n",
        "    # inputs should have shape (batch_size, latent_dim_quantum)\n",
        "    for i in range(latent_dim_quantum):\n",
        "        # Apply RY gate to each qubit with the corresponding input from the latent space\n",
        "        qml.RY(inputs[:, i], wires=i)\n",
        "    for i in range(latent_dim_quantum - 1):\n",
        "        qml.CZ(wires=[i, i+1])\n",
        "    # Return a list of expectation values. PennyLane with torch interface\n",
        "    # should handle batching and convert this list of measurement processes to a tensor\n",
        "    # of shape (batch_size, latent_dim_quantum).\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(latent_dim_quantum)]\n",
        "\n",
        "\n",
        "# Define the Hybrid VAE model, accepting the QNode as an argument and using the global latent_dim_quantum\n",
        "class HybridVAE(nn.Module):\n",
        "    def __init__(self, input_dim=100, latent_dim_classical=32, quantum_layer=None, latent_dim_quantum=4):\n",
        "        super().__init__()\n",
        "        self.latent_dim_classical = latent_dim_classical\n",
        "        self.latent_dim_quantum = latent_dim_quantum # Use the passed or default quantum latent dim\n",
        "\n",
        "        # Classical Encoder\n",
        "        self.encoder_classical = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 2 * (self.latent_dim_classical + self.latent_dim_quantum)) # Output 2x for mu and log_var\n",
        "        )\n",
        "\n",
        "        self.quantum_layer = quantum_layer # Store the passed QNode\n",
        "\n",
        "        # Classical Decoder\n",
        "        self.decoder_classical = nn.Sequential(\n",
        "            nn.Linear(self.latent_dim_classical + self.latent_dim_quantum, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, input_dim)\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Classical Encoder forward pass\n",
        "        start_encode_time = time.time()\n",
        "        encoded = self.encoder_classical(x)\n",
        "        end_encode_time = time.time()\n",
        "        ENCODER_PROCESSING_TIME.observe(end_encode_time - start_encode_time)\n",
        "\n",
        "        # Reshape to separate mu and log_var for the combined latent space\n",
        "        mu_log_var = encoded.view(-1, 2, self.latent_dim_classical + self.latent_dim_quantum)\n",
        "        mu = mu_log_var[:, 0, :]\n",
        "        log_var = mu_log_var[:, 1, :]\n",
        "\n",
        "        # Log latent space parameter means\n",
        "        LATENT_MU.labels(latent_dim_type='classical').set(mu[:, :self.latent_dim_classical].mean().item())\n",
        "        LATENT_LOG_VAR.labels(latent_dim_type='classical').set(log_var[:, :self.latent_dim_classical].mean().item())\n",
        "        LATENT_MU.labels(latent_dim_type='quantum').set(mu[:, self.latent_dim_classical:].mean().item())\n",
        "        LATENT_LOG_VAR.labels(latent_dim_type='quantum').set(log_var[:, self.latent_dim_classical:].mean().item())\n",
        "\n",
        "\n",
        "        # Split latent space for classical and quantum parts\n",
        "        mu_classical = mu[:, :self.latent_dim_classical]\n",
        "        log_var_classical = log_var[:, :self.latent_dim_classical]\n",
        "        mu_quantum = mu[:, self.latent_dim_classical:]\n",
        "        log_var_quantum = log_var[:, self.latent_dim_classical:] # Use quantum log_var here\n",
        "\n",
        "        # Reparameterization trick for classical latent space\n",
        "        z_classical = self.reparameterize(mu_classical, log_var_classical)\n",
        "\n",
        "        # Pass quantum latent space through quantum layer\n",
        "        # Apply reparameterization trick for quantum latent space inputs to the quantum circuit\n",
        "        z_quantum_input = self.reparameterize(mu_quantum, log_var_quantum)\n",
        "\n",
        "        start_quantum_time = time.time()\n",
        "        # Pass the entire batch of quantum latent inputs to the quantum layer\n",
        "        z_quantum_output = self.quantum_layer(z_quantum_input)\n",
        "        end_quantum_time = time.time()\n",
        "        QUANTUM_CIRCUIT_DURATION.observe(end_quantum_time - start_quantum_time)\n",
        "        QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.inc(z_quantum_input.size(0)) # Increment by batch size\n",
        "\n",
        "        # If the output is a list of tensors, stack it. If it's already a single tensor, keep it.\n",
        "        if isinstance(z_quantum_output, list):\n",
        "             z_quantum_output = torch.stack(z_quantum_output, dim=1)\n",
        "\n",
        "        # Ensure data types match before concatenation\n",
        "        z_quantum_output = z_quantum_output.float()\n",
        "\n",
        "        # Log quantum expectation value statistics\n",
        "        QUANTUM_EXPVAL_MEAN.set(z_quantum_output.mean().item())\n",
        "        QUANTUM_EXPVAL_STD.set(z_quantum_output.std().item())\n",
        "\n",
        "\n",
        "        # Concatenate classical and quantum latent representations\n",
        "        z = torch.cat((z_classical, z_quantum_output), dim=1)\n",
        "\n",
        "        # Classical Decoder forward pass\n",
        "        start_decode_time = time.time()\n",
        "        reconstruction = self.decoder_classical(z)\n",
        "        end_decode_time = time.time()\n",
        "        DECODER_PROCESSING_TIME.observe(end_decode_time - start_decode_time)\n",
        "\n",
        "\n",
        "        return reconstruction, mu, log_var\n",
        "\n",
        "\n",
        "# Define the loss function (VAE loss: Reconstruction + KL Divergence)\n",
        "def vae_loss(recon_x, x, mu, log_var):\n",
        "    BCE = F.mse_loss(recon_x, x, reduction='sum') # Using MSE for reconstruction loss\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "\n",
        "    # Log loss components\n",
        "    RECONSTRUCTION_ERROR_MSE.set(BCE.item() / x.size(0)) # Log mean MSE\n",
        "    KL_DIVERGENCE_LOSS.set(KLD.item())\n",
        "    TOTAL_VAE_LOSS.set((BCE + KLD).item())\n",
        "\n",
        "    return BCE + KLD\n",
        "\n",
        "# Example usage in a training loop (adapted from previous cells)\n",
        "# Assuming X_train_tensor and X_test_tensor are already defined\n",
        "\n",
        "# Start an MLflow run\n",
        "with mlflow.start_run(run_name=\"Hybrid VAE Training with Metrics\"):\n",
        "    # Log hyperparameters\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 10\n",
        "    classical_latent_dim = 32\n",
        "    # quantum_latent_dim is now determined by the QNode passed to the model\n",
        "\n",
        "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
        "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
        "    mlflow.log_param(\"classical_latent_dim\", classical_latent_dim)\n",
        "    mlflow.log_param(\"quantum_latent_dim\", latent_dim_quantum) # Log the actual quantum dim\n",
        "\n",
        "\n",
        "    # Instantiate the model, passing the globally defined QNode and quantum_latent_dim\n",
        "    model = HybridVAE(input_dim=X_train_tensor.shape[1],\n",
        "                      latent_dim_classical=classical_latent_dim,\n",
        "                      quantum_layer=quantum_circuit,\n",
        "                      latent_dim_quantum=latent_dim_quantum)\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    print(\"Starting Hybrid VAE training with Prometheus metrics...\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_epoch_time = time.time()\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Process the entire training tensor as a single batch\n",
        "        recon, mu, log_var = model(X_train_tensor)\n",
        "        loss = vae_loss(recon, X_train_tensor, mu, log_var)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        end_epoch_time = time.time()\n",
        "        epoch_duration = end_epoch_time - start_epoch_time\n",
        "        TRAINING_TIME_PER_EPOCH.observe(epoch_duration)\n",
        "\n",
        "\n",
        "        avg_train_loss = train_loss / X_train_tensor.size(0)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Log training loss for the epoch to MLflow\n",
        "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "        # Log Prometheus metrics related to the epoch (loss, etc.)\n",
        "        # Note: These logs to MLflow are for demonstration. In a real system,\n",
        "        # Prometheus would scrape a metrics endpoint.\n",
        "        try:\n",
        "            mlflow.log_metric(\"total_vae_loss_epoch\", TOTAL_VAE_LOSS.collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"reconstruction_error_mse_epoch\", RECONSTRUCTION_ERROR_MSE.collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"kl_divergence_loss_epoch\", KL_DIVERGENCE_LOSS.collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"training_time_per_epoch_s\", epoch_duration, step=epoch)\n",
        "            mlflow.log_metric(\"quantum_circuit_evaluations_total\", QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.collect()[0].samples[0].value, step=epoch)\n",
        "             # Log latent space and quantum metrics to MLflow\n",
        "            mlflow.log_metric(\"latent_mu_classical_mean\", LATENT_MU.labels(latent_dim_type='classical').collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"latent_log_var_classical_mean\", LATENT_LOG_VAR.labels(latent_dim_type='classical').collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"latent_mu_quantum_mean\", LATENT_MU.labels(latent_dim_type='quantum').collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"latent_log_var_quantum_mean\", LATENT_LOG_VAR.labels(latent_dim_type='quantum').collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"quantum_expval_mean\", QUANTUM_EXPVAL_MEAN.collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"quantum_expval_std\", QUANTUM_EXPVAL_STD.collect()[0].samples[0].value, step=epoch)\n",
        "            # Note: Summary metrics (like duration) from Prometheus client are Histograms.\n",
        "            # Logging their current observation count/sum might be less informative for trend over epoch.\n",
        "            # Logging average duration per epoch might be more suitable here, but requires calculation.\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"Error logging Prometheus metrics to MLflow: {e}\")\n",
        "\n",
        "\n",
        "    print(\"Hybrid VAE training finished.\")\n",
        "\n",
        "    # Log the trained model\n",
        "    mlflow.pytorch.log_model(model, \"hybrid_vae_model\")\n",
        "\n",
        "# The MLflow run automatically ends when exiting the 'with' block\n",
        "\n",
        "# Example usage in an inference scenario\n",
        "print(\"\\nStarting Hybrid VAE inference with Prometheus metrics...\")\n",
        "\n",
        "# Start a new MLflow run for inference benchmarking/logging\n",
        "with mlflow.start_run(run_name=\"Hybrid VAE Inference with Metrics\"):\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation for inference\n",
        "        start_total_inference_time = time.time()\n",
        "\n",
        "        # Encode\n",
        "        start_encode_time = time.time()\n",
        "        encoded_test_data = model.encoder_classical(X_test_tensor)\n",
        "        end_encode_time = time.time()\n",
        "        ENCODER_PROCESSING_TIME.observe(end_encode_time - start_encode_time)\n",
        "\n",
        "        mu_log_var_test = encoded_test_data.view(-1, 2, model.latent_dim_classical + model.latent_dim_quantum)\n",
        "        mu_test = mu_log_var_test[:, 0, :]\n",
        "        log_var_test = mu_log_var_test[:, 1, :]\n",
        "\n",
        "        # Log latent space parameter means for inference\n",
        "        LATENT_MU.labels(latent_dim_type='classical').set(mu_test[:, :model.latent_dim_classical].mean().item())\n",
        "        LATENT_LOG_VAR.labels(latent_dim_type='classical').set(log_var_test[:, :model.latent_dim_classical].mean().item())\n",
        "        LATENT_MU.labels(latent_dim_type='quantum').set(mu_test[:, model.latent_dim_classical:].mean().item())\n",
        "        LATENT_LOG_VAR.labels(latent_dim_type='quantum').set(log_var_test[:, model.latent_dim_classical:].mean().item())\n",
        "\n",
        "\n",
        "        mu_classical_test = mu_test[:, :model.latent_dim_classical]\n",
        "        log_var_classical_test = log_var_test[:, :model.latent_dim_classical]\n",
        "        mu_quantum_test = mu_test[:, model.latent_dim_classical:]\n",
        "        log_var_quantum_test = log_var_test[:, model.latent_dim_classical:]\n",
        "\n",
        "        z_classical_test = model.reparameterize(mu_classical_test, log_var_classical_test)\n",
        "        z_quantum_input_test = model.reparameterize(mu_quantum_test, log_var_quantum_test)\n",
        "\n",
        "        # Quantum Layer\n",
        "        start_quantum_time = time.time()\n",
        "        z_quantum_output_test = model.quantum_layer(z_quantum_input_test)\n",
        "        end_quantum_time = time.time()\n",
        "        QUANTUM_CIRCUIT_DURATION.observe(end_quantum_time - start_quantum_time)\n",
        "        QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.inc(z_quantum_input_test.size(0)) # Increment by batch size\n",
        "\n",
        "        if isinstance(z_quantum_output_test, list):\n",
        "             z_quantum_output_test = torch.stack(z_quantum_output_test, dim=1)\n",
        "\n",
        "        z_quantum_output_test = z_quantum_output_test.float()\n",
        "\n",
        "        # Log quantum expectation value statistics for inference\n",
        "        QUANTUM_EXPVAL_MEAN.set(z_quantum_output_test.mean().item())\n",
        "        QUANTUM_EXPVAL_STD.set(z_quantum_output_test.std().item())\n",
        "\n",
        "\n",
        "        z_test = torch.cat((z_classical_test, z_quantum_output_test), dim=1)\n",
        "\n",
        "        # Decode\n",
        "        start_decode_time = time.time()\n",
        "        reconstructed_test_data = model.decoder_classical(z_test)\n",
        "        end_decode_time = time.time()\n",
        "        DECODER_PROCESSING_TIME.observe(end_decode_time - start_decode_time)\n",
        "\n",
        "        end_total_inference_time = time.time()\n",
        "        total_inference_duration = end_total_inference_time - start_total_inference_time\n",
        "        TOTAL_INFERENCE_TIME.observe(total_inference_duration)\n",
        "\n",
        "\n",
        "        # Calculate and log inference reconstruction error\n",
        "        inference_reconstruction_error = F.mse_loss(reconstructed_test_data, X_test_tensor, reduction='mean').item()\n",
        "        RECONSTRUCTION_ERROR_MSE.set(inference_reconstruction_error) # Set for inference\n",
        "\n",
        "        # Log inference-specific metrics to MLflow\n",
        "        mlflow.log_metric(\"total_inference_time_s\", total_inference_duration)\n",
        "        mlflow.log_metric(\"inference_reconstruction_error_mse\", inference_reconstruction_error)\n",
        "        # Log component durations (these are Summaries, so logging their sum/count or calculating avg here)\n",
        "        # For simplicity, logging the *current* observation from the Summary which is less meaningful for a single run,\n",
        "        # but demonstrates logging. A real setup would scrape Prometheus.\n",
        "        try:\n",
        "            mlflow.log_metric(\"encoder_processing_time_s_sum\", ENCODER_PROCESSING_TIME.collect()[0].samples[1].value)\n",
        "            mlflow.log_metric(\"quantum_circuit_duration_s_sum\", QUANTUM_CIRCUIT_DURATION.collect()[0].samples[1].value)\n",
        "            mlflow.log_metric(\"decoder_processing_time_s_sum\", DECODER_PROCESSING_TIME.collect()[0].samples[1].value)\n",
        "            mlflow.log_metric(\"inference_quantum_circuit_evaluations\", QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_latent_mu_classical_mean\", LATENT_MU.labels(latent_dim_type='classical').collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_latent_log_var_classical_mean\", LATENT_LOG_VAR.labels(latent_dim_type='classical').collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_latent_mu_quantum_mean\", LATENT_MU.labels(latent_dim_type='quantum').collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_latent_log_var_quantum_mean\", LATENT_LOG_VAR.labels(latent_dim_type='quantum').collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_quantum_expval_mean\", QUANTUM_EXPVAL_MEAN.collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_quantum_expval_std\", QUANTUM_EXPVAL_STD.collect()[0].samples[0].value)\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"Error logging Prometheus metrics to MLflow during inference: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nPrometheus metrics integrated into training and inference.\")\n",
        "print(\"Metrics can be accessed via `prometheus_client.REGISTRY.collect()` (e.g., for exposition).\")\n",
        "\n",
        "# Illustrative: Print some collected metrics\n",
        "print(\"\\nIllustrative collected metrics:\")\n",
        "for metric in prometheus_client.REGISTRY.collect():\n",
        "    print(f\"Metric: {metric.name}\")\n",
        "    for sample in metric.samples:\n",
        "        print(f\"  Sample: {sample.name}, Labels: {sample.labels}, Value: {sample.value}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics already registered: Duplicated timeseries in CollectorRegistry: {'encoder_processing_time_seconds_sum', 'encoder_processing_time_seconds_created', 'encoder_processing_time_seconds', 'encoder_processing_time_seconds_count'}. Skipping metric definition.\n",
            "Starting Hybrid VAE training with Prometheus metrics...\n",
            "Epoch [1/10], Loss: 1566.4047\n",
            "Epoch [2/10], Loss: 1557.2369\n",
            "Epoch [3/10], Loss: 1549.9280\n",
            "Epoch [4/10], Loss: 1543.3081\n",
            "Epoch [5/10], Loss: 1537.3746\n",
            "Epoch [6/10], Loss: 1531.6289\n",
            "Epoch [7/10], Loss: 1526.6816\n",
            "Epoch [8/10], Loss: 1522.5228\n",
            "Epoch [9/10], Loss: 1518.0074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/04 18:24:38 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Loss: 1513.8805\n",
            "Hybrid VAE training finished.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "cannot pickle '_thread.lock' object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1575462300.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;31m# Log the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hybrid_vae_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;31m# The MLflow run automatically ends when exiting the 'with' block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/pytorch/__init__.py\u001b[0m in \u001b[0;36mlog_model\u001b[0;34m(pytorch_model, artifact_path, conda_env, code_paths, pickle_module, registered_model_name, signature, input_example, await_registration_for, extra_files, pip_requirements, extra_pip_requirements, metadata, name, params, tags, model_type, step, model_id, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \"\"\"\n\u001b[1;32m    287\u001b[0m     \u001b[0mpickle_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmlflow_pytorch_pickle_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m     return Model.log(\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0martifact_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0martifact_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/models/model.py\u001b[0m in \u001b[0;36mlog\u001b[0;34m(cls, artifact_path, flavor, registered_model_name, await_registration_for, metadata, run_id, resources, auth_policy, prompts, name, model_type, params, tags, step, model_id, **kwargs)\u001b[0m\n\u001b[1;32m   1205\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 )\n\u001b[0;32m-> 1207\u001b[0;31m                 \u001b[0mflavor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlflow_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlflow_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m                 \u001b[0;31m# `save_model` calls `load_model` to infer the model requirements, which may result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m                 \u001b[0;31m# in __pycache__ directories being created in the model directory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/pytorch/__init__.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(pytorch_model, path, conda_env, mlflow_model, code_paths, pickle_module, signature, input_example, extra_files, pip_requirements, extra_pip_requirements, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScriptModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytorch_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytorch_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0mtorchserve_artifacts_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                 \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0mpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyTorchPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m     \u001b[0mdata_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m     \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cloudpickle/cloudpickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m   1301\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"recursion\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot pickle '_thread.lock' object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "Z6HGQkLdVJyB",
        "outputId": "78979c2d-1115-4bf5-ccb2-ab46f438a204"
      },
      "source": [
        "import prometheus_client\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pennylane as qml\n",
        "import mlflow\n",
        "\n",
        "\n",
        "''''\n",
        "The TypeError: cannot pickle '_thread.lock' object persists during mlflow.pytorch.log_model. This error is\n",
        "still related to pickling issues, likely because the quantum_circuit QNode, even when defined outside the\n",
        "class, might still hold references to non-picklable PennyLane internal objects (like the device or its resources)\n",
        " when stored as an attribute of the HybridVAE model. A more robust solution to avoid pickling issues with PennyLane\n",
        "  QNodes when logging the PyTorch model is to make the QNode callable directly within the forward pass without\n",
        "  storing it as a persistent attribute of the nn.Module that gets pickled. This means re-creating the QNode\n",
        "  (or accessing a globally defined one) within the forward method, ensuring it doesn't become part of the\n",
        "  model's state that needs to be serialized. This might have performance implications but resolves the pickling error.\n",
        "   Alternatively, if the QNode can be defined such that it doesn't hold non-picklable state, that would be ideal,\n",
        "   but redefining it in forward is a common workaround. I will modify the HybridVAE class to not store the\n",
        "   quantum_layer as an attribute, but instead access the globally defined quantum_circuit QNode directly\n",
        "   within the forward method.\n",
        "\n",
        "''''\n",
        "\n",
        "\n",
        "# Define Prometheus metrics (Illustrative)\n",
        "# Removing the explicit registry reset attempt to avoid AttributeError\n",
        "\n",
        "# Classical Encoder Metrics\n",
        "try:\n",
        "    ENCODER_PROCESSING_TIME = prometheus_client.Summary('encoder_processing_time_seconds', 'Time taken for classical encoder processing')\n",
        "    LATENT_MU = prometheus_client.Gauge('latent_mu', 'Mean of the latent space mu values', ['latent_dim_type'])\n",
        "    LATENT_LOG_VAR = prometheus_client.Gauge('latent_log_var', 'Mean of the latent space log_var values', ['latent_dim_type'])\n",
        "\n",
        "\n",
        "    # Quantum Layer Metrics\n",
        "    QUANTUM_CIRCUIT_DURATION = prometheus_client.Summary('quantum_circuit_duration_seconds', 'Duration of quantum circuit execution')\n",
        "    QUANTUM_EXPVAL_MEAN = prometheus_client.Gauge('quantum_expval_mean', 'Mean of quantum expectation values')\n",
        "    QUANTUM_EXPVAL_STD = prometheus_client.Gauge('quantum_expval_std', 'Standard deviation of quantum expectation values')\n",
        "    QUANTUM_CIRCUIT_EVALUATIONS_TOTAL = prometheus_client.Counter('quantum_circuit_evaluations_total', 'Total number of quantum circuit evaluations')\n",
        "\n",
        "\n",
        "    # Classical Decoder Metrics\n",
        "    DECODER_PROCESSING_TIME = prometheus_client.Summary('decoder_processing_time_seconds', 'Time taken for classical decoder processing')\n",
        "\n",
        "    # Overall VAE Metrics\n",
        "    TOTAL_INFERENCE_TIME = prometheus_client.Summary('total_inference_time_seconds', 'Total time taken for inference requests')\n",
        "    TRAINING_TIME_PER_EPOCH = prometheus_client.Summary('training_time_per_epoch_seconds', 'Total time taken for one training epoch')\n",
        "    RECONSTRUCTION_ERROR_MSE = prometheus_client.Gauge('reconstruction_error_mse', 'Mean Squared Error of reconstruction')\n",
        "    KL_DIVERGENCE_LOSS = prometheus_client.Gauge('kl_divergence_loss', 'Kullback-Leibler Divergence loss')\n",
        "    TOTAL_VAE_LOSS = prometheus_client.Gauge('total_vae_loss', 'Total VAE loss (Reconstruction + KL)')\n",
        "except ValueError as e:\n",
        "    print(f\"Metrics already registered: {e}. Skipping metric definition.\")\n",
        "\n",
        "\n",
        "# Define the Quantum Device and QNode globally\n",
        "latent_dim_quantum = 4\n",
        "dev = qml.device(\"default.qubit\", wires=latent_dim_quantum)\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def quantum_circuit(inputs):\n",
        "    # Simple variational quantum circuit\n",
        "    # inputs should have shape (batch_size, latent_dim_quantum)\n",
        "    for i in range(latent_dim_quantum):\n",
        "        # Apply RY gate to each qubit with the corresponding input from the latent space\n",
        "        qml.RY(inputs[:, i], wires=i)\n",
        "    for i in range(latent_dim_quantum - 1):\n",
        "        qml.CZ(wires=[i, i+1])\n",
        "    # Return a list of expectation values. PennyLane with torch interface\n",
        "    # should handle batching and convert this list of measurement processes to a tensor\n",
        "    # of shape (batch_size, latent_dim_quantum).\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(latent_dim_quantum)]\n",
        "\n",
        "\n",
        "# Define the Hybrid VAE model, and access the global QNode in the forward pass\n",
        "class HybridVAE(nn.Module):\n",
        "    def __init__(self, input_dim=100, latent_dim_classical=32, latent_dim_quantum=4):\n",
        "        super().__init__()\n",
        "        self.latent_dim_classical = latent_dim_classical\n",
        "        self.latent_dim_quantum = latent_dim_quantum # Use the passed or default quantum latent dim\n",
        "\n",
        "        # Classical Encoder\n",
        "        self.encoder_classical = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 2 * (self.latent_dim_classical + self.latent_dim_quantum)) # Output 2x for mu and log_var\n",
        "        )\n",
        "\n",
        "        # Classical Decoder\n",
        "        self.decoder_classical = nn.Sequential(\n",
        "            nn.Linear(self.latent_dim_classical + self.latent_dim_quantum, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, input_dim)\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Classical Encoder forward pass\n",
        "        start_encode_time = time.time()\n",
        "        encoded = self.encoder_classical(x)\n",
        "        end_encode_time = time.time()\n",
        "        ENCODER_PROCESSING_TIME.observe(end_encode_time - start_encode_time)\n",
        "\n",
        "        # Reshape to separate mu and log_var for the combined latent space\n",
        "        mu_log_var = encoded.view(-1, 2, self.latent_dim_classical + self.latent_dim_quantum)\n",
        "        mu = mu_log_var[:, 0, :]\n",
        "        log_var = mu_log_var[:, 1, :]\n",
        "\n",
        "        # Log latent space parameter means\n",
        "        LATENT_MU.labels(latent_dim_type='classical').set(mu[:, :self.latent_dim_classical].mean().item())\n",
        "        LATENT_LOG_VAR.labels(latent_dim_type='classical').set(log_var[:, :self.latent_dim_classical].mean().item())\n",
        "        LATENT_MU.labels(latent_dim_type='quantum').set(mu[:, self.latent_dim_classical:].mean().item())\n",
        "        LATENT_LOG_VAR.labels(latent_dim_type='quantum').set(log_var[:, self.latent_dim_classical:].mean().item())\n",
        "\n",
        "\n",
        "        # Split latent space for classical and quantum parts\n",
        "        mu_classical = mu[:, :self.latent_dim_classical]\n",
        "        log_var_classical = log_var[:, :self.latent_dim_classical]\n",
        "        mu_quantum = mu[:, self.latent_dim_classical:]\n",
        "        log_var_quantum = log_var[:, self.latent_dim_classical:] # Use quantum log_var here\n",
        "\n",
        "        # Reparameterization trick for classical latent space\n",
        "        z_classical = self.reparameterize(mu_classical, log_var_classical)\n",
        "\n",
        "        # Pass quantum latent space through quantum layer\n",
        "        # Apply reparameterization trick for quantum latent space inputs to the quantum circuit\n",
        "        z_quantum_input = self.reparameterize(mu_quantum, log_var_quantum)\n",
        "\n",
        "        start_quantum_time = time.time()\n",
        "        # Pass the entire batch of quantum latent inputs to the globally defined quantum_circuit\n",
        "        z_quantum_output = quantum_circuit(z_quantum_input)\n",
        "        end_quantum_time = time.time()\n",
        "        QUANTUM_CIRCUIT_DURATION.observe(end_quantum_time - start_quantum_time)\n",
        "        QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.inc(z_quantum_input.size(0)) # Increment by batch size\n",
        "\n",
        "        # If the output is a list of tensors, stack it. If it's already a single tensor, keep it.\n",
        "        if isinstance(z_quantum_output, list):\n",
        "             z_quantum_output = torch.stack(z_quantum_output, dim=1)\n",
        "\n",
        "        # Ensure data types match before concatenation\n",
        "        z_quantum_output = z_quantum_output.float()\n",
        "\n",
        "        # Log quantum expectation value statistics\n",
        "        QUANTUM_EXPVAL_MEAN.set(z_quantum_output.mean().item())\n",
        "        QUANTUM_EXPVAL_STD.set(z_quantum_output.std().item())\n",
        "\n",
        "\n",
        "        # Concatenate classical and quantum latent representations\n",
        "        z = torch.cat((z_classical, z_quantum_output), dim=1)\n",
        "\n",
        "        # Classical Decoder forward pass\n",
        "        start_decode_time = time.time()\n",
        "        reconstruction = self.decoder_classical(z)\n",
        "        end_decode_time = time.time()\n",
        "        DECODER_PROCESSING_TIME.observe(end_decode_time - start_decode_time)\n",
        "\n",
        "\n",
        "        return reconstruction, mu, log_var\n",
        "\n",
        "\n",
        "# Define the loss function (VAE loss: Reconstruction + KL Divergence)\n",
        "def vae_loss(recon_x, x, mu, log_var):\n",
        "    BCE = F.mse_loss(recon_x, x, reduction='sum') # Using MSE for reconstruction loss\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "\n",
        "    # Log loss components\n",
        "    RECONSTRUCTION_ERROR_MSE.set(BCE.item() / x.size(0)) # Log mean MSE\n",
        "    KL_DIVERGENCE_LOSS.set(KLD.item())\n",
        "    TOTAL_VAE_LOSS.set((BCE + KLD).item())\n",
        "\n",
        "    return BCE + KLD\n",
        "\n",
        "# Example usage in a training loop (adapted from previous cells)\n",
        "# Assuming X_train_tensor and X_test_tensor are already defined\n",
        "\n",
        "# Start an MLflow run\n",
        "with mlflow.start_run(run_name=\"Hybrid VAE Training with Metrics\"):\n",
        "    # Log hyperparameters\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 10\n",
        "    classical_latent_dim = 32\n",
        "    # quantum_latent_dim is now determined by the QNode passed to the model\n",
        "\n",
        "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
        "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
        "    mlflow.log_param(\"classical_latent_dim\", classical_latent_dim)\n",
        "    mlflow.log_param(\"quantum_latent_dim\", latent_dim_quantum) # Log the actual quantum dim\n",
        "\n",
        "\n",
        "    # Instantiate the model, the QNode is accessed globally in forward\n",
        "    model = HybridVAE(input_dim=X_train_tensor.shape[1],\n",
        "                      latent_dim_classical=classical_latent_dim,\n",
        "                      latent_dim_quantum=latent_dim_quantum)\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    print(\"Starting Hybrid VAE training with Prometheus metrics...\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_epoch_time = time.time()\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Process the entire training tensor as a single batch\n",
        "        recon, mu, log_var = model(X_train_tensor)\n",
        "        loss = vae_loss(recon, X_train_tensor, mu, log_var)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        end_epoch_time = time.time()\n",
        "        epoch_duration = end_epoch_time - start_epoch_time\n",
        "        TRAINING_TIME_PER_EPOCH.observe(epoch_duration)\n",
        "\n",
        "\n",
        "        avg_train_loss = train_loss / X_train_tensor.size(0)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Log training loss for the epoch to MLflow\n",
        "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "        # Log Prometheus metrics related to the epoch (loss, etc.)\n",
        "        # Note: These logs to MLflow are for demonstration. In a real system,\n",
        "        # Prometheus would scrape a metrics endpoint.\n",
        "        try:\n",
        "            mlflow.log_metric(\"total_vae_loss_epoch\", TOTAL_VAE_LOSS.collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"reconstruction_error_mse_epoch\", RECONSTRUCTION_ERROR_MSE.collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"kl_divergence_loss_epoch\", KL_DIVERGENCE_LOSS.collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"training_time_per_epoch_s\", epoch_duration, step=epoch)\n",
        "            mlflow.log_metric(\"quantum_circuit_evaluations_total\", QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.collect()[0].samples[0].value, step=epoch)\n",
        "             # Log latent space and quantum metrics to MLflow\n",
        "            mlflow.log_metric(\"latent_mu_classical_mean\", LATENT_MU.labels(latent_dim_type='classical').collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"latent_log_var_classical_mean\", LATENT_LOG_VAR.labels(latent_dim_type='classical').collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"latent_mu_quantum_mean\", LATENT_MU.labels(latent_dim_type='quantum').collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"latent_log_var_quantum_mean\", LATENT_LOG_VAR.labels(latent_dim_type='quantum').collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"quantum_expval_mean\", QUANTUM_EXPVAL_MEAN.collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"quantum_expval_std\", QUANTUM_EXPVAL_STD.collect()[0].samples[0].value, step=epoch)\n",
        "            # Note: Summary metrics (like duration) from Prometheus client are Histograms.\n",
        "            # Logging their current observation count/sum might be less informative for trend over epoch.\n",
        "            # Logging average duration per epoch might be more suitable here, but requires calculation.\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"Error logging Prometheus metrics to MLflow: {e}\")\n",
        "\n",
        "\n",
        "    print(\"Hybrid VAE training finished.\")\n",
        "\n",
        "    # Log the trained model\n",
        "    mlflow.pytorch.log_model(model, \"hybrid_vae_model\")\n",
        "\n",
        "# The MLflow run automatically ends when exiting the 'with' block\n",
        "\n",
        "# Example usage in an inference scenario\n",
        "print(\"\\nStarting Hybrid VAE inference with Prometheus metrics...\")\n",
        "\n",
        "# Start a new MLflow run for inference benchmarking/logging\n",
        "with mlflow.start_run(run_name=\"Hybrid VAE Inference with Metrics\"):\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation for inference\n",
        "        start_total_inference_time = time.time()\n",
        "\n",
        "        # Encode\n",
        "        start_encode_time = time.time()\n",
        "        encoded_test_data = model.encoder_classical(X_test_tensor)\n",
        "        end_encode_time = time.time()\n",
        "        ENCODER_PROCESSING_TIME.observe(end_encode_time - start_encode_time)\n",
        "\n",
        "        mu_log_var_test = encoded_test_data.view(-1, 2, model.latent_dim_classical + model.latent_dim_quantum)\n",
        "        mu_test = mu_log_var_test[:, 0, :]\n",
        "        log_var_test = mu_log_var_test[:, 1, :]\n",
        "\n",
        "        # Log latent space parameter means for inference\n",
        "        LATENT_MU.labels(latent_dim_type='classical').set(mu_test[:, :model.latent_dim_classical].mean().item())\n",
        "        LATENT_LOG_VAR.labels(latent_dim_type='classical').set(log_var_test[:, :model.latent_dim_classical].mean().item())\n",
        "        LATENT_MU.labels(latent_dim_type='quantum').set(mu_test[:, model.latent_dim_classical:].mean().item())\n",
        "        LATENT_LOG_VAR.labels(latent_dim_type='quantum').set(log_var_test[:, model.latent_dim_classical:].mean().item())\n",
        "\n",
        "\n",
        "        mu_classical_test = mu_test[:, :model.latent_dim_classical]\n",
        "        log_var_classical_test = log_var_test[:, :model.latent_dim_classical]\n",
        "        mu_quantum_test = mu_test[:, model.latent_dim_classical:]\n",
        "        log_var_quantum_test = log_var_test[:, model.latent_dim_classical:]\n",
        "\n",
        "        z_classical_test = model.reparameterize(mu_classical_test, log_var_classical_test)\n",
        "        z_quantum_input_test = model.reparameterize(mu_quantum_test, log_var_quantum_test)\n",
        "\n",
        "        # Quantum Layer\n",
        "        start_quantum_time = time.time()\n",
        "        # Access the globally defined quantum_circuit\n",
        "        z_quantum_output_test = quantum_circuit(z_quantum_input_test)\n",
        "        end_quantum_time = time.time()\n",
        "        QUANTUM_CIRCUIT_DURATION.observe(end_quantum_time - start_quantum_time)\n",
        "        QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.inc(z_quantum_input_test.size(0)) # Increment by batch size\n",
        "\n",
        "        if isinstance(z_quantum_output_test, list):\n",
        "             z_quantum_output_test = torch.stack(z_quantum_output_test, dim=1)\n",
        "\n",
        "        z_quantum_output_test = z_quantum_output_test.float()\n",
        "\n",
        "        # Log quantum expectation value statistics for inference\n",
        "        QUANTUM_EXPVAL_MEAN.set(z_quantum_output_test.mean().item())\n",
        "        QUANTUM_EXPVAL_STD.set(z_quantum_output_test.std().item())\n",
        "\n",
        "\n",
        "        z_test = torch.cat((z_classical_test, z_quantum_output_test), dim=1)\n",
        "\n",
        "        # Decode\n",
        "        start_decode_time = time.time()\n",
        "        reconstructed_test_data = model.decoder_classical(z_test)\n",
        "        end_decode_time = time.time()\n",
        "        DECODER_PROCESSING_TIME.observe(end_decode_time - start_decode_time)\n",
        "\n",
        "        end_total_inference_time = time.time()\n",
        "        total_inference_duration = end_total_inference_time - start_total_inference_time\n",
        "        TOTAL_INFERENCE_TIME.observe(total_inference_duration)\n",
        "\n",
        "\n",
        "        # Calculate and log inference reconstruction error\n",
        "        inference_reconstruction_error = F.mse_loss(reconstructed_test_data, X_test_tensor, reduction='mean').item()\n",
        "        RECONSTRUCTION_ERROR_MSE.set(inference_reconstruction_error) # Set for inference\n",
        "\n",
        "        # Log inference-specific metrics to MLflow\n",
        "        mlflow.log_metric(\"total_inference_time_s\", total_inference_duration)\n",
        "        mlflow.log_metric(\"inference_reconstruction_error_mse\", inference_reconstruction_error)\n",
        "        # Log component durations (these are Summaries, so logging their sum/count or calculating avg here)\n",
        "        # For simplicity, logging the *current* observation from the Summary which is less meaningful for a single run,\n",
        "        # but demonstrates logging. A real setup would scrape Prometheus.\n",
        "        try:\n",
        "            mlflow.log_metric(\"encoder_processing_time_s_sum\", ENCODER_PROCESSING_TIME.collect()[0].samples[1].value)\n",
        "            mlflow.log_metric(\"quantum_circuit_duration_s_sum\", QUANTUM_CIRCUIT_DURATION.collect()[0].samples[1].value)\n",
        "            mlflow.log_metric(\"decoder_processing_time_s_sum\", DECODER_PROCESSING_TIME.collect()[0].samples[1].value)\n",
        "            mlflow.log_metric(\"inference_quantum_circuit_evaluations\", QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_latent_mu_classical_mean\", LATENT_MU.labels(latent_dim_type='classical').collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_latent_log_var_classical_mean\", LATENT_LOG_VAR.labels(latent_dim_type='classical').collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_latent_mu_quantum_mean\", LATENT_MU.labels(latent_dim_type='quantum').collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_latent_log_var_quantum_mean\", LATENT_LOG_VAR.labels(latent_dim_type='quantum').collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_quantum_expval_mean\", QUANTUM_EXPVAL_MEAN.collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_quantum_expval_std\", QUANTUM_EXPVAL_STD.collect()[0].samples[0].value)\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"Error logging Prometheus metrics to MLflow during inference: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nPrometheus metrics integrated into training and inference.\")\n",
        "print(\"Metrics can be accessed via `prometheus_client.REGISTRY.collect()` (e.g., for exposition).\")\n",
        "\n",
        "# Illustrative: Print some collected metrics\n",
        "print(\"\\nIllustrative collected metrics:\")\n",
        "for metric in prometheus_client.REGISTRY.collect():\n",
        "    print(f\"Metric: {metric.name}\")\n",
        "    for sample in metric.samples:\n",
        "        print(f\"  Sample: {sample.name}, Labels: {sample.labels}, Value: {sample.value}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics already registered: Duplicated timeseries in CollectorRegistry: {'encoder_processing_time_seconds_sum', 'encoder_processing_time_seconds_created', 'encoder_processing_time_seconds', 'encoder_processing_time_seconds_count'}. Skipping metric definition.\n",
            "Starting Hybrid VAE training with Prometheus metrics...\n",
            "Epoch [1/10], Loss: 1558.4143\n",
            "Epoch [2/10], Loss: 1550.4979\n",
            "Epoch [3/10], Loss: 1543.9958\n",
            "Epoch [4/10], Loss: 1538.2204\n",
            "Epoch [5/10], Loss: 1532.8018\n",
            "Epoch [6/10], Loss: 1527.7726\n",
            "Epoch [7/10], Loss: 1522.8945\n",
            "Epoch [8/10], Loss: 1518.6598\n",
            "Epoch [9/10], Loss: 1514.3839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/04 18:25:15 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Loss: 1510.9395\n",
            "Hybrid VAE training finished.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "cannot pickle '_thread.lock' object",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1242733790.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;31m# Log the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m     \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hybrid_vae_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;31m# The MLflow run automatically ends when exiting the 'with' block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/pytorch/__init__.py\u001b[0m in \u001b[0;36mlog_model\u001b[0;34m(pytorch_model, artifact_path, conda_env, code_paths, pickle_module, registered_model_name, signature, input_example, await_registration_for, extra_files, pip_requirements, extra_pip_requirements, metadata, name, params, tags, model_type, step, model_id, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \"\"\"\n\u001b[1;32m    287\u001b[0m     \u001b[0mpickle_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmlflow_pytorch_pickle_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m     return Model.log(\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0martifact_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0martifact_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/models/model.py\u001b[0m in \u001b[0;36mlog\u001b[0;34m(cls, artifact_path, flavor, registered_model_name, await_registration_for, metadata, run_id, resources, auth_policy, prompts, name, model_type, params, tags, step, model_id, **kwargs)\u001b[0m\n\u001b[1;32m   1205\u001b[0m                     \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 )\n\u001b[0;32m-> 1207\u001b[0;31m                 \u001b[0mflavor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlflow_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlflow_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m                 \u001b[0;31m# `save_model` calls `load_model` to infer the model requirements, which may result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m                 \u001b[0;31m# in __pycache__ directories being created in the model directory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/pytorch/__init__.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(pytorch_model, path, conda_env, mlflow_model, code_paths, pickle_module, signature, input_example, extra_files, pip_requirements, extra_pip_requirements, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScriptModule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytorch_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpytorch_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0mtorchserve_artifacts_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    945\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m                 \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0mpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyTorchPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m     \u001b[0mdata_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m     \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/cloudpickle/cloudpickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m   1301\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"recursion\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot pickle '_thread.lock' object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Oh4JoI0VY7i",
        "outputId": "0d7f6f87-621d-4de7-a049-2e1b0305d389"
      },
      "source": [
        "import prometheus_client\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pennylane as qml\n",
        "import mlflow\n",
        "\n",
        "\n",
        "'''\n",
        "Error fix patch: The TypeError: cannot pickle '_thread.lock' object persists during mlflow.pytorch.log_model.\n",
        "\n",
        "'''\n",
        "\n",
        "# Define Prometheus metrics (Illustrative)\n",
        "# Removing the explicit registry reset attempt to avoid AttributeError\n",
        "\n",
        "# Classical Encoder Metrics\n",
        "try:\n",
        "    ENCODER_PROCESSING_TIME = prometheus_client.Summary('encoder_processing_time_seconds', 'Time taken for classical encoder processing')\n",
        "    LATENT_MU = prometheus_client.Gauge('latent_mu', 'Mean of the latent space mu values', ['latent_dim_type'])\n",
        "    LATENT_LOG_VAR = prometheus_client.Gauge('latent_log_var', 'Mean of the latent space log_var values', ['latent_dim_type'])\n",
        "\n",
        "\n",
        "    # Quantum Layer Metrics\n",
        "    QUANTUM_CIRCUIT_DURATION = prometheus_client.Summary('quantum_circuit_duration_seconds', 'Duration of quantum circuit execution')\n",
        "    QUANTUM_EXPVAL_MEAN = prometheus_client.Gauge('quantum_expval_mean', 'Mean of quantum expectation values')\n",
        "    QUANTUM_EXPVAL_STD = prometheus_client.Gauge('quantum_expval_std', 'Standard deviation of quantum expectation values')\n",
        "    QUANTUM_CIRCUIT_EVALUATIONS_TOTAL = prometheus_client.Counter('quantum_circuit_evaluations_total', 'Total number of quantum circuit evaluations')\n",
        "\n",
        "\n",
        "    # Classical Decoder Metrics\n",
        "    DECODER_PROCESSING_TIME = prometheus_client.Summary('decoder_processing_time_seconds', 'Time taken for classical decoder processing')\n",
        "\n",
        "    # Overall VAE Metrics\n",
        "    TOTAL_INFERENCE_TIME = prometheus_client.Summary('total_inference_time_seconds', 'Total time taken for inference requests')\n",
        "    TRAINING_TIME_PER_EPOCH = prometheus_client.Summary('training_time_per_epoch_seconds', 'Total time taken for one training epoch')\n",
        "    RECONSTRUCTION_ERROR_MSE = prometheus_client.Gauge('reconstruction_error_mse', 'Mean Squared Error of reconstruction')\n",
        "    KL_DIVERGENCE_LOSS = prometheus_client.Gauge('kl_divergence_loss', 'Kullback-Leibler Divergence loss')\n",
        "    TOTAL_VAE_LOSS = prometheus_client.Gauge('total_vae_loss', 'Total VAE loss (Reconstruction + KL)')\n",
        "except ValueError as e:\n",
        "    print(f\"Metrics already registered: {e}. Skipping metric definition.\")\n",
        "\n",
        "\n",
        "# Define the Quantum Device and QNode globally\n",
        "latent_dim_quantum = 4\n",
        "dev = qml.device(\"default.qubit\", wires=latent_dim_quantum)\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def quantum_circuit(inputs):\n",
        "    # Simple variational quantum circuit\n",
        "    # inputs should have shape (batch_size, latent_dim_quantum)\n",
        "    for i in range(latent_dim_quantum):\n",
        "        # Apply RY gate to each qubit with the corresponding input from the latent space\n",
        "        qml.RY(inputs[:, i], wires=i)\n",
        "    for i in range(latent_dim_quantum - 1):\n",
        "        qml.CZ(wires=[i, i+1])\n",
        "    # Return a list of expectation values. PennyLane with torch interface\n",
        "    # should handle batching and convert this list of measurement processes to a tensor\n",
        "    # of shape (batch_size, latent_dim_quantum).\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(latent_dim_quantum)]\n",
        "\n",
        "\n",
        "# Define the Hybrid VAE model, and access the global QNode in the forward pass\n",
        "class HybridVAE(nn.Module):\n",
        "    def __init__(self, input_dim=100, latent_dim_classical=32, latent_dim_quantum=4):\n",
        "        super().__init__()\n",
        "        self.latent_dim_classical = latent_dim_classical\n",
        "        self.latent_dim_quantum = latent_dim_quantum # Use the passed or default quantum latent dim\n",
        "\n",
        "        # Classical Encoder\n",
        "        self.encoder_classical = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 2 * (self.latent_dim_classical + self.latent_dim_quantum)) # Output 2x for mu and log_var\n",
        "        )\n",
        "\n",
        "        # Classical Decoder\n",
        "        self.decoder_classical = nn.Sequential(\n",
        "            nn.Linear(self.latent_dim_classical + self.latent_dim_quantum, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, input_dim)\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Classical Encoder forward pass\n",
        "        start_encode_time = time.time()\n",
        "        encoded = self.encoder_classical(x)\n",
        "        end_encode_time = time.time()\n",
        "        ENCODER_PROCESSING_TIME.observe(end_encode_time - start_encode_time)\n",
        "\n",
        "        # Reshape to separate mu and log_var for the combined latent space\n",
        "        mu_log_var = encoded.view(-1, 2, self.latent_dim_classical + self.latent_dim_quantum)\n",
        "        mu = mu_log_var[:, 0, :]\n",
        "        log_var = mu_log_var[:, 1, :]\n",
        "\n",
        "        # Log latent space parameter means\n",
        "        LATENT_MU.labels(latent_dim_type='classical').set(mu[:, :self.latent_dim_classical].mean().item())\n",
        "        LATENT_LOG_VAR.labels(latent_dim_type='classical').set(log_var[:, :self.latent_dim_classical].mean().item())\n",
        "        LATENT_MU.labels(latent_dim_type='quantum').set(mu[:, self.latent_dim_classical:].mean().item())\n",
        "        LATENT_LOG_VAR.labels(latent_dim_type='quantum').set(log_var[:, self.latent_dim_classical:].mean().item())\n",
        "\n",
        "\n",
        "        # Split latent space for classical and quantum parts\n",
        "        mu_classical = mu[:, :self.latent_dim_classical]\n",
        "        log_var_classical = log_var[:, :self.latent_dim_classical]\n",
        "        mu_quantum = mu[:, self.latent_dim_classical:]\n",
        "        log_var_quantum = log_var[:, self.latent_dim_classical:] # Use quantum log_var here\n",
        "\n",
        "        # Reparameterization trick for classical latent space\n",
        "        z_classical = self.reparameterize(mu_classical, log_var_classical)\n",
        "\n",
        "        # Pass quantum latent space through quantum layer\n",
        "        # Apply reparameterization trick for quantum latent space inputs to the quantum circuit\n",
        "        z_quantum_input = self.reparameterize(mu_quantum, log_var_quantum)\n",
        "\n",
        "        start_quantum_time = time.time()\n",
        "        # Pass the entire batch of quantum latent inputs to the globally defined quantum_circuit\n",
        "        z_quantum_output = quantum_circuit(z_quantum_input)\n",
        "        end_quantum_time = time.time()\n",
        "        QUANTUM_CIRCUIT_DURATION.observe(end_quantum_time - start_quantum_time)\n",
        "        QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.inc(z_quantum_input.size(0)) # Increment by batch size\n",
        "\n",
        "        # If the output is a list of tensors, stack it. If it's already a single tensor, keep it.\n",
        "        if isinstance(z_quantum_output, list):\n",
        "             z_quantum_output = torch.stack(z_quantum_output, dim=1)\n",
        "\n",
        "        # Ensure data types match before concatenation\n",
        "        z_quantum_output = z_quantum_output.float()\n",
        "\n",
        "        # Log quantum expectation value statistics\n",
        "        QUANTUM_EXPVAL_MEAN.set(z_quantum_output.mean().item())\n",
        "        QUANTUM_EXPVAL_STD.set(z_quantum_output.std().item())\n",
        "\n",
        "\n",
        "        # Concatenate classical and quantum latent representations\n",
        "        z = torch.cat((z_classical, z_quantum_output), dim=1)\n",
        "\n",
        "        # Classical Decoder forward pass\n",
        "        start_decode_time = time.time()\n",
        "        reconstruction = self.decoder_classical(z)\n",
        "        end_decode_time = time.time()\n",
        "        DECODER_PROCESSING_TIME.observe(end_decode_time - start_decode_time)\n",
        "\n",
        "\n",
        "        return reconstruction, mu, log_var\n",
        "\n",
        "\n",
        "# Define the loss function (VAE loss: Reconstruction + KL Divergence)\n",
        "def vae_loss(recon_x, x, mu, log_var):\n",
        "    BCE = F.mse_loss(recon_x, x, reduction='sum') # Using MSE for reconstruction loss\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "\n",
        "    # Log loss components\n",
        "    RECONSTRUCTION_ERROR_MSE.set(BCE.item() / x.size(0)) # Log mean MSE\n",
        "    KL_DIVERGENCE_LOSS.set(KLD.item())\n",
        "    TOTAL_VAE_LOSS.set((BCE + KLD).item())\n",
        "\n",
        "    return BCE + KLD\n",
        "\n",
        "# Example usage in a training loop (adapted from previous cells)\n",
        "# Assuming X_train_tensor and X_test_tensor are already defined\n",
        "\n",
        "# Start an MLflow run\n",
        "with mlflow.start_run(run_name=\"Hybrid VAE Training with Metrics\"):\n",
        "    # Log hyperparameters\n",
        "    learning_rate = 0.001\n",
        "    num_epochs = 10\n",
        "    classical_latent_dim = 32\n",
        "    # quantum_latent_dim is now determined by the QNode passed to the model\n",
        "\n",
        "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
        "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
        "    mlflow.log_param(\"classical_latent_dim\", classical_latent_dim)\n",
        "    mlflow.log_param(\"quantum_latent_dim\", latent_dim_quantum) # Log the actual quantum dim\n",
        "\n",
        "\n",
        "    # Instantiate the model, the QNode is accessed globally in forward\n",
        "    model = HybridVAE(input_dim=X_train_tensor.shape[1],\n",
        "                      latent_dim_classical=classical_latent_dim,\n",
        "                      latent_dim_quantum=latent_dim_quantum)\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    print(\"Starting Hybrid VAE training with Prometheus metrics...\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_epoch_time = time.time()\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Process the entire training tensor as a single batch\n",
        "        recon, mu, log_var = model(X_train_tensor)\n",
        "        loss = vae_loss(recon, X_train_tensor, mu, log_var)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        end_epoch_time = time.time()\n",
        "        epoch_duration = end_epoch_time - start_epoch_time\n",
        "        TRAINING_TIME_PER_EPOCH.observe(epoch_duration)\n",
        "\n",
        "\n",
        "        avg_train_loss = train_loss / X_train_tensor.size(0)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Log training loss for the epoch to MLflow\n",
        "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
        "        # Log Prometheus metrics related to the epoch (loss, etc.)\n",
        "        # Note: These logs to MLflow are for demonstration. In a real system,\n",
        "        # Prometheus would scrape a metrics endpoint.\n",
        "        try:\n",
        "            mlflow.log_metric(\"total_vae_loss_epoch\", TOTAL_VAE_LOSS.collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"reconstruction_error_mse_epoch\", RECONSTRUCTION_ERROR_MSE.collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"kl_divergence_loss_epoch\", KL_DIVERGENCE_LOSS.collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"training_time_per_epoch_s\", epoch_duration, step=epoch)\n",
        "            mlflow.log_metric(\"quantum_circuit_evaluations_total\", QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.collect()[0].samples[0].value, step=epoch)\n",
        "             # Log latent space and quantum metrics to MLflow\n",
        "            mlflow.log_metric(\"latent_mu_classical_mean\", LATENT_MU.labels(latent_dim_type='classical').collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"latent_log_var_classical_mean\", LATENT_LOG_VAR.labels(latent_dim_type='classical').collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"latent_mu_quantum_mean\", LATENT_MU.labels(latent_dim_type='quantum').collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"latent_log_var_quantum_mean\", LATENT_LOG_VAR.labels(latent_dim_type='quantum').collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"quantum_expval_mean\", QUANTUM_EXPVAL_MEAN.collect()[0].samples[0].value, step=epoch)\n",
        "            mlflow.log_metric(\"quantum_expval_std\", QUANTUM_EXPVAL_STD.collect()[0].samples[0].value, step=epoch)\n",
        "            # Note: Summary metrics (like duration) from Prometheus client are Histograms.\n",
        "            # Logging their current observation count/sum might be less informative for trend over epoch.\n",
        "            # Logging average duration per epoch might be more suitable here, but requires calculation.\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"Error logging Prometheus metrics to MLflow: {e}\")\n",
        "\n",
        "\n",
        "    print(\"Hybrid VAE training finished.\")\n",
        "\n",
        "    # Log the trained model - Attempting to catch the pickling error\n",
        "    try:\n",
        "        mlflow.pytorch.log_model(model, \"hybrid_vae_model\")\n",
        "    except TypeError as e:\n",
        "        if \"cannot pickle '_thread.lock' object\" in str(e):\n",
        "            print(\"\\nSkipping mlflow.pytorch.log_model due to pickling error with PennyLane/PyTorch interaction.\")\n",
        "            print(\"The model cannot be pickled and logged as a single PyTorch artifact in this environment.\")\n",
        "            print(\"Consider logging classical parts separately or using a different serialization method.\")\n",
        "        else:\n",
        "            raise # Re-raise other TypeErrors\n",
        "\n",
        "\n",
        "# The MLflow run automatically ends when exiting the 'with' block\n",
        "\n",
        "# Example usage in an inference scenario\n",
        "print(\"\\nStarting Hybrid VAE inference with Prometheus metrics...\")\n",
        "\n",
        "# Start a new MLflow run for inference benchmarking/logging\n",
        "with mlflow.start_run(run_name=\"Hybrid VAE Inference with Metrics\"):\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation for inference\n",
        "        start_total_inference_time = time.time()\n",
        "\n",
        "        # Encode\n",
        "        start_encode_time = time.time()\n",
        "        encoded_test_data = model.encoder_classical(X_test_tensor)\n",
        "        end_encode_time = time.time()\n",
        "        ENCODER_PROCESSING_TIME.observe(end_encode_time - start_encode_time)\n",
        "\n",
        "        mu_log_var_test = encoded_test_data.view(-1, 2, model.latent_dim_classical + model.latent_dim_quantum)\n",
        "        mu_test = mu_log_var_test[:, 0, :]\n",
        "        log_var_test = mu_log_var_test[:, 1, :]\n",
        "\n",
        "        # Log latent space parameter means for inference\n",
        "        LATENT_MU.labels(latent_dim_type='classical').set(mu_test[:, :model.latent_dim_classical].mean().item())\n",
        "        LATENT_LOG_VAR.labels(latent_dim_type='classical').set(log_var_test[:, :model.latent_dim_classical].mean().item())\n",
        "        LATENT_MU.labels(latent_dim_type='quantum').set(mu_test[:, model.latent_dim_classical:].mean().item())\n",
        "        LATENT_LOG_VAR.labels(latent_dim_type='quantum').set(log_var_test[:, model.latent_dim_classical:].mean().item())\n",
        "\n",
        "\n",
        "        mu_classical_test = mu_test[:, :model.latent_dim_classical]\n",
        "        log_var_classical_test = log_var_test[:, :model.latent_dim_classical]\n",
        "        mu_quantum_test = mu_test[:, model.latent_dim_classical:]\n",
        "        log_var_quantum_test = log_var_test[:, model.latent_dim_classical:]\n",
        "\n",
        "        z_classical_test = model.reparameterize(mu_classical_test, log_var_classical_test)\n",
        "        z_quantum_input_test = model.reparameterize(mu_quantum_test, log_var_quantum_test)\n",
        "\n",
        "        # Quantum Layer\n",
        "        start_quantum_time = time.time()\n",
        "        # Access the globally defined quantum_circuit\n",
        "        z_quantum_output_test = quantum_circuit(z_quantum_input_test)\n",
        "        end_quantum_time = time.time()\n",
        "        QUANTUM_CIRCUIT_DURATION.observe(end_quantum_time - start_quantum_time)\n",
        "        QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.inc(z_quantum_input_test.size(0)) # Increment by batch size\n",
        "\n",
        "        if isinstance(z_quantum_output_test, list):\n",
        "             z_quantum_output_test = torch.stack(z_quantum_output_test, dim=1)\n",
        "\n",
        "        z_quantum_output_test = z_quantum_output_test.float()\n",
        "\n",
        "        # Log quantum expectation value statistics for inference\n",
        "        QUANTUM_EXPVAL_MEAN.set(z_quantum_output_test.mean().item())\n",
        "        QUANTUM_EXPVAL_STD.set(z_quantum_output_test.std().item())\n",
        "\n",
        "\n",
        "        z_test = torch.cat((z_classical_test, z_quantum_output_test), dim=1)\n",
        "\n",
        "        # Decode\n",
        "        start_decode_time = time.time()\n",
        "        reconstructed_test_data = model.decoder_classical(z_test)\n",
        "        end_decode_time = time.time()\n",
        "        DECODER_PROCESSING_TIME.observe(end_decode_time - start_decode_time)\n",
        "\n",
        "        end_total_inference_time = time.time()\n",
        "        total_inference_duration = end_total_inference_time - start_total_inference_time\n",
        "        TOTAL_INFERENCE_TIME.observe(total_inference_duration)\n",
        "\n",
        "\n",
        "        # Calculate and log inference reconstruction error\n",
        "        inference_reconstruction_error = F.mse_loss(reconstructed_test_data, X_test_tensor, reduction='mean').item()\n",
        "        RECONSTRUCTION_ERROR_MSE.set(inference_reconstruction_error) # Set for inference\n",
        "\n",
        "        # Log inference-specific metrics to MLflow\n",
        "        mlflow.log_metric(\"total_inference_time_s\", total_inference_duration)\n",
        "        mlflow.log_metric(\"inference_reconstruction_error_mse\", inference_reconstruction_error)\n",
        "        # Log component durations (these are Summaries, so logging their sum/count or calculating avg here)\n",
        "        # For simplicity, logging the *current* observation from the Summary which is less meaningful for a single run,\n",
        "        # but demonstrates logging. A real setup would scrape Prometheus.\n",
        "        try:\n",
        "            mlflow.log_metric(\"encoder_processing_time_s_sum\", ENCODER_PROCESSING_TIME.collect()[0].samples[1].value)\n",
        "            mlflow.log_metric(\"quantum_circuit_duration_s_sum\", QUANTUM_CIRCUIT_DURATION.collect()[0].samples[1].value)\n",
        "            mlflow.log_metric(\"decoder_processing_time_s_sum\", DECODER_PROCESSING_TIME.collect()[0].samples[1].value)\n",
        "            mlflow.log_metric(\"inference_quantum_circuit_evaluations\", QUANTUM_CIRCUIT_EVALUATIONS_TOTAL.collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_latent_mu_classical_mean\", LATENT_MU.labels(latent_dim_type='classical').collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_latent_log_var_classical_mean\", LATENT_LOG_VAR.labels(latent_dim_type='classical').collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_latent_mu_quantum_mean\", LATENT_MU.labels(latent_dim_type='quantum').collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_latent_log_var_quantum_mean\", LATENT_LOG_VAR.labels(latent_dim_type='quantum').collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_quantum_expval_mean\", QUANTUM_EXPVAL_MEAN.collect()[0].samples[0].value)\n",
        "            mlflow.log_metric(\"inference_quantum_expval_std\", QUANTUM_EXPVAL_STD.collect()[0].samples[0].value)\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"Error logging Prometheus metrics to MLflow during inference: {e}\")\n",
        "\n",
        "\n",
        "print(\"\\nPrometheus metrics integrated into training and inference.\")\n",
        "print(\"Metrics can be accessed via `prometheus_client.REGISTRY.collect()` (e.g., for exposition).\")\n",
        "\n",
        "# Illustrative: Print some collected metrics\n",
        "print(\"\\nIllustrative collected metrics:\")\n",
        "for metric in prometheus_client.REGISTRY.collect():\n",
        "    print(f\"Metric: {metric.name}\")\n",
        "    for sample in metric.samples:\n",
        "        print(f\"  Sample: {sample.name}, Labels: {sample.labels}, Value: {sample.value}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics already registered: Duplicated timeseries in CollectorRegistry: {'encoder_processing_time_seconds_sum', 'encoder_processing_time_seconds_created', 'encoder_processing_time_seconds', 'encoder_processing_time_seconds_count'}. Skipping metric definition.\n",
            "Starting Hybrid VAE training with Prometheus metrics...\n",
            "Epoch [1/10], Loss: 1567.8052\n",
            "Epoch [2/10], Loss: 1559.4803\n",
            "Epoch [3/10], Loss: 1551.3306\n",
            "Epoch [4/10], Loss: 1545.4582\n",
            "Epoch [5/10], Loss: 1539.5165\n",
            "Epoch [6/10], Loss: 1534.2100\n",
            "Epoch [7/10], Loss: 1529.7705\n",
            "Epoch [8/10], Loss: 1525.0996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/08/04 18:26:18 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Loss: 1520.9509\n",
            "Epoch [10/10], Loss: 1517.2171\n",
            "Hybrid VAE training finished.\n",
            "\n",
            "Skipping mlflow.pytorch.log_model due to pickling error with PennyLane/PyTorch interaction.\n",
            "The model cannot be pickled and logged as a single PyTorch artifact in this environment.\n",
            "Consider logging classical parts separately or using a different serialization method.\n",
            "\n",
            "Starting Hybrid VAE inference with Prometheus metrics...\n",
            "\n",
            "Prometheus metrics integrated into training and inference.\n",
            "Metrics can be accessed via `prometheus_client.REGISTRY.collect()` (e.g., for exposition).\n",
            "\n",
            "Illustrative collected metrics:\n",
            "Metric: python_gc_objects_collected\n",
            "  Sample: python_gc_objects_collected_total, Labels: {'generation': '0'}, Value: 46851\n",
            "  Sample: python_gc_objects_collected_total, Labels: {'generation': '1'}, Value: 8759\n",
            "  Sample: python_gc_objects_collected_total, Labels: {'generation': '2'}, Value: 91981\n",
            "Metric: python_gc_objects_uncollectable\n",
            "  Sample: python_gc_objects_uncollectable_total, Labels: {'generation': '0'}, Value: 0\n",
            "  Sample: python_gc_objects_uncollectable_total, Labels: {'generation': '1'}, Value: 0\n",
            "  Sample: python_gc_objects_uncollectable_total, Labels: {'generation': '2'}, Value: 0\n",
            "Metric: python_gc_collections\n",
            "  Sample: python_gc_collections_total, Labels: {'generation': '0'}, Value: 3836\n",
            "  Sample: python_gc_collections_total, Labels: {'generation': '1'}, Value: 348\n",
            "  Sample: python_gc_collections_total, Labels: {'generation': '2'}, Value: 16\n",
            "Metric: python_info\n",
            "  Sample: python_info, Labels: {'version': '3.11.13', 'implementation': 'CPython', 'major': '3', 'minor': '11', 'patchlevel': '13'}, Value: 1\n",
            "Metric: process_virtual_memory_bytes\n",
            "  Sample: process_virtual_memory_bytes, Labels: {}, Value: 12165001216.0\n",
            "Metric: process_resident_memory_bytes\n",
            "  Sample: process_resident_memory_bytes, Labels: {}, Value: 2479472640.0\n",
            "Metric: process_start_time_seconds\n",
            "  Sample: process_start_time_seconds, Labels: {}, Value: 1754328003.45\n",
            "Metric: process_cpu_seconds\n",
            "  Sample: process_cpu_seconds_total, Labels: {}, Value: 142.74\n",
            "Metric: process_open_fds\n",
            "  Sample: process_open_fds, Labels: {}, Value: 90\n",
            "Metric: process_max_fds\n",
            "  Sample: process_max_fds, Labels: {}, Value: 1048576.0\n",
            "Metric: encoder_processing_time_seconds\n",
            "  Sample: encoder_processing_time_seconds_count, Labels: {}, Value: 41.0\n",
            "  Sample: encoder_processing_time_seconds_sum, Labels: {}, Value: 0.11967206001281738\n",
            "  Sample: encoder_processing_time_seconds_created, Labels: {}, Value: 1754331812.5596707\n",
            "Metric: latent_mu\n",
            "  Sample: latent_mu, Labels: {'latent_dim_type': 'classical'}, Value: 0.03714928776025772\n",
            "  Sample: latent_mu, Labels: {'latent_dim_type': 'quantum'}, Value: -0.02514534443616867\n",
            "Metric: latent_log_var\n",
            "  Sample: latent_log_var, Labels: {'latent_dim_type': 'classical'}, Value: -0.2514926791191101\n",
            "  Sample: latent_log_var, Labels: {'latent_dim_type': 'quantum'}, Value: -0.1787012815475464\n",
            "Metric: quantum_circuit_duration_seconds\n",
            "  Sample: quantum_circuit_duration_seconds_count, Labels: {}, Value: 41.0\n",
            "  Sample: quantum_circuit_duration_seconds_sum, Labels: {}, Value: 1.2435452938079834\n",
            "  Sample: quantum_circuit_duration_seconds_created, Labels: {}, Value: 1754331812.5599935\n",
            "Metric: quantum_expval_mean\n",
            "  Sample: quantum_expval_mean, Labels: {}, Value: 0.4902682304382324\n",
            "Metric: quantum_expval_std\n",
            "  Sample: quantum_expval_std, Labels: {}, Value: 0.5532671213150024\n",
            "Metric: quantum_circuit_evaluations\n",
            "  Sample: quantum_circuit_evaluations_total, Labels: {}, Value: 161000.0\n",
            "  Sample: quantum_circuit_evaluations_created, Labels: {}, Value: 1754331812.5602179\n",
            "Metric: decoder_processing_time_seconds\n",
            "  Sample: decoder_processing_time_seconds_count, Labels: {}, Value: 41.0\n",
            "  Sample: decoder_processing_time_seconds_sum, Labels: {}, Value: 0.09960579872131348\n",
            "  Sample: decoder_processing_time_seconds_created, Labels: {}, Value: 1754331812.5602903\n",
            "Metric: total_inference_time_seconds\n",
            "  Sample: total_inference_time_seconds_count, Labels: {}, Value: 1.0\n",
            "  Sample: total_inference_time_seconds_sum, Labels: {}, Value: 0.011347055435180664\n",
            "  Sample: total_inference_time_seconds_created, Labels: {}, Value: 1754331812.5603554\n",
            "Metric: training_time_per_epoch_seconds\n",
            "  Sample: training_time_per_epoch_seconds_count, Labels: {}, Value: 40.0\n",
            "  Sample: training_time_per_epoch_seconds_sum, Labels: {}, Value: 3.3218472003936768\n",
            "  Sample: training_time_per_epoch_seconds_created, Labels: {}, Value: 1754331812.560416\n",
            "Metric: reconstruction_error_mse\n",
            "  Sample: reconstruction_error_mse, Labels: {}, Value: 15.233182907104492\n",
            "Metric: kl_divergence_loss\n",
            "  Sample: kl_divergence_loss, Labels: {}, Value: 62206.625\n",
            "Metric: total_vae_loss\n",
            "  Sample: total_vae_loss, Labels: {}, Value: 6068868.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39e7fa57"
      },
      "source": [
        "## Implement automated checks\n",
        "\n",
        "Define criteria for automated checks based on key metrics (e.g., threshold for reconstruction error, acceptable range for quantum output values).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72aaf398",
        "outputId": "0cca74fc-d292-4e3a-d60a-41f7388d259d"
      },
      "source": [
        "# Define threshold values for automated checks\n",
        "reconstruction_error_threshold = 20.0  # Example threshold for MSE reconstruction error\n",
        "quantum_expval_mean_range = (-0.8, 0.8) # Example acceptable range for mean quantum expectation value\n",
        "quantum_expval_std_threshold = 0.2     # Example threshold for standard deviation of quantum expectation values\n",
        "# Add other thresholds as needed based on observed metric ranges\n",
        "\n",
        "print(\"\\nDefined automated check thresholds:\")\n",
        "print(f\"  Reconstruction Error (MSE) Threshold: < {reconstruction_error_threshold}\")\n",
        "print(f\"  Quantum Expval Mean Range: {quantum_expval_mean_range}\")\n",
        "print(f\"  Quantum Expval Std Threshold: < {quantum_expval_std_threshold}\")\n",
        "\n",
        "\n",
        "# --- Automated Checks within Training Loop (Illustrative) ---\n",
        "# These checks run at the end of each epoch\n",
        "\n",
        "# Assuming the training loop from the previous cell is still active or rerun\n",
        "# (This block should be placed within the training loop, after loss calculation and logging)\n",
        "\n",
        "# Check Reconstruction Error\n",
        "if avg_train_loss > reconstruction_error_threshold:\n",
        "    print(f\"ALERT: Training Reconstruction Error ({avg_train_loss:.4f}) exceeds threshold ({reconstruction_error_threshold}) at Epoch {epoch+1}!\")\n",
        "    # In a real system, this would trigger an alert\n",
        "\n",
        "# Check Quantum Expectation Value Mean and Std (using Prometheus gauge values)\n",
        "try:\n",
        "    current_quantum_expval_mean = QUANTUM_EXPVAL_MEAN.collect()[0].samples[0].value\n",
        "    current_quantum_expval_std = QUANTUM_EXPVAL_STD.collect()[0].samples[0].value\n",
        "\n",
        "    if not (quantum_expval_mean_range[0] <= current_quantum_expval_mean <= quantum_expval_mean_range[1]):\n",
        "         print(f\"ALERT: Training Quantum Expval Mean ({current_quantum_expval_mean:.4f}) is outside expected range ({quantum_expval_mean_range}) at Epoch {epoch+1}!\")\n",
        "         # Trigger alert\n",
        "\n",
        "    if current_quantum_expval_std > quantum_expval_std_threshold:\n",
        "        print(f\"ALERT: Training Quantum Expval Std ({current_quantum_expval_std:.4f}) exceeds threshold ({quantum_expval_std_threshold}) at Epoch {epoch+1}!\")\n",
        "        # Trigger alert\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error collecting or checking quantum metrics in training loop: {e}\")\n",
        "\n",
        "\n",
        "# --- Automated Checks within Inference (Illustrative) ---\n",
        "# These checks run after inference is complete\n",
        "\n",
        "# Assuming the inference code from the previous cell is still active or rerun\n",
        "# (This block should be placed after the inference_reconstruction_error is calculated)\n",
        "\n",
        "# Check Inference Reconstruction Error\n",
        "if inference_reconstruction_error > reconstruction_error_threshold:\n",
        "    print(f\"ALERT: Inference Reconstruction Error ({inference_reconstruction_error:.4f}) exceeds threshold ({reconstruction_error_threshold})!\")\n",
        "    # Trigger alert\n",
        "\n",
        "# Check Quantum Expectation Value Mean and Std during Inference\n",
        "try:\n",
        "    current_inference_quantum_expval_mean = QUANTUM_EXPVAL_MEAN.collect()[0].samples[0].value\n",
        "    current_inference_quantum_expval_std = QUANTUM_EXPVAL_STD.collect()[0].samples[0].value\n",
        "\n",
        "    if not (quantum_expval_mean_range[0] <= current_inference_quantum_expval_mean <= quantum_expval_mean_range[1]):\n",
        "         print(f\"ALERT: Inference Quantum Expval Mean ({current_inference_quantum_expval_mean:.4f}) is outside expected range ({quantum_expval_mean_range})!\")\n",
        "         # Trigger alert\n",
        "\n",
        "    if current_inference_quantum_expval_std > quantum_expval_std_threshold:\n",
        "        print(f\"ALERT: Inference Quantum Expval Std ({current_inference_quantum_expval_std:.4f}) exceeds threshold ({quantum_expval_std_threshold})!\")\n",
        "        # Trigger alert\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error collecting or checking quantum metrics during inference: {e}\")\n",
        "\n",
        "print(\"\\nAutomated checks integrated (illustrative).\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Defined automated check thresholds:\n",
            "  Reconstruction Error (MSE) Threshold: < 20.0\n",
            "  Quantum Expval Mean Range: (-0.8, 0.8)\n",
            "  Quantum Expval Std Threshold: < 0.2\n",
            "ALERT: Training Reconstruction Error (1517.2171) exceeds threshold (20.0) at Epoch 10!\n",
            "ALERT: Training Quantum Expval Std (0.5533) exceeds threshold (0.2) at Epoch 10!\n",
            "ALERT: Inference Quantum Expval Std (0.5533) exceeds threshold (0.2)!\n",
            "\n",
            "Automated checks integrated (illustrative).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e72e3b54"
      },
      "source": [
        "## Discuss alerting mechanisms\n",
        "\n",
        "Explain how alerting could be set up based on failed checks (e.g., integration with alert managers).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92218c29",
        "outputId": "a3c37369-a40b-4fa1-99c6-fbe9615349e9"
      },
      "source": [
        "print(\"### 🚨 Alerting Mechanisms\")\n",
        "print(\"\")\n",
        "print(\"Alerting is the critical next step after implementing automated checks. When a check fails (e.g., inference latency exceeds a threshold, error rate spikes), an alert is triggered to notify the appropriate personnel (e.g., researchers, MLOps engineers) that there is a potential issue requiring investigation and action. For the Hybrid Quantum-Classical VAE, effective alerting ensures the reliability and trustworthiness of the model's outputs for downstream research.\")\n",
        "print(\"\")\n",
        "print(\"#### Purpose of Alerting:\")\n",
        "print(\"\")\n",
        "print(\"*   **Early Problem Detection:** Alerts provide immediate notification of issues, often before they significantly impact end-users or research outcomes.\")\n",
        "print(\"*   **Reduced Downtime:** By quickly identifying problems, the time it takes to diagnose and resolve them is minimized.\")\n",
        "print(\"*   **Proactive Intervention:** Alerts based on saturation or subtle performance degradation can allow for proactive scaling or optimization before critical failures occur.\")\n",
        "print(\"*   **Maintaining Service Level Objectives (SLOs):** Alerting is essential for monitoring adherence to defined SLOs and taking action when they are breached.\")\n",
        "print(\"\")\n",
        "print(\"#### Mechanisms for Setting Up Alerts:\")\n",
        "print(\"\")\n",
        "print(\"Alerts are typically configured within a monitoring system that collects the metrics and evaluates the automated checks. Common mechanisms include:\")\n",
        "print(\"\")\n",
        "print(\"*   **Prometheus Alertmanager:** If using Prometheus for metrics collection, Alertmanager is the standard component for handling alerts. It receives alerts from Prometheus (when a configured alerting rule based on a metric is triggered), groups, silences, and routes them to various notification channels (e.g., email, Slack, PagerDuty, webhooks).\")\n",
        "print(\"    *   **How it receives checks:** Prometheus scrapes the metrics endpoint exposed by the application (or an exporter), and Prometheus servers evaluate alerting rules based on PromQL queries against these metrics. If a rule is met, an alert is sent to Alertmanager.\")\n",
        "print(\"\")\n",
        "print(\"*   **Cloud-Based Alerting Services:** Cloud providers offer integrated monitoring and alerting services:\")\n",
        "print(\"    *   **AWS CloudWatch Alarms:** Can create alarms based on CloudWatch metrics (which can be pushed from your application) or logs. Alarms can trigger actions like sending notifications via SNS, auto-scaling, or initiating EC2 actions.\")\n",
        "print(\"    *   **Google Cloud Monitoring Alerts:** Allows creating alerting policies based on metrics from Google Cloud services or custom metrics. Policies can trigger notifications via various channels.\")\n",
        "print(\"    *   **How they receive checks:** Applications push custom metrics to the cloud monitoring service APIs, or the service processes logs to extract metric information or detect specific patterns.\")\n",
        "print(\"\")\n",
        "print(\"*   **MLflow (Limited Alerting):** While MLflow is primarily for experiment tracking, it has limited alerting capabilities. You can set up simple webhook notifications based on experiment updates, but it's not a full-fledged production monitoring and alerting system like Prometheus or cloud services.\")\n",
        "print(\"    *   **How it receives checks:** MLflow receives metrics and parameters pushed during the run. Any custom alerting logic would need to be built on top of MLflow's API or webhooks.\")\n",
        "print(\"\")\n",
        "print(\"*   **Custom Scripting:** For simpler setups or specific needs, custom scripts can be written to periodically check metrics (e.g., query a database, read a file, call an API) and send notifications (e.g., using email libraries, Slack webhooks) if thresholds are breached.\")\n",
        "print(\"    *   **How it receives checks:** The script directly accesses the source of the metrics (database, file, API endpoint).\")\n",
        "print(\"\")\n",
        "print(\"#### Defining Actionable Alerts:\")\n",
        "print(\"\")\n",
        "print(\"An effective alert is not just a notification; it's **actionable**. Good alerts include:\")\n",
        "print(\"\")\n",
        "print(\"*   **Clear Description:** What is the problem? (e.g., \\\"High VAE Inference Latency\\\")\")\n",
        "print(\"*   **Context:** What triggered the alert? (e.g., \\\"Average latency 1.5s, threshold is 1.0s\\\")\")\n",
        "print(\"*   **Relevant Information:** Links to logs, dashboards, or run IDs (e.g., MLflow run ID) that can help diagnose the issue.\")\n",
        "print(\"*   **Severity:** Is this a warning, critical, or informational alert?\")\n",
        "print(\"*   **Suggested Next Steps (Optional but helpful):** What should the responder do first? (e.g., \\\"Check recent logs for errors, verify resource utilization\\\").\")\n",
        "print(\"\")\n",
        "print(\"By integrating alerting mechanisms with our automated checks and ensuring alerts are actionable, we can quickly identify and resolve issues with the Hybrid VAE, maintaining its reliability and supporting high-quality research outcomes.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### 🚨 Alerting Mechanisms\n",
            "\n",
            "Alerting is the critical next step after implementing automated checks. When a check fails (e.g., inference latency exceeds a threshold, error rate spikes), an alert is triggered to notify the appropriate personnel (e.g., researchers, MLOps engineers) that there is a potential issue requiring investigation and action. For the Hybrid Quantum-Classical VAE, effective alerting ensures the reliability and trustworthiness of the model's outputs for downstream research.\n",
            "\n",
            "#### Purpose of Alerting:\n",
            "\n",
            "*   **Early Problem Detection:** Alerts provide immediate notification of issues, often before they significantly impact end-users or research outcomes.\n",
            "*   **Reduced Downtime:** By quickly identifying problems, the time it takes to diagnose and resolve them is minimized.\n",
            "*   **Proactive Intervention:** Alerts based on saturation or subtle performance degradation can allow for proactive scaling or optimization before critical failures occur.\n",
            "*   **Maintaining Service Level Objectives (SLOs):** Alerting is essential for monitoring adherence to defined SLOs and taking action when they are breached.\n",
            "\n",
            "#### Mechanisms for Setting Up Alerts:\n",
            "\n",
            "Alerts are typically configured within a monitoring system that collects the metrics and evaluates the automated checks. Common mechanisms include:\n",
            "\n",
            "*   **Prometheus Alertmanager:** If using Prometheus for metrics collection, Alertmanager is the standard component for handling alerts. It receives alerts from Prometheus (when a configured alerting rule based on a metric is triggered), groups, silences, and routes them to various notification channels (e.g., email, Slack, PagerDuty, webhooks).\n",
            "    *   **How it receives checks:** Prometheus scrapes the metrics endpoint exposed by the application (or an exporter), and Prometheus servers evaluate alerting rules based on PromQL queries against these metrics. If a rule is met, an alert is sent to Alertmanager.\n",
            "\n",
            "*   **Cloud-Based Alerting Services:** Cloud providers offer integrated monitoring and alerting services:\n",
            "    *   **AWS CloudWatch Alarms:** Can create alarms based on CloudWatch metrics (which can be pushed from your application) or logs. Alarms can trigger actions like sending notifications via SNS, auto-scaling, or initiating EC2 actions.\n",
            "    *   **Google Cloud Monitoring Alerts:** Allows creating alerting policies based on metrics from Google Cloud services or custom metrics. Policies can trigger notifications via various channels.\n",
            "    *   **How they receive checks:** Applications push custom metrics to the cloud monitoring service APIs, or the service processes logs to extract metric information or detect specific patterns.\n",
            "\n",
            "*   **MLflow (Limited Alerting):** While MLflow is primarily for experiment tracking, it has limited alerting capabilities. You can set up simple webhook notifications based on experiment updates, but it's not a full-fledged production monitoring and alerting system like Prometheus or cloud services.\n",
            "    *   **How it receives checks:** MLflow receives metrics and parameters pushed during the run. Any custom alerting logic would need to be built on top of MLflow's API or webhooks.\n",
            "\n",
            "*   **Custom Scripting:** For simpler setups or specific needs, custom scripts can be written to periodically check metrics (e.g., query a database, read a file, call an API) and send notifications (e.g., using email libraries, Slack webhooks) if thresholds are breached.\n",
            "    *   **How it receives checks:** The script directly accesses the source of the metrics (database, file, API endpoint).\n",
            "\n",
            "#### Defining Actionable Alerts:\n",
            "\n",
            "An effective alert is not just a notification; it's **actionable**. Good alerts include:\n",
            "\n",
            "*   **Clear Description:** What is the problem? (e.g., \"High VAE Inference Latency\")\n",
            "*   **Context:** What triggered the alert? (e.g., \"Average latency 1.5s, threshold is 1.0s\")\n",
            "*   **Relevant Information:** Links to logs, dashboards, or run IDs (e.g., MLflow run ID) that can help diagnose the issue.\n",
            "*   **Severity:** Is this a warning, critical, or informational alert?\n",
            "*   **Suggested Next Steps (Optional but helpful):** What should the responder do first? (e.g., \"Check recent logs for errors, verify resource utilization\").\n",
            "\n",
            "By integrating alerting mechanisms with our automated checks and ensuring alerts are actionable, we can quickly identify and resolve issues with the Hybrid VAE, maintaining its reliability and supporting high-quality research outcomes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "034ee357",
        "outputId": "538eec5f-d9e5-4ccd-fc1d-3c2e45c36a2e"
      },
      "source": [
        "print(\"*   **Severity:** Is this a warning, critical, or informational alert?\")\n",
        "print(\"*   **Suggested Next Steps (Optional but helpful):** What should the responder do first? (e.g., \\\"Check recent logs for errors, verify resource utilization\\\").\")\n",
        "print(\"\")\n",
        "print(\"By integrating alerting mechanisms with our automated checks and ensuring alerts are actionable, we can quickly identify and resolve issues with the Hybrid VAE, maintaining its reliability and supporting high-quality research outcomes.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*   **Severity:** Is this a warning, critical, or informational alert?\n",
            "*   **Suggested Next Steps (Optional but helpful):** What should the responder do first? (e.g., \"Check recent logs for errors, verify resource utilization\").\n",
            "\n",
            "By integrating alerting mechanisms with our automated checks and ensuring alerts are actionable, we can quickly identify and resolve issues with the Hybrid VAE, maintaining its reliability and supporting high-quality research outcomes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bd39ae6"
      },
      "source": [
        "## Introduce streamlit for visualization\n",
        "\n",
        "Explain the role of Streamlit in creating interactive dashboards for visualizing collected metrics and Golden Signals.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f772fa19",
        "outputId": "fa1a3de9-af64-4d87-b908-17e56d4b0b90"
      },
      "source": [
        "print(\"### 📈 Visualizing Monitoring Data with Streamlit (Illustrative)\")\n",
        "print(\"\")\n",
        "print(\"Visualizing the collected metrics, logs, and alerts is essential for easily understanding the health and performance of the Hybrid VAE and the impact on research outcomes. While full-fledged monitoring dashboards like Grafana are common in production, tools like Streamlit can be used to create interactive dashboards for visualizing monitoring data in a more research-focused or development environment.\")\n",
        "print(\"\")\n",
        "print(\"Streamlit allows quickly building web applications using pure Python. Here's a conceptual outline of how Streamlit could be used to visualize monitoring data for the Hybrid VAE:\")\n",
        "print(\"\")\n",
        "print(\"#### 1. Data Source:\")\n",
        "print(\"\")\n",
        "print(\"The Streamlit application would need access to the collected monitoring data. This could be:\")\n",
        "print(\"*   **MLflow Tracking Server:** Retrieve logged metrics, parameters, and artifacts directly from MLflow.\")\n",
        "print(\"*   **Centralized Logging/Metrics System:** Connect to a database or service where detailed metrics and logs are stored (e.g., a simple file for this notebook's illustration, or a time-series database like Prometheus in a more robust setup).\")\n",
        "print(\"\")\n",
        "print(\"#### 2. Dashboard Components:\")\n",
        "print(\"\")\n",
        "print(\"A Streamlit dashboard could include various components to visualize different aspects of the monitoring data:\")\n",
        "print(\"\")\n",
        "print(\"*   **Key Performance Indicators (KPIs):** Display the current status of key metrics like average inference latency, error rate, and recent training loss.\")\n",
        "print(\"*   **Metric Plots:**\")\n",
        "print(\"    *   Line plots showing the trend of training loss, reconstruction error, and KL divergence over epochs.\")\n",
        "print(\"    *   Time-series plots of inference latency and traffic over time.\")\n",
        "print(\"    *   Histograms or distribution plots for quantum expectation values to understand their range and variability.\")\n",
        "print(\"    *   Resource utilization plots (CPU, GPU, memory).\")\n",
        "print(\"*   **Alerts Summary:** Display a list of recent alerts triggered based on the SRE Golden Signals, including the alert type, time, and triggering value.\")\n",
        "print(\"*   **Log Viewer:** Integrate a simple log viewer to display recent log entries, potentially with filtering capabilities.\")\n",
        "print(\"*   **Experiment Comparison:** Allow users to select different MLflow runs and compare their metrics (e.g., loss curves, benchmark times) side-by-side.\")\n",
        "print(\"\")\n",
        "print(\"#### 3. Interactivity:\")\n",
        "print(\"\")\n",
        "print(\"Streamlit's interactive widgets can enhance the dashboard:\")\n",
        "print(\"\")\n",
        "print(\"*   **Time Range Selector:** Allow users to select a specific time range for viewing metrics and logs.\")\n",
        "print(\"*   **Model Version Selector:** If multiple models are tracked in MLflow, allow selecting a specific model version.\")\n",
        "print(\"*   **Filtering:** Provide options to filter logs by severity level or keywords.\")\n",
        "print(\"\")\n",
        "print(\"#### Conceptual Streamlit Code Snippet:\")\n",
        "print(\"\")\n",
        "print(\"```python\")\n",
        "print(\"# Example (conceptual) Streamlit app structure\")\n",
        "print(\"\")\n",
        "print(\"import streamlit as st\")\n",
        "print(\"import pandas as pd\")\n",
        "print(\"import matplotlib.pyplot as plt\")\n",
        "print(\"# import mlflow # Uncomment if fetching data from MLflow\")\n",
        "print(\"\")\n",
        "print(\"st.title('Hybrid VAE Monitoring Dashboard')\")\n",
        "print(\"\")\n",
        "print(\"# --- Load Data (Illustrative - replace with actual data loading) ---\")\n",
        "print(\"# For this example, we'll use the 'metrics' dictionary from the notebook\")\n",
        "print(\"# import __main__ # Access variables from the main notebook script (Not possible in a real Streamlit app)\")\n",
        "print(\"# monitoring_data = __main__.metrics\") # Access the 'metrics' dictionary\")\n",
        "print(\"\")\n",
        "print(\"# In a real Streamlit app, load data from your source (MLflow, database, etc.)\")\n",
        "print(\"# Example: Load data from a simulated source or MLflow run\")\n",
        "print(\"monitoring_data = {\")\n",
        "print(\"    0: {'train_loss': 1560.5},\")\n",
        "print(\"    1: {'train_loss': 1553.1},\")\n",
        "print(\"    9: {'train_loss': 1507.0},\")\n",
        "print(\"    'inference_duration': 0.0108,\")\n",
        "print(\"    'inference_reconstruction_error': 15.446\")\n",
        "print(\"}\")\n",
        "print(\"\")\n",
        "print(\"\")\n",
        "print(\"# Convert training loss to a DataFrame\")\n",
        "print(\"train_loss_data = [{'epoch': e, 'loss': d['train_loss']} for e, d in monitoring_data.items() if isinstance(e, int)]\")\n",
        "print(\"train_loss_df = pd.DataFrame(train_loss_data)\")\n",
        "print(\"\")\n",
        "print(\"# Get inference metrics\")\n",
        "print(\"inference_duration = monitoring_data.get('inference_duration', 0)\")\n",
        "print(\"reconstruction_error = monitoring_data.get('inference_reconstruction_error', 0)\")\n",
        "print(\"\")\n",
        "print(\"\")\n",
        "print(\"# --- Display KPIs ---\")\n",
        "print(\"st.header('Key Performance Indicators')\")\n",
        "print(\"st.write(f'**Last Recorded Inference Duration:** {inference_duration:.4f} seconds')\") # Use st.write\n",
        "print(\"st.write(f'**Last Recorded Reconstruction Error (MSE):** {reconstruction_error:.4f}')\") # Use st.write\n",
        "print(\"st.write(f\\\"**Last Training Epoch Loss:** {train_loss_df['loss'].iloc[-1]:.4f}\\\" if not train_loss_df.empty else \\\"**Last Training Epoch Loss:** N/A\\\")\") # Use st.write\n",
        "print(\"\")\n",
        "print(\"\")\n",
        "print(\"#### Training Loss Over Epochs\")\n",
        "print(\"if not train_loss_df.empty:\")\n",
        "print(\"    fig, ax = plt.subplots()\")\n",
        "print(\"    ax.plot(train_loss_df['epoch'], train_loss_df['loss'])\")\n",
        "print(\"    ax.set_xlabel('Epoch')\")\n",
        "print(\"    ax.set_ylabel('Training Loss')\")\n",
        "print(\"    st.pyplot(fig)\")\n",
        "print(\"else:\")\n",
        "print(\"    st.write('No training loss data available.')\")\n",
        "print(\"\")\n",
        "print(\"# --- Conceptual Plot for Quantum Expectation Values (requires collecting this data) ---\")\n",
        "print(\"# st.header('Quantum Layer Metrics')\")\n",
        "print(\"# if 'quantum_expval_distribution' in monitoring_data:\")\n",
        "print(\"#     st.subheader('Quantum Expectation Value Distribution')\")\n",
        "print(\"#     fig, ax = plt.subplots()\")\n",
        "print(\"#     ax.hist(monitoring_data['quantum_expval_distribution'], bins=20)\")\n",
        "print(\"#     ax.set_xlabel('Expectation Value')\")\n",
        "print(\"#     ax.set_ylabel('Frequency')\")\n",
        "print(\"#     st.pyplot(fig)\")\n",
        "print(\"# else:\")\n",
        "print(\"#     st.write('Quantum expectation value distribution data not available.')\")\n",
        "print(\"\")\n",
        "print(\"\")\n",
        "print(\"# --- Conceptual Log Viewer (requires collecting logs to a file or database) ---\")\n",
        "print(\"# st.header('Recent Logs')\")\n",
        "print(\"# try:\")\n",
        "print(\"#     with open('app.log', 'r') as f:\")\n",
        "print(\"#         logs = f.readlines()\")\n",
        "print(\"#     for log_entry in logs[-20:]: # Display last 20 logs\")\n",
        "print(\"#         st.text(log_entry.strip())\")\n",
        "print(\"# except FileNotFoundError:\")\n",
        "print(\"#     st.write('Log file not found.')\")\n",
        "print(\"\")\n",
        "print(\"```\")\n",
        "print(\"\")\n",
        "print(\"To run a Streamlit app, you would typically save the code to a `.py` file (e.g., `monitor_app.py`) and run it from your terminal using `streamlit run monitor_app.py`. In this notebook context, this is illustrative.\")\n",
        "print(\"\")\n",
        "print(\"Visualizing monitoring data makes it significantly easier to track the performance and health of the Hybrid VAE, identify issues quickly, and gain insights that can inform both model development and scientific conclusions.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### 📈 Visualizing Monitoring Data with Streamlit (Illustrative)\n",
            "\n",
            "Visualizing the collected metrics, logs, and alerts is essential for easily understanding the health and performance of the Hybrid VAE and the impact on research outcomes. While full-fledged monitoring dashboards like Grafana are common in production, tools like Streamlit can be used to create interactive dashboards for visualizing monitoring data in a more research-focused or development environment.\n",
            "\n",
            "Streamlit allows quickly building web applications using pure Python. Here's a conceptual outline of how Streamlit could be used to visualize monitoring data for the Hybrid VAE:\n",
            "\n",
            "#### 1. Data Source:\n",
            "\n",
            "The Streamlit application would need access to the collected monitoring data. This could be:\n",
            "*   **MLflow Tracking Server:** Retrieve logged metrics, parameters, and artifacts directly from MLflow.\n",
            "*   **Centralized Logging/Metrics System:** Connect to a database or service where detailed metrics and logs are stored (e.g., a simple file for this notebook's illustration, or a time-series database like Prometheus in a more robust setup).\n",
            "\n",
            "#### 2. Dashboard Components:\n",
            "\n",
            "A Streamlit dashboard could include various components to visualize different aspects of the monitoring data:\n",
            "\n",
            "*   **Key Performance Indicators (KPIs):** Display the current status of key metrics like average inference latency, error rate, and recent training loss.\n",
            "*   **Metric Plots:**\n",
            "    *   Line plots showing the trend of training loss, reconstruction error, and KL divergence over epochs.\n",
            "    *   Time-series plots of inference latency and traffic over time.\n",
            "    *   Histograms or distribution plots for quantum expectation values to understand their range and variability.\n",
            "    *   Resource utilization plots (CPU, GPU, memory).\n",
            "*   **Alerts Summary:** Display a list of recent alerts triggered based on the SRE Golden Signals, including the alert type, time, and triggering value.\n",
            "*   **Log Viewer:** Integrate a simple log viewer to display recent log entries, potentially with filtering capabilities.\n",
            "*   **Experiment Comparison:** Allow users to select different MLflow runs and compare their metrics (e.g., loss curves, benchmark times) side-by-side.\n",
            "\n",
            "#### 3. Interactivity:\n",
            "\n",
            "Streamlit's interactive widgets can enhance the dashboard:\n",
            "\n",
            "*   **Time Range Selector:** Allow users to select a specific time range for viewing metrics and logs.\n",
            "*   **Model Version Selector:** If multiple models are tracked in MLflow, allow selecting a specific model version.\n",
            "*   **Filtering:** Provide options to filter logs by severity level or keywords.\n",
            "\n",
            "#### Conceptual Streamlit Code Snippet:\n",
            "\n",
            "```python\n",
            "# Example (conceptual) Streamlit app structure\n",
            "\n",
            "import streamlit as st\n",
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt\n",
            "# import mlflow # Uncomment if fetching data from MLflow\n",
            "\n",
            "st.title('Hybrid VAE Monitoring Dashboard')\n",
            "\n",
            "# --- Load Data (Illustrative - replace with actual data loading) ---\n",
            "# For this example, we'll use the 'metrics' dictionary from the notebook\n",
            "# import __main__ # Access variables from the main notebook script (Not possible in a real Streamlit app)\n",
            "# monitoring_data = __main__.metrics\n",
            "\n",
            "# In a real Streamlit app, load data from your source (MLflow, database, etc.)\n",
            "# Example: Load data from a simulated source or MLflow run\n",
            "monitoring_data = {\n",
            "    0: {'train_loss': 1560.5},\n",
            "    1: {'train_loss': 1553.1},\n",
            "    9: {'train_loss': 1507.0},\n",
            "    'inference_duration': 0.0108,\n",
            "    'inference_reconstruction_error': 15.446\n",
            "}\n",
            "\n",
            "\n",
            "# Convert training loss to a DataFrame\n",
            "train_loss_data = [{'epoch': e, 'loss': d['train_loss']} for e, d in monitoring_data.items() if isinstance(e, int)]\n",
            "train_loss_df = pd.DataFrame(train_loss_data)\n",
            "\n",
            "# Get inference metrics\n",
            "inference_duration = monitoring_data.get('inference_duration', 0)\n",
            "reconstruction_error = monitoring_data.get('inference_reconstruction_error', 0)\n",
            "\n",
            "\n",
            "# --- Display KPIs ---\n",
            "st.header('Key Performance Indicators')\n",
            "st.write(f'**Last Recorded Inference Duration:** {inference_duration:.4f} seconds')\n",
            "st.write(f'**Last Recorded Reconstruction Error (MSE):** {reconstruction_error:.4f}')\n",
            "st.write(f\"**Last Training Epoch Loss:** {train_loss_df['loss'].iloc[-1]:.4f}\" if not train_loss_df.empty else \"**Last Training Epoch Loss:** N/A\")\n",
            "\n",
            "\n",
            "#### Training Loss Over Epochs\n",
            "if not train_loss_df.empty:\n",
            "    fig, ax = plt.subplots()\n",
            "    ax.plot(train_loss_df['epoch'], train_loss_df['loss'])\n",
            "    ax.set_xlabel('Epoch')\n",
            "    ax.set_ylabel('Training Loss')\n",
            "    st.pyplot(fig)\n",
            "else:\n",
            "    st.write('No training loss data available.')\n",
            "\n",
            "# --- Conceptual Plot for Quantum Expectation Values (requires collecting this data) ---\n",
            "# st.header('Quantum Layer Metrics')\n",
            "# if 'quantum_expval_distribution' in monitoring_data:\n",
            "#     st.subheader('Quantum Expectation Value Distribution')\n",
            "#     fig, ax = plt.subplots()\n",
            "#     ax.hist(monitoring_data['quantum_expval_distribution'], bins=20)\n",
            "#     ax.set_xlabel('Expectation Value')\n",
            "#     ax.set_ylabel('Frequency')\n",
            "#     st.pyplot(fig)\n",
            "# else:\n",
            "#     st.write('Quantum expectation value distribution data not available.')\n",
            "\n",
            "\n",
            "# --- Conceptual Log Viewer (requires collecting logs to a file or database) ---\n",
            "# st.header('Recent Logs')\n",
            "# try:\n",
            "#     with open('app.log', 'r') as f:\n",
            "#         logs = f.readlines()\n",
            "#     for log_entry in logs[-20:]: # Display last 20 logs\n",
            "#         st.text(log_entry.strip())\n",
            "# except FileNotFoundError:\n",
            "#     st.write('Log file not found.')\n",
            "\n",
            "```\n",
            "\n",
            "To run a Streamlit app, you would typically save the code to a `.py` file (e.g., `monitor_app.py`) and run it from your terminal using `streamlit run monitor_app.py`. In this notebook context, this is illustrative.\n",
            "\n",
            "Visualizing monitoring data makes it significantly easier to track the performance and health of the Hybrid VAE, identify issues quickly, and gain insights that can inform both model development and scientific conclusions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "946ed337"
      },
      "source": [
        "## Illustrative streamlit dashboard code (conceptual)\n",
        "\n",
        " illustrative code examples for setting up a basic monitoring system (e.g., using a simple logging framework or demonstrating how metrics would be collected). Acknowledge that a full-fledged monitoring setup is beyond the scope of a single notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c55eb0d3",
        "outputId": "73daf6cc-7a7b-4871-cb6d-532e15b149ab"
      },
      "source": [
        "print(\"### 🛠️ Illustrative Monitoring Setup\")\n",
        "print(\"\")\n",
        "print(\"This section provides conceptual and illustrative code snippets to demonstrate the basic principles of setting up monitoring for the Hybrid VAE within this notebook environment. It's important to note that a full-fledged, production-grade monitoring system involves more complex components like dedicated time-series databases (e.g., Prometheus), visualization dashboards (e.g., Grafana), distributed tracing systems (e.g., Jaeger, Zipkin), and centralized logging platforms (e.g., ELK stack, Splunk). The examples here are simplified for demonstration purposes and do not require setting up these external systems.\")\n",
        "print(\"\")\n",
        "print(\"---\")\n",
        "print(\"\")\n",
        "print(\"#### 1. Basic Logging\")\n",
        "print(\"\")\n",
        "print(\"Logging is essential for recording events, errors, and detailed information about the execution flow.\")\n",
        "print(\"\")\n",
        "print(\"```python\")\n",
        "print(\"import logging\")\n",
        "print(\"# Configure basic logging (if not already done)\")\n",
        "print(\"logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\")\n",
        "print(\"\")\n",
        "print(\"# Example logging statements within the code:\")\n",
        "print(\"logging.info('Starting training epoch...')\")\n",
        "print(\"try:\")\n",
        "print(\"    # Code that might fail\")\n",
        "print(\"    pass\")\n",
        "print(\"except Exception as e:\")\n",
        "print(\"    logging.error(f'An error occurred during training: {e}', exc_info=True)\")\n",
        "print(\"logging.info('Training epoch finished.')\")\n",
        "print(\"```\")\n",
        "print(\"\")\n",
        "print(\"---\")\n",
        "print(\"\")\n",
        "print(\"#### 2. Simple Metrics Collection (Manual)\")\n",
        "print(\"\")\n",
        "print(\"For quick monitoring within a script, you can use simple data structures like dictionaries to collect metrics.\")\n",
        "print(\"\")\n",
        "print(\"```python\")\n",
        "print(\"import time\")\n",
        "print(\"# Simple dictionary to simulate metrics collection\")\n",
        "print(\"metrics = {}\")\n",
        "print(\"\")\n",
        "print(\"# Example within a training loop:\")\n",
        "print(\"start_time = time.time()\")\n",
        "print(\"# ... training code ...\")\n",
        "print(\"end_time = time.time()\")\n",
        "print(\"epoch_duration = end_time - start_time\")\n",
        "print(\"metrics['epoch_duration_seconds'] = epoch_duration\")\n",
        "print(\"metrics['training_loss'] = latest_loss # Assuming latest_loss is calculated\")\n",
        "print(\"\")\n",
        "print(\"# Example within an inference function:\")\n",
        "print(\"start_time = time.time()\")\n",
        "print(\"# ... inference code ...\")\n",
        "print(\"end_time = time.time()\")\n",
        "print(\"inference_duration = end_time - start_time\")\n",
        "print(\"metrics['inference_duration_seconds'] = inference_duration\")\n",
        "print(\"metrics['reconstruction_error'] = calculated_error # Assuming error is calculated\")\n",
        "print(\"\")\n",
        "print(\"# Print collected metrics (for demonstration)\")\n",
        "print(\"print(metrics)\")\n",
        "print(\"```\")\n",
        "print(\"\")\n",
        "print(\"---\")\n",
        "print(\"\")\n",
        "print(\"#### 3. Conceptual Integration with Advanced Systems\")\n",
        "print(\"\")\n",
        "print(\"For production environments, you would integrate with dedicated monitoring systems. Here are conceptual examples:\")\n",
        "print(\"\")\n",
        "print(\"##### Prometheus Client (for metrics)\")\n",
        "print(\"\")\n",
        "print(\"```python\")\n",
        "print(\"# import prometheus_client\")\n",
        "print(\"# import time\")\n",
        "print(\"\")\n",
        "print(\"# # Define metrics\")\n",
        "print(\"# INFERENCE_LATENCY = prometheus_client.Summary('inference_latency_seconds', 'Time taken for inference requests')\")\n",
        "print(\"# QUANTUM_CIRCUIT_DURATION = prometheus_client.Summary('quantum_circuit_duration_seconds', 'Duration of quantum circuit execution')\")\n",
        "print(\"# RECONSTRUCTION_ERROR = prometheus_client.Gauge('reconstruction_error_mse', 'Mean Squared Error of reconstruction')\")\n",
        "print(\"\")\n",
        "print(\"# # In the inference code:\")\n",
        "print(\"# start_time = time.time()\")\n",
        "print(\"# # ... inference process ...\")\n",
        "print(\"# end_time = time.time()\")\n",
        "print(\"# INFERENCE_LATENCY.observe(end_time - start_time)\")\n",
        "print(\"# RECONSTRUCTION_ERROR.set(calculated_error)\")\n",
        "print(\"\")\n",
        "print(\"# # Around quantum layer call:\")\n",
        "print(\"# start_q_time = time.time()\")\n",
        "print(\"# # ... quantum circuit execution ...\")\n",
        "print(\"# end_q_time = time.time()\")\n",
        "print(\"# QUANTUM_CIRCUIT_DURATION.observe(end_q_time - start_q_time)\")\n",
        "print(\"\")\n",
        "print(\"# # Expose metrics via an HTTP server (typically in a separate service)\")\n",
        "print(\"# # prometheus_client.start_http_server(8000)\")\n",
        "print(\"```\")\n",
        "print(\"\")\n",
        "print(\"##### OpenTelemetry (for tracing)\")\n",
        "print(\"\")\n",
        "print(\"```python\")\n",
        "print(\"# from opentelemetry import trace\")\n",
        "print(\"# from opentelemetry.sdk.trace import TracerProvider\")\n",
        "print(\"# from opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor\")\n",
        "print(\"# import time\")\n",
        "print(\"\")\n",
        "print(\"# # Configure tracer (typically done once at application start)\")\n",
        "print(\"# # provider = TracerProvider()\")\n",
        "print(\"# # processor = SimpleSpanProcessor(ConsoleSpanExporter()) # Export spans to console\")\n",
        "print(\"# # provider.add_span_processor(processor)\")\n",
        "print(\"# # trace.set_tracer_provider(provider)\")\n",
        "print(\"# # tracer = trace.get_tracer(__name__)\")\n",
        "print(\"\")\n",
        "print(\"# # Around specific operations\")\n",
        "print(\"# with tracer.start_as_current_span('vae_inference'):\")\n",
        "print(\"#     with tracer.start_as_current_span('classical_encode'):\")\n",
        "print(\"#         # ... encoder code ...\")\n",
        "print(\"#         pass\")\n",
        "print(\"#     with tracer.start_as_current_span('quantum_layer'):\")\n",
        "print(\"#         # ... quantum circuit execution ...\")\n",
        "print(\"#         pass\")\n",
        "print(\"#     with tracer.start_as_current_span('classical_decode'):\")\n",
        "print(\"#         # ... decoder code ...\")\n",
        "print(\"#         pass\")\n",
        "print(\"```\")\n",
        "print(\"\")\n",
        "print(\"These examples illustrate the basic concepts. A production monitoring setup would involve more detailed metric collection, structured logging, distributed tracing across services, and integration with dedicated monitoring platforms for storage, analysis, visualization, and alerting.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### 🛠️ Illustrative Monitoring Setup\n",
            "\n",
            "This section provides conceptual and illustrative code snippets to demonstrate the basic principles of setting up monitoring for the Hybrid VAE within this notebook environment. It's important to note that a full-fledged, production-grade monitoring system involves more complex components like dedicated time-series databases (e.g., Prometheus), visualization dashboards (e.g., Grafana), distributed tracing systems (e.g., Jaeger, Zipkin), and centralized logging platforms (e.g., ELK stack, Splunk). The examples here are simplified for demonstration purposes and do not require setting up these external systems.\n",
            "\n",
            "---\n",
            "\n",
            "#### 1. Basic Logging\n",
            "\n",
            "Logging is essential for recording events, errors, and detailed information about the execution flow.\n",
            "\n",
            "```python\n",
            "import logging\n",
            "# Configure basic logging (if not already done)\n",
            "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
            "\n",
            "# Example logging statements within the code:\n",
            "logging.info('Starting training epoch...')\n",
            "try:\n",
            "    # Code that might fail\n",
            "    pass\n",
            "except Exception as e:\n",
            "    logging.error(f'An error occurred during training: {e}', exc_info=True)\n",
            "logging.info('Training epoch finished.')\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "#### 2. Simple Metrics Collection (Manual)\n",
            "\n",
            "For quick monitoring within a script, you can use simple data structures like dictionaries to collect metrics.\n",
            "\n",
            "```python\n",
            "import time\n",
            "# Simple dictionary to simulate metrics collection\n",
            "metrics = {}\n",
            "\n",
            "# Example within a training loop:\n",
            "start_time = time.time()\n",
            "# ... training code ...\n",
            "end_time = time.time()\n",
            "epoch_duration = end_time - start_time\n",
            "metrics['epoch_duration_seconds'] = epoch_duration\n",
            "metrics['training_loss'] = latest_loss # Assuming latest_loss is calculated\n",
            "\n",
            "# Example within an inference function:\n",
            "start_time = time.time()\n",
            "# ... inference code ...\n",
            "end_time = time.time()\n",
            "inference_duration = end_time - start_time\n",
            "metrics['inference_duration_seconds'] = inference_duration\n",
            "metrics['reconstruction_error'] = calculated_error # Assuming error is calculated\n",
            "\n",
            "# Print collected metrics (for demonstration)\n",
            "print(metrics)\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "#### 3. Conceptual Integration with Advanced Systems\n",
            "\n",
            "For production environments, you would integrate with dedicated monitoring systems. Here are conceptual examples:\n",
            "\n",
            "##### Prometheus Client (for metrics)\n",
            "\n",
            "```python\n",
            "# import prometheus_client\n",
            "# import time\n",
            "\n",
            "# # Define metrics\n",
            "# INFERENCE_LATENCY = prometheus_client.Summary('inference_latency_seconds', 'Time taken for inference requests')\n",
            "# QUANTUM_CIRCUIT_DURATION = prometheus_client.Summary('quantum_circuit_duration_seconds', 'Duration of quantum circuit execution')\n",
            "# RECONSTRUCTION_ERROR = prometheus_client.Gauge('reconstruction_error_mse', 'Mean Squared Error of reconstruction')\n",
            "\n",
            "# # In the inference code:\n",
            "# start_time = time.time()\n",
            "# # ... inference process ...\n",
            "# end_time = time.time()\n",
            "# INFERENCE_LATENCY.observe(end_time - start_time)\n",
            "# RECONSTRUCTION_ERROR.set(calculated_error)\n",
            "\n",
            "# # Around quantum layer call:\n",
            "# start_q_time = time.time()\n",
            "# # ... quantum circuit execution ...\n",
            "# end_q_time = time.time()\n",
            "# QUANTUM_CIRCUIT_DURATION.observe(end_q_time - start_q_time)\n",
            "\n",
            "# # Expose metrics via an HTTP server (typically in a separate service)\n",
            "# # prometheus_client.start_http_server(8000)\n",
            "```\n",
            "\n",
            "##### OpenTelemetry (for tracing)\n",
            "\n",
            "```python\n",
            "# from opentelemetry import trace\n",
            "# from opentelemetry.sdk.trace import TracerProvider\n",
            "# from opentelemetry.sdk.trace.export import ConsoleSpanExporter, SimpleSpanProcessor\n",
            "# import time\n",
            "\n",
            "# # Configure tracer (typically done once at application start)\n",
            "# # provider = TracerProvider()\n",
            "# # processor = SimpleSpanProcessor(ConsoleSpanExporter()) # Export spans to console\n",
            "# # provider.add_span_processor(processor)\n",
            "# # trace.set_tracer_provider(provider)\n",
            "# # tracer = trace.get_tracer(__name__)\n",
            "\n",
            "# # Around specific operations\n",
            "# with tracer.start_as_current_span('vae_inference'):\n",
            "#     with tracer.start_as_current_span('classical_encode'):\n",
            "#         # ... encoder code ...\n",
            "#         pass\n",
            "#     with tracer.start_as_current_span('quantum_layer'):\n",
            "#         # ... quantum circuit execution ...\n",
            "#         pass\n",
            "#     with tracer.start_as_current_span('classical_decode'):\n",
            "#         # ... decoder code ...\n",
            "#         pass\n",
            "```\n",
            "\n",
            "These examples illustrate the basic concepts. A production monitoring setup would involve more detailed metric collection, structured logging, distributed tracing across services, and integration with dedicated monitoring platforms for storage, analysis, visualization, and alerting.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e7c32c6"
      },
      "source": [
        "## Connect sre golden signals to implementation\n",
        "\n",
        "Map the SRE Golden Signals to the specific metrics and checks implemented for the Hybrid VAE.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd51fda8",
        "outputId": "4bad947d-baa0-497d-f6b7-3f889c688316"
      },
      "source": [
        "print(\"### 🚦 SRE Golden Signals and Hybrid VAE Monitoring\")\n",
        "print(\"\")\n",
        "print(\"The SRE Golden Signals provide a powerful framework for monitoring the Hybrid VAE from an operational perspective, but they also offer crucial insights into the quality and reliability of the model's output for **end-user research outcomes**.\")\n",
        "print(\"\")\n",
        "print(\"Here's how the Golden Signals map to our implemented metrics and checks:\")\n",
        "print(\"\")\n",
        "print(\"#### Latency:\")\n",
        "print(\"\")\n",
        "print(\"*   **Monitoring:** We monitor **Total Inference Time** (`total_inference_time_seconds`), **Encoder Processing Time** (`encoder_processing_time_seconds`), **Quantum Circuit Duration** (`quantum_circuit_duration_seconds`), and **Decoder Processing Time** (`decoder_processing_time_seconds`). These metrics provide a breakdown of where time is spent during an inference request.\")\n",
        "print(\"*   **Checks:** Automated checks are set up based on a defined **Reconstruction Error Threshold** (`reconstruction_error_threshold`). While this is an error check, high reconstruction error can sometimes correlate with increased processing time as the model struggles to reconstruct the input.\")\n",
        "print(\"*   **Research Outcome Relevance:** High latency directly impacts the speed of research workflows, especially when processing large datasets or performing interactive analysis. Monitoring latency ensures that the VAE is performant enough to support timely scientific discovery.\")\n",
        "print(\"\")\n",
        "print(\"#### Traffic:\")\n",
        "print(\"\")\n",
        "print(\"*   **Monitoring:** We are not explicitly monitoring request rate (Traffic) in this notebook due to its simulated nature. In a production setup, this would involve tracking the number of inference requests per unit of time.\")\n",
        "print(\"*   **Checks:** No direct automated checks for traffic are implemented in this illustrative setup.\")\n",
        "print(\"*   **Research Outcome Relevance:** Understanding traffic patterns helps ensure the system can handle the demand placed on the VAE, preventing performance degradation (increased latency, errors) that could disrupt research activities.\")\n",
        "print(\"\")\n",
        "print(\"#### Errors:\")\n",
        "print(\"\")\n",
        "print(\"*   **Monitoring:** We monitor **Total VAE Loss** (`total_vae_loss`), **Reconstruction Error MSE** (`reconstruction_error_mse`), and **KL Divergence Loss** (`kl_divergence_loss`) during training. During inference, we monitor **Inference Reconstruction Error MSE** (`inference_reconstruction_error_mse`). Implicitly, system-level errors (exceptions) would be captured by logging.\")\n",
        "print(\"*   **Checks:** Automated checks are implemented based on the **Reconstruction Error Threshold** (`reconstruction_error_threshold`). Additionally, checks on the **Quantum Expval Mean Range** (`quantum_expval_mean_range`) and **Quantum Expval Std Threshold** (`quantum_expval_std_threshold`) can indicate potential issues with the quantum layer's output, which might manifest as increased reconstruction error or affect downstream analysis.\")\n",
        "print(\"*   **Research Outcome Relevance:** High error rates or poor reconstruction quality directly compromise the validity and trustworthiness of research findings derived from the VAE's latent space or reconstructed data. Monitoring errors ensures the model is producing reliable outputs.\")\n",
        "print(\"\")\n",
        "print(\"#### Saturation:\")\n",
        "print(\"\")\n",
        "print(\"*   **Monitoring:** While not explicitly collected as Prometheus metrics in this notebook, **Resource Utilization** (CPU, GPU, Memory) would be monitored in a production environment.\")\n",
        "print(\"*   **Checks:** No direct automated checks for saturation are implemented in this illustrative setup.\")\n",
        "print(\"*   **Research Outcome Relevance:** Monitoring saturation helps understand the resource constraints of running the VAE. High saturation can lead to increased latency and errors, limiting the scale of research that can be performed and potentially impacting the stability of the research environment.\")\n",
        "print(\"\")\n",
        "print(\"By mapping the SRE Golden Signals to our collected metrics and implemented checks, we gain a structured approach to monitoring the Hybrid VAE's operational health. This operational insight is directly translatable into understanding the model's reliability, performance, and the overall quality of the end-user research outcomes it supports.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### 🚦 SRE Golden Signals and Hybrid VAE Monitoring\n",
            "\n",
            "The SRE Golden Signals provide a powerful framework for monitoring the Hybrid VAE from an operational perspective, but they also offer crucial insights into the quality and reliability of the model's output for **end-user research outcomes**.\n",
            "\n",
            "Here's how the Golden Signals map to our implemented metrics and checks:\n",
            "\n",
            "#### Latency:\n",
            "\n",
            "*   **Monitoring:** We monitor **Total Inference Time** (`total_inference_time_seconds`), **Encoder Processing Time** (`encoder_processing_time_seconds`), **Quantum Circuit Duration** (`quantum_circuit_duration_seconds`), and **Decoder Processing Time** (`decoder_processing_time_seconds`). These metrics provide a breakdown of where time is spent during an inference request.\n",
            "*   **Checks:** Automated checks are set up based on a defined **Reconstruction Error Threshold** (`reconstruction_error_threshold`). While this is an error check, high reconstruction error can sometimes correlate with increased processing time as the model struggles to reconstruct the input.\n",
            "*   **Research Outcome Relevance:** High latency directly impacts the speed of research workflows, especially when processing large datasets or performing interactive analysis. Monitoring latency ensures that the VAE is performant enough to support timely scientific discovery.\n",
            "\n",
            "#### Traffic:\n",
            "\n",
            "*   **Monitoring:** We are not explicitly monitoring request rate (Traffic) in this notebook due to its simulated nature. In a production setup, this would involve tracking the number of inference requests per unit of time.\n",
            "*   **Checks:** No direct automated checks for traffic are implemented in this illustrative setup.\n",
            "*   **Research Outcome Relevance:** Understanding traffic patterns helps ensure the system can handle the demand placed on the VAE, preventing performance degradation (increased latency, errors) that could disrupt research activities.\n",
            "\n",
            "#### Errors:\n",
            "\n",
            "*   **Monitoring:** We monitor **Total VAE Loss** (`total_vae_loss`), **Reconstruction Error MSE** (`reconstruction_error_mse`), and **KL Divergence Loss** (`kl_divergence_loss`) during training. During inference, we monitor **Inference Reconstruction Error MSE** (`inference_reconstruction_error_mse`). Implicitly, system-level errors (exceptions) would be captured by logging.\n",
            "*   **Checks:** Automated checks are implemented based on the **Reconstruction Error Threshold** (`reconstruction_error_threshold`). Additionally, checks on the **Quantum Expval Mean Range** (`quantum_expval_mean_range`) and **Quantum Expval Std Threshold** (`quantum_expval_std_threshold`) can indicate potential issues with the quantum layer's output, which might manifest as increased reconstruction error or affect downstream analysis.\n",
            "*   **Research Outcome Relevance:** High error rates or poor reconstruction quality directly compromise the validity and trustworthiness of research findings derived from the VAE's latent space or reconstructed data. Monitoring errors ensures the model is producing reliable outputs.\n",
            "\n",
            "#### Saturation:\n",
            "\n",
            "*   **Monitoring:** While not explicitly collected as Prometheus metrics in this notebook, **Resource Utilization** (CPU, GPU, Memory) would be monitored in a production environment.\n",
            "*   **Checks:** No direct automated checks for saturation are implemented in this illustrative setup.\n",
            "*   **Research Outcome Relevance:** Monitoring saturation helps understand the resource constraints of running the VAE. High saturation can lead to increased latency and errors, limiting the scale of research that can be performed and potentially impacting the stability of the research environment.\n",
            "\n",
            "By mapping the SRE Golden Signals to our collected metrics and implemented checks, we gain a structured approach to monitoring the Hybrid VAE's operational health. This operational insight is directly translatable into understanding the model's reliability, performance, and the overall quality of the end-user research outcomes it supports.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63458930"
      },
      "source": [
        "## Summarize advanced monitoring\n",
        "\n",
        "Summarize the importance and benefits of implementing a robust monitoring system with automated checks and visualization for the Hybrid VAE in a production-like research environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b041058",
        "outputId": "85da2111-0324-4816-e717-f26e17ff0f8b"
      },
      "source": [
        "print(\"### 👁️ Summary of Advanced Monitoring\")\n",
        "print(\"\")\n",
        "print(\"Implementing a robust monitoring system with detailed metrics, automated checks, alerting, and visualization is paramount for the successful deployment and utilization of the Hybrid Quantum-Classical VAE in a production-like research environment. This advanced level of observability extends beyond basic experiment tracking to provide critical insights into the model's operational health and its direct impact on end-user research outcomes.\")\n",
        "print(\"\")\n",
        "print(\"The key benefits include:\")\n",
        "print(\"\")\n",
        "print(\"1.  **Enhanced Reliability and Trust:** Detailed metrics (like reconstruction error, quantum expectation value distribution) and automated error checks ensure the VAE consistently produces valid and high-quality outputs, building trust in the research findings derived from the model.\")\n",
        "print(\"2.  **Proactive Issue Detection and Faster Debugging:** Automated checks based on SRE Golden Signals (Latency, Errors, Saturation) trigger timely alerts, enabling proactive identification of performance degradation or failures. Coupled with detailed logs and potential tracing, this significantly reduces the time required to diagnose and resolve issues, minimizing disruption to research workflows.\")\n",
        "print(\"3.  **Optimized Performance and Resource Utilization:** Monitoring component-specific processing times and resource saturation helps identify bottlenecks in both classical and quantum parts, guiding optimization efforts and ensuring efficient use of computational resources for large-scale research.\")\n",
        "print(\"4.  **Improved Model Development:** Insights gained from monitoring data (e.g., how changes in quantum circuit parameters affect expectation values and reconstruction) can directly inform iterative model improvements and guide the development of more effective hybrid architectures.\")\n",
        "print(\"5.  **Informed Research Decisions:** Visualizing monitoring data through dashboards provides researchers with a clear understanding of the VAE's performance characteristics and limitations, allowing them to make informed decisions about data scale, computational resources, and the interpretation of results.\")\n",
        "print(\"\")\n",
        "print(\"In conclusion, integrating advanced monitoring practices into the MLOps lifecycle for the Hybrid VAE transforms it from a potentially opaque model into a transparent and reliable tool. This not only ensures operational stability but also directly contributes to the trustworthiness, efficiency, and validity of the scientific discoveries made by end-users.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### 👁️ Summary of Advanced Monitoring\n",
            "\n",
            "Implementing a robust monitoring system with detailed metrics, automated checks, alerting, and visualization is paramount for the successful deployment and utilization of the Hybrid Quantum-Classical VAE in a production-like research environment. This advanced level of observability extends beyond basic experiment tracking to provide critical insights into the model's operational health and its direct impact on end-user research outcomes.\n",
            "\n",
            "The key benefits include:\n",
            "\n",
            "1.  **Enhanced Reliability and Trust:** Detailed metrics (like reconstruction error, quantum expectation value distribution) and automated error checks ensure the VAE consistently produces valid and high-quality outputs, building trust in the research findings derived from the model.\n",
            "2.  **Proactive Issue Detection and Faster Debugging:** Automated checks based on SRE Golden Signals (Latency, Errors, Saturation) trigger timely alerts, enabling proactive identification of performance degradation or failures. Coupled with detailed logs and potential tracing, this significantly reduces the time required to diagnose and resolve issues, minimizing disruption to research workflows.\n",
            "3.  **Optimized Performance and Resource Utilization:** Monitoring component-specific processing times and resource saturation helps identify bottlenecks in both classical and quantum parts, guiding optimization efforts and ensuring efficient use of computational resources for large-scale research.\n",
            "4.  **Improved Model Development:** Insights gained from monitoring data (e.g., how changes in quantum circuit parameters affect expectation values and reconstruction) can directly inform iterative model improvements and guide the development of more effective hybrid architectures.\n",
            "5.  **Informed Research Decisions:** Visualizing monitoring data through dashboards provides researchers with a clear understanding of the VAE's performance characteristics and limitations, allowing them to make informed decisions about data scale, computational resources, and the interpretation of results.\n",
            "\n",
            "In conclusion, integrating advanced monitoring practices into the MLOps lifecycle for the Hybrid VAE transforms it from a potentially opaque model into a transparent and reliable tool. This not only ensures operational stability but also directly contributes to the trustworthiness, efficiency, and validity of the scientific discoveries made by end-users.\n"
          ]
        }
      ]
    }
  ]
}